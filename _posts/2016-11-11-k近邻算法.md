---
layout: post
categories: [Python,scikitlearn]
description: none
keywords: Python
---
# k-近邻算法
k-近邻算法是一个有监督的机器学习算法。k-近邻算法也称为knn算法，可以解决分类问题，也可以解决回归问题。

涵盖的内容如下：
- k-近邻算法的原理、优缺点及参数k取值对算法性能的影响；
- 使用k-近邻算法处理分类问题的示例；
- 使用k-近邻算法解决回归问题的示例；
- 使用k-近邻算法进行糖尿病检测的实例；
- 基于统计学的特征选择；
- 扩展阅读之k-近邻算法性能优化；
- 扩展阅读之卡方检测及F值检测。

## 算法原理
近邻算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。k-近邻算法的核心思想是未标记样本的类别，由距离其最近的k个邻居投票来决定。

假设，我们有一个已经标记的数据集，即已经知道了数据集中每个样本所属的类别。此时，有一个未标记的数据样本，我们的任务是预测出这个数据样本所属的类别。

knn算法的原理比较简单，它是被用来做分类的一个算法，它的数据集是有标记的，即knn属于监督学习的算法。基本原理是，对于一个新给的数据，找到与其最近的k个数据，这k个数据中哪个类别的数据最多，则认为该数据也属于这个类别。用一个词语来理解就是物以类聚或者近朱者赤近墨者黑。

KNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。

假设X_test为待标记的数据样本，X_train为已标记的数据集，算法原理的伪代码如下：
- 遍历X_train中的所有样本，计算每个样本与X_test的距离，并把距离保存在Distance数组中。
- 对Distance数组进行排序，取距离最近的k个点，记为X_knn。
- 在X_knn中统计每个类别的个数，即class0在X_knn中有几个样本，class1在X_knn中有几个样本等。
- 待标记样本的类别，就是在X_knn中样本个数最多的那个类别。

## 使用场景
k邻近值算法适合于分类问题。分类问题是机器学习非常重要的一个组成部分，目标是根据已知样本的某些特征，判断样本属于哪一类，具体细分如下：
- 二分类问题：表示分类任务中有两个类别的样本属于哪种已知样本类。
- 多类分类问题：表示分类任务中有多类别。
- 多标签分类问题：给每个样本一系列的目标标签。

分类问题在数学上就是把数据映射到n维空间的样本点（n指特征维度），我们需要做的是对n维样本空间的点进行类别区分，某些点会归属到某个类别。常见的实际应用场景有如下：
- 垃圾邮件识别：可以作为二分类问题，将邮件分为你「垃圾邮件」或者「正常邮件」。
- 图像内容识别：因为图像的内容种类不止一个，图像内容可能是猫、狗、人等等，因此是多类分类问题。
- 文本情感分析：既可以作为二分类问题，将情感分为褒贬两种，还可以作为多类分类问题，将情感种类扩展，比如分为：十分消极、消极、积极、十分积极等。
像机器学习常见的一些任务：鸢尾花分类、手写数字识别、猫狗分类都可以使用k邻近算法解决。鸢尾花分类就是一项多分类任务，按照鸢尾花4个特征（四维）对鸢尾花数据样本再空间进行映射，相似或相同的数据点会大致分布到一起，这样，在已知这部分样本点后，再把未知的样本按照4个特征放入空间中，未知样本周围是哪一类样本占多数就归为这一类。手写数字识别亦是如此。具有相似多个特征的样本会大致分布在一个位置，未知样本再投影到空间中会位于相近的位置。

而以上提到的任务也都与分类有关，故k邻近算法一般应用于机器学习中的分类问题/任务。当然肯定不会只有机器学习有分类任务，还有邮件识别等具体场景领域。

## 算法优缺点
优点：准确性高，对异常值和噪声有较高的容忍度。缺点：计算量较大，对内存的需求也较大。从算法原理可以看出来，每次对一个未标记样本进行分类时，都需要全部计算一遍距离。

## 算法参数
其算法参数是k，参数选择需要根据数据来决定。k值越大，模型的偏差越大，对噪声数据越不敏感，当k值很大时，可能造成模型欠拟合；k值越小，模型的方差就会越大，当k值太小，就会造成模型过拟合。

## 算法的变种
k-近邻算法有一些变种，其中之一就是可以增加邻居的权重。默认情况下，在计算距离时，都是使用相同权重。实际上，我们可以针对不同的邻居指定不同的距离权重，如距离越近权重越高。这个可以通过指定算法的weights参数来实现。

另外一个变种是，使用一定半径内的点取代距离最近的k个点。在scikit-learn里，RadiusNeighborsClassifier类实现了这个算法的变种。当数据采样不均匀时，该算法变种可以取得更好的性能。

## 示例:使用k-近邻算法进行分类
在scikit-learn里，使用k-近邻算法进行分类处理的是sklearn.neighbors.KNeighbors Classifier类。

### 生成已标记的数据集：
```python
from sklearn.datasets import make_blobs
# 生成数据
centers = [[-2, 2], [2, 2], [0, 4]]
X, y = make_blobs(n_samples=60, centers=centers,
                  random_state=0, cluster_std=0.60)

```
我们使用sklearn.datasets包下的make_blobs（）函数来生成数据集，上面代码中，生成60个训练样本，这60个样本分布在以centers参数指定中心点周围。
cluster_std是标准差，用来指明生成的点分布的松散程度。生成的训练数据集放在变量X里面，数据集的类别标记放在y里面。
使用matplotlib库，它可以很容易地把生成的点画出来：
```python
# 画出数据
plt.figure(figsize=(16, 10), dpi=144)
c = np.array(centers)
plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='cool'); # 画出样本
plt.scatter(c[:, 0], c[:, 1], s=100, marker='^', c='orange'); # 画出中心点
plt.show()
```


### 使用KNeighborsClassifier来对算法进行训练，我们选择的参数是k=5：
```python
from sklearn.neighbors import KNeighborsClassifier
# 模型训练
k = 5
clf = KNeighborsClassifier(n_neighbors=k)
clf.fit(X, y);
```

### 对一个新的样本进行预测：
```python
# 进行预测
X_sample = np.array([[0, 2]])
y_sample = clf.predict(X_sample)
neighbors = clf.kneighbors(X_sample, return_distance=False)
```
我们要预测的样本是`[0，2]`，使用kneighbors（）方法，把这个样本周围距离最近的5个点取出来。取出来的点是训练样本X里的索引，从0开始计算。

### 把待预测的样本以及和其最近的5个点标记出来：
```python
# 画出示意图
plt.figure(figsize=(16,10), dpi=144)
c = np.array(centers)
plt.scatter(X[:,0], X[:,1], c=y, s=100, cmap='cool') # 出样本
plt.scatter(c[:,0], c[:,1], s=100, marker='^',c='k') # 中心点
plt.scatter(X_sample[0][0], X_sample[0][1],c=y_sample, marker="x",
           s=100, cmap='cool')      # 待预测的点
 
for i in neighbors[0]:
    plt.plot([X[i][0], X_sample[0][0]], [X[i][1], X_sample[0][1]],
            'k--', linewidth=0.6)  # 预测点与距离最近的5个样本的连线
# plt.savefig('knn_predict.png')
plt.show()
```

## 完整代码如下
```python
from sklearn.datasets import make_blobs
from sklearn.neighbors import KNeighborsClassifier
from matplotlib import pyplot as plt
import numpy as np
# 生成数据
centers = [[-2, 2], [2, 2], [0, 4]]
X, y = make_blobs(n_samples=60, centers=centers,
                  random_state=0, cluster_std=0.60)

print(X)
print("---------")
print(y)

# 模型训练
k = 5
clf = KNeighborsClassifier(n_neighbors=k)
clf.fit(X, y);

# 进行预测
X_sample = np.array([[0, 2]])
y_sample = clf.predict(X_sample)
neighbors = clf.kneighbors(X_sample, return_distance=False)

print(X_sample)
print(y_sample)
print(neighbors)


# 画出示意图
plt.figure(figsize=(16,10), dpi=144)
c = np.array(centers)
plt.scatter(X[:,0], X[:,1], c=y, s=100, cmap='cool') # 出样本
plt.scatter(c[:,0], c[:,1], s=100, marker='^',c='k') # 中心点
plt.scatter(X_sample[0][0], X_sample[0][1],c=y_sample, marker="x",
           s=100, cmap='cool')      # 待预测的点
 
for i in neighbors[0]:
    plt.plot([X[i][0], X_sample[0][0]], [X[i][1], X_sample[0][1]],
            'k--', linewidth=0.6)  # 预测点与距离最近的5个样本的连线
# plt.savefig('knn_predict.png')
plt.show()
```

## 示例：使用k-近邻算法进行回归拟合
分类问题的预测值是离散的，我们也可以用k-近邻算法在连续区间内对数值进行预测，进行回归拟合。在scikit-learn里，使用k-近邻算法进行回归拟合的算法是sklearn.neighbors.KNeighborsRegressor类。

### 生成数据集，它在余弦曲线的基础上加入了噪声：
```python
import numpy as np
n_dots = 40
X = 5 * np.random.rand(n_dots, 1)
y = np.cos(X).ravel()
 
# 添加一些噪声
y += 0.2 * np.random.rand(n_dots) - 0.1
```

### 使用KNeighborsRegressor来训练模型：
```python
# 训练模型
from sklearn.neighbors import KNeighborsRegressor
k = 5
knn = KNeighborsRegressor(k)
knn.fit(X, y);
```
我们要怎么样来进行回归拟合呢？

一个方法是，在X轴上的指定区间内生成足够多的点，针对这些足够密集的点，使用训练出来的模型进行预测，得到预测值y_pred，然后在坐标轴上，把所有的预测点连接起来，这样就画出了拟合曲线。

我们针对足够密集的点进行预测：
```python
# 生成足够密集的点并进行预测
T = np.linspace(0, 5, 500)[:, np.newaxis]
y_pred = knn.predict(T)
knn.score(X, y)
```
可以用score（）方法计算拟合曲线针对训练样本的拟合准确性，在笔者的环境下输出结果为：
```python
0.99000494130215722      # 在读者的环境运行时，值会略有差异
```
把这些预测点连起来，构成拟合曲线：
```python
# 画出拟合曲线
plt.figure(figsize=(16, 10), dpi=144)
plt.scatter(X, y, c='g', label='data', s=100)          # 画出训练样本
plt.plot(T, y_pred, c='k', label='prediction', lw=4)   # 画出拟合曲线
plt.axis('tight')
plt.title("KNeighborsRegressor (k = %i)" % k)
plt.show()
```

## 下面以Scikit-learn内置的鸢尾花数据集为例，演示k-近邻分类模型的使用。
在研究机器学习算法时，通常从样本集中随机抽取部分样本作为测试集，其余样本作为训练集。Scikit-learn提供了train_test_split( )，使用它可以将样本集按照指定的比例随机分割成训练集和测试集。下面使用10%的样本作为测试集对鸢尾花进行分类，模型精度在93%左右。
```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier # 导入k-近邻分类模型
from sklearn.model_selection import train_test_split as tsplit
X, y = load_iris(return_X_y=True) # 获取鸢尾花数据集，返回样本集和标签集
X_train, X_test, y_train, y_test = tsplit(X, y, test_size=0.1) # 拆分
m = KNeighborsClassifier() # 实例化模型。n_neighbors参数指定k值，默认k=5
m.fit(X_train, y_train) # 模型训练
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                 weights='uniform')
t=m.score(X_test, y_test) # 模型测试精度（介于0~1）
print(t)

```
k-近邻分类模型理论成熟，计算精度高，对异常值不敏感，但相对其他分类模型而言计算量大，占用内存多。由于k-近邻分类模型主要靠周围有限的邻近样本，而不是靠判别类域的方法来确定所属类别，因此对于类域交叉或重叠较多的待分类样本集来说，k-近邻分类模型相较其他模型来说更为合适。k-近邻分类模型比较适用于样本数量比较大的自动分类，当样本数量较小时容易产生错误分类。




