---
layout: post
categories: [Python,scikitlearn]
description: none
keywords: Python
---
# k-近邻算法
k-近邻算法是一个有监督的机器学习算法。k-近邻算法也称为knn算法，可以解决分类问题，也可以解决回归问题。

涵盖的内容如下：
- k-近邻算法的原理、优缺点及参数k取值对算法性能的影响；
- 使用k-近邻算法处理分类问题的示例；
- 使用k-近邻算法解决回归问题的示例；
- 使用k-近邻算法进行糖尿病检测的实例；
- 基于统计学的特征选择；
- 扩展阅读之k-近邻算法性能优化；
- 扩展阅读之卡方检测及F值检测。

## 算法原理
k-近邻算法的核心思想是未标记样本的类别，由距离其最近的k个邻居投票来决定。

假设，我们有一个已经标记的数据集，即已经知道了数据集中每个样本所属的类别。此时，有一个未标记的数据样本，我们的任务是预测出这个数据样本所属的类别。k-近邻算法的原理是，计算待标记的数据样本和数据集中每个样本的距离，取距离最近的k个样本。待标记的数据样本所属的类别，就由这k个距离最近的样本投票产生。

假设X_test为待标记的数据样本，X_train为已标记的数据集，算法原理的伪代码如下：
- 遍历X_train中的所有样本，计算每个样本与X_test的距离，并把距离保存在Distance数组中。
- 对Distance数组进行排序，取距离最近的k个点，记为X_knn。
- 在X_knn中统计每个类别的个数，即class0在X_knn中有几个样本，class1在X_knn中有几个样本等。
- 待标记样本的类别，就是在X_knn中样本个数最多的那个类别。

## 算法优缺点
优点：准确性高，对异常值和噪声有较高的容忍度。缺点：计算量较大，对内存的需求也较大。从算法原理可以看出来，每次对一个未标记样本进行分类时，都需要全部计算一遍距离。

## 算法参数
其算法参数是k，参数选择需要根据数据来决定。k值越大，模型的偏差越大，对噪声数据越不敏感，当k值很大时，可能造成模型欠拟合；k值越小，模型的方差就会越大，当k值太小，就会造成模型过拟合。

## 算法的变种
k-近邻算法有一些变种，其中之一就是可以增加邻居的权重。默认情况下，在计算距离时，都是使用相同权重。实际上，我们可以针对不同的邻居指定不同的距离权重，如距离越近权重越高。这个可以通过指定算法的weights参数来实现。

另外一个变种是，使用一定半径内的点取代距离最近的k个点。在scikit-learn里，RadiusNeighborsClassifier类实现了这个算法的变种。当数据采样不均匀时，该算法变种可以取得更好的性能。

## 示例:使用k-近邻算法进行分类
在scikit-learn里，使用k-近邻算法进行分类处理的是sklearn.neighbors.KNeighbors Classifier类。

### 生成已标记的数据集：
```shell
from sklearn.datasets import make_blobs
# 生成数据
centers = [[-2, 2], [2, 2], [0, 4]]
X, y = make_blobs(n_samples=60, centers=centers,
                  random_state=0, cluster_std=0.60)

```
我们使用sklearn.datasets包下的make_blobs（）函数来生成数据集，上面代码中，生成60个训练样本，这60个样本分布在以centers参数指定中心点周围。
cluster_std是标准差，用来指明生成的点分布的松散程度。生成的训练数据集放在变量X里面，数据集的类别标记放在y里面。
使用matplotlib库，它可以很容易地把生成的点画出来：
```shell
# 画出数据
plt.figure(figsize=(16, 10), dpi=144)
c = np.array(centers)
plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='cool'); # 画出样本
plt.scatter(c[:, 0], c[:, 1], s=100, marker='^', c='orange'); # 画出中心点
plt.show()
```


### 使用KNeighborsClassifier来对算法进行训练，我们选择的参数是k=5：
```shell
from sklearn.neighbors import KNeighborsClassifier
# 模型训练
k = 5
clf = KNeighborsClassifier(n_neighbors=k)
clf.fit(X, y);
```

### 对一个新的样本进行预测：
```shell
# 进行预测
X_sample = np.array([[0, 2]])
y_sample = clf.predict(X_sample)
neighbors = clf.kneighbors(X_sample, return_distance=False)
```
我们要预测的样本是[0，2]，使用kneighbors（）方法，把这个样本周围距离最近的5个点取出来。取出来的点是训练样本X里的索引，从0开始计算。

### 把待预测的样本以及和其最近的5个点标记出来：
```shell
# 画出示意图
plt.figure(figsize=(16,10), dpi=144)
c = np.array(centers)
plt.scatter(X[:,0], X[:,1], c=y, s=100, cmap='cool') # 出样本
plt.scatter(c[:,0], c[:,1], s=100, marker='^',c='k') # 中心点
plt.scatter(X_sample[0][0], X_sample[0][1],c=y_sample, marker="x",
           s=100, cmap='cool')      # 待预测的点
 
for i in neighbors[0]:
    plt.plot([X[i][0], X_sample[0][0]], [X[i][1], X_sample[0][1]],
            'k--', linewidth=0.6)  # 预测点与距离最近的5个样本的连线
# plt.savefig('knn_predict.png')
plt.show()
```

## 完整代码如下
```shell
from sklearn.datasets import make_blobs
from sklearn.neighbors import KNeighborsClassifier
from matplotlib import pyplot as plt
import numpy as np
# 生成数据
centers = [[-2, 2], [2, 2], [0, 4]]
X, y = make_blobs(n_samples=60, centers=centers,
                  random_state=0, cluster_std=0.60)

print(X)
print("---------")
print(y)

# 模型训练
k = 5
clf = KNeighborsClassifier(n_neighbors=k)
clf.fit(X, y);

# 进行预测
X_sample = np.array([[0, 2]])
y_sample = clf.predict(X_sample)
neighbors = clf.kneighbors(X_sample, return_distance=False)

print(X_sample)
print(y_sample)
print(neighbors)


# 画出示意图
plt.figure(figsize=(16,10), dpi=144)
c = np.array(centers)
plt.scatter(X[:,0], X[:,1], c=y, s=100, cmap='cool') # 出样本
plt.scatter(c[:,0], c[:,1], s=100, marker='^',c='k') # 中心点
plt.scatter(X_sample[0][0], X_sample[0][1],c=y_sample, marker="x",
           s=100, cmap='cool')      # 待预测的点
 
for i in neighbors[0]:
    plt.plot([X[i][0], X_sample[0][0]], [X[i][1], X_sample[0][1]],
            'k--', linewidth=0.6)  # 预测点与距离最近的5个样本的连线
# plt.savefig('knn_predict.png')
plt.show()
```

## 示例：使用k-近邻算法进行回归拟合
分类问题的预测值是离散的，我们也可以用k-近邻算法在连续区间内对数值进行预测，进行回归拟合。在scikit-learn里，使用k-近邻算法进行回归拟合的算法是sklearn.neighbors.KNeighborsRegressor类。

### 生成数据集，它在余弦曲线的基础上加入了噪声：
```shell
import numpy as np
n_dots = 40
X = 5 * np.random.rand(n_dots, 1)
y = np.cos(X).ravel()
 
# 添加一些噪声
y += 0.2 * np.random.rand(n_dots) - 0.1
```

### 使用KNeighborsRegressor来训练模型：
```shell
# 训练模型
from sklearn.neighbors import KNeighborsRegressor
k = 5
knn = KNeighborsRegressor(k)
knn.fit(X, y);
```
我们要怎么样来进行回归拟合呢？

一个方法是，在X轴上的指定区间内生成足够多的点，针对这些足够密集的点，使用训练出来的模型进行预测，得到预测值y_pred，然后在坐标轴上，把所有的预测点连接起来，这样就画出了拟合曲线。

我们针对足够密集的点进行预测：
```shell
# 生成足够密集的点并进行预测
T = np.linspace(0, 5, 500)[:, np.newaxis]
y_pred = knn.predict(T)
knn.score(X, y)
```
可以用score（）方法计算拟合曲线针对训练样本的拟合准确性，在笔者的环境下输出结果为：
```shell
0.99000494130215722      # 在读者的环境运行时，值会略有差异
```
把这些预测点连起来，构成拟合曲线：
```shell
# 画出拟合曲线
plt.figure(figsize=(16, 10), dpi=144)
plt.scatter(X, y, c='g', label='data', s=100)          # 画出训练样本
plt.plot(T, y_pred, c='k', label='prediction', lw=4)   # 画出拟合曲线
plt.axis('tight')
plt.title("KNeighborsRegressor (k = %i)" % k)
plt.show()
```






