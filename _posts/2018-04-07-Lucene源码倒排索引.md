---
layout: post
categories: [Lucene]
description: none
keywords: Lucene
---
# Lucene源码倒排索引

## 倒排索引的写过程
lucene将倒排索引的信息写入.tim和.tip文件，这部分代码也是lucene最核心的一部分。倒排索引的写过程从BlockTreeTermsWriter的write函数开始，
BlockTreeTermsWriter::write
```
  public void write(Fields fields) throws IOException {

    String lastField = null;
    for(String field : fields) {
      lastField = field;

      Terms terms = fields.terms(field);
      if (terms == null) {
        continue;
      }
      List<PrefixTerm> prefixTerms = null;

      TermsEnum termsEnum = terms.iterator();
      TermsWriter termsWriter = new TermsWriter(fieldInfos.fieldInfo(field));
      int prefixTermUpto = 0;

      while (true) {
        BytesRef term = termsEnum.next();
        termsWriter.write(term, termsEnum, null);
      }
      termsWriter.finish();
    }
  }
```
遍历每个域，首先通过terms函数根据field名返回一个FreqProxTerms，包含了该域的所有Term；接下来fieldInfo根据域名返回域信息，并以此创建一个TermsWriter，TermsWriter是倒排索引写的主要类，接下来依次取出FreqProxTerms中的每个term，并调用TermsWriter的write函数写入.tim文件，并创建对应的索引信息，最后通过TermsWriter的finish函数将索引信息写入.tip文件中，下面依次来看。

BlockTreeTermsWriter::write->TermsWriter::write
```
    public void write(BytesRef text, TermsEnum termsEnum, PrefixTerm prefixTerm) throws IOException {

      BlockTermState state = postingsWriter.writeTerm(text, termsEnum, docsSeen);
      if (state != null) {

        pushTerm(text);

        PendingTerm term = new PendingTerm(text, state, prefixTerm);
        pending.add(term);

        if (prefixTerm == null) {
          sumDocFreq += state.docFreq;
          sumTotalTermFreq += state.totalTermFreq;
          numTerms++;
          if (firstPendingTerm == null) {
            firstPendingTerm = term;
          }
          lastPendingTerm = term;
        }
      }
    }
```
TermsWriter的write函数一次处理一个Term。postingsWriter是Lucene50PostingsWriter。write函数首先通过Lucene50PostingsWriter的writeTerm函数记录每个Term以及对应文档的相应信息。
成员变量pending是一个PendingEntry列表，PendingEntry用来保存一个Term或者是一个Block，pending列表用来保存多个待处理的Term。
pushTerm是write里的核心函数，用于具体处理一个Term，后面详细来看。write函数的最后统计文档频和词频信息并记录到sumDocFreq和sumTotalTermFreq两个成员变量中。

BlockTreeTermsWriter::write->TermsWriter::write->Lucene50PostingsWriter::writeTerm
```
  public final BlockTermState writeTerm(BytesRef term, TermsEnum termsEnum, FixedBitSet docsSeen) throws IOException {
    startTerm();
    postingsEnum = termsEnum.postings(postingsEnum, enumFlags);

    int docFreq = 0;
    long totalTermFreq = 0;
    while (true) {
      int docID = postingsEnum.nextDoc();
      if (docID == PostingsEnum.NO_MORE_DOCS) {
        break;
      }
      docFreq++;
      docsSeen.set(docID);
      int freq;
      if (writeFreqs) {
        freq = postingsEnum.freq();
        totalTermFreq += freq;
      } else {
        freq = -1;
      }
      startDoc(docID, freq);

      if (writePositions) {
        for(int i=0;i<freq;i++) {
          int pos = postingsEnum.nextPosition();
          BytesRef payload = writePayloads ? postingsEnum.getPayload() : null;
          int startOffset;
          int endOffset;
          if (writeOffsets) {
            startOffset = postingsEnum.startOffset();
            endOffset = postingsEnum.endOffset();
          } else {
            startOffset = -1;
            endOffset = -1;
          }
          addPosition(pos, payload, startOffset, endOffset);
        }
      }

      finishDoc();
    }

    if (docFreq == 0) {
      return null;
    } else {
      BlockTermState state = newTermState();
      state.docFreq = docFreq;
      state.totalTermFreq = writeFreqs ? totalTermFreq : -1;
      finishTerm(state);
      return state;
    }
  }
```
startTerm设置.doc、.pos和.pay三个文件的指针。postings函数创建FreqProxPostingsEnum或者FreqProxDocsEnum，内部封装了FreqProxTermsWriterPerField，即第五章中每个PerField的termsHashPerField成员变量，termsHashPerField的内部保存了对应Field的所有Terms信息。
writeTerm函数接下来通过nextDoc获得下一个文档ID，获得freq词频，并累加到totalTermFreq（总词频）中。再调用startDoc记录文档的信息。addPosition函数记录词的位置、偏移和payload信息，必要时写入文件中。finishDoc记录文件指针等信息。然后创建BlockTermState，设置相应词频和文档频信息以最终返回。
writeTerm函数最后通过finishTerm写入文档信息至.doc文件，写入位置信息至.pos文件。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm
```
    private void pushTerm(BytesRef text) throws IOException {

      int limit = Math.min(lastTerm.length(), text.length);
      int pos = 0;

      while (pos < limit && lastTerm.byteAt(pos) == text.bytes[text.offset+pos]) {
        pos++;
      }

      for(int i=lastTerm.length()-1;i>=pos;i--) {

        int prefixTopSize = pending.size() - prefixStarts[i];
        if (prefixTopSize >= minItemsInBlock) {
          writeBlocks(i+1, prefixTopSize);
          prefixStarts[i] -= prefixTopSize-1;
        }
      }

      if (prefixStarts.length < text.length) {
        prefixStarts = ArrayUtil.grow(prefixStarts, text.length);
      }

      for(int i=pos;i<text.length;i++) {
        prefixStarts[i] = pending.size();
      }

      lastTerm.copyBytes(text);
    }
```
lastTerm保存了上一次处理的Term。pushTerm函数的核心功能是计算一定的条件，当满足一定条件时，就表示pending列表中待处理的一个或者多个Term，需要保存为一个block，此时调用writeBlocks函数进行保存。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks
```
    void writeBlocks(int prefixLength, int count) throws IOException {
      int lastSuffixLeadLabel = -1;

      boolean hasTerms = false;
      boolean hasPrefixTerms = false;
      boolean hasSubBlocks = false;

      int start = pending.size()-count;
      int end = pending.size();
      int nextBlockStart = start;
      int nextFloorLeadLabel = -1;

      for (int i=start; i<end; i++) {

        PendingEntry ent = pending.get(i);
        int suffixLeadLabel;

        if (ent.isTerm) {
          PendingTerm term = (PendingTerm) ent;
          if (term.termBytes.length == prefixLength) {
            suffixLeadLabel = -1;
          } else {
            suffixLeadLabel = term.termBytes[prefixLength] & 0xff;
          }
        } else {
          PendingBlock block = (PendingBlock) ent;
          suffixLeadLabel = block.prefix.bytes[block.prefix.offset + prefixLength] & 0xff;
        }

        if (suffixLeadLabel != lastSuffixLeadLabel) {
          int itemsInBlock = i - nextBlockStart;
          if (itemsInBlock >= minItemsInBlock && end-nextBlockStart > maxItemsInBlock) {
            boolean isFloor = itemsInBlock < count;
            newBlocks.add(writeBlock(prefixLength, isFloor, nextFloorLeadLabel, nextBlockStart, i, hasTerms, hasPrefixTerms, hasSubBlocks));

            hasTerms = false;
            hasSubBlocks = false;
            hasPrefixTerms = false;
            nextFloorLeadLabel = suffixLeadLabel;
            nextBlockStart = i;
          }

          lastSuffixLeadLabel = suffixLeadLabel;
        }

        if (ent.isTerm) {
          hasTerms = true;
          hasPrefixTerms |= ((PendingTerm) ent).prefixTerm != null;
        } else {
          hasSubBlocks = true;
        }
      }

      if (nextBlockStart < end) {
        int itemsInBlock = end - nextBlockStart;
        boolean isFloor = itemsInBlock < count;
        newBlocks.add(writeBlock(prefixLength, isFloor, nextFloorLeadLabel, nextBlockStart, end, hasTerms, hasPrefixTerms, hasSubBlocks));
      }
      PendingBlock firstBlock = newBlocks.get(0);
      firstBlock.compileIndex(newBlocks, scratchBytes, scratchIntsRef);

      pending.subList(pending.size()-count, pending.size()).clear();
      pending.add(firstBlock);
      newBlocks.clear();
    }
```
hasTerms表示将要合并的项中是否含有Term（因为特殊情况下，合并的项只有子block）。
hasPrefixTerms表示是否有词的前缀，假设一直为false。
hasSubBlocks和hasTerms对应，表示将要合并的项中是否含有子block。
start和end的规定了需要合并的Term或Block在待处理的pending列表中的范围。
writeBlocks函数接下来遍历pending列表中每个待处理的Term或者Block，suffixLeadLabel保存了树中某个节点下的各个Term的byte，lastSuffixLeadLabel则是对应的最后一个不同的byte，检查所有项中是否有Term和子block，并对hasTerms和hasSubBlocks进行相应的设置。如果pending中的Term或block太多，大于minItemsInBlock和maxItemsInBlock计算出来的阈值，就会调用writeBlock写成一个block，最后也会写一次。
writeBlocks函数接下来通过compileIndex函数将一个block的信息写入FST结构中（保存在其成员变量index中），FST是有限状态机的缩写，其实就是将一棵树的信息保存在其自身的结构中，而这颗树是由所有Term的每个byte形成的，后面来看。
writeBlocks函数最后清空被保存的一部分pending列表，并添加刚刚创建的block到pending列表中。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->writeBlock
```
    private PendingBlock writeBlock(int prefixLength, boolean isFloor, int floorLeadLabel, int start, int end, boolean hasTerms, boolean hasPrefixTerms, boolean hasSubBlocks) throws IOException {

      long startFP = termsOut.getFilePointer();

      boolean hasFloorLeadLabel = isFloor && floorLeadLabel != -1;

      final BytesRef prefix = new BytesRef(prefixLength + (hasFloorLeadLabel ? 1 : 0));
      System.arraycopy(lastTerm.get().bytes, 0, prefix.bytes, 0, prefixLength);
      prefix.length = prefixLength;

      int numEntries = end - start;
      int code = numEntries << 1;
      if (end == pending.size()) {
        code |= 1;
      }
      termsOut.writeVInt(code);

      boolean isLeafBlock = hasSubBlocks == false && hasPrefixTerms == false;
      final List<FST<BytesRef>> subIndices;
      boolean absolute = true;

      if (isLeafBlock) {
        subIndices = null;
        for (int i=start;i<end;i++) {
          PendingEntry ent = pending.get(i);

          PendingTerm term = (PendingTerm) ent;
          BlockTermState state = term.state;
          final int suffix = term.termBytes.length - prefixLength;

          suffixWriter.writeVInt(suffix);
          suffixWriter.writeBytes(term.termBytes, prefixLength, suffix);
          statsWriter.writeVInt(state.docFreq);
          if (fieldInfo.getIndexOptions() != IndexOptions.DOCS) {
            statsWriter.writeVLong(state.totalTermFreq - state.docFreq);
          }

          postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
          for (int pos = 0; pos < longsSize; pos++) {
            metaWriter.writeVLong(longs[pos]);
          }
          bytesWriter.writeTo(metaWriter);
          bytesWriter.reset();
          absolute = false;
        }
      } else {
        ...
      }

      termsOut.writeVInt((int) (suffixWriter.getFilePointer() << 1) | (isLeafBlock ? 1:0));
      suffixWriter.writeTo(termsOut);
      suffixWriter.reset();

      termsOut.writeVInt((int) statsWriter.getFilePointer());
      statsWriter.writeTo(termsOut);
      statsWriter.reset();

      termsOut.writeVInt((int) metaWriter.getFilePointer());
      metaWriter.writeTo(termsOut);
      metaWriter.reset();

      if (hasFloorLeadLabel) {
        prefix.bytes[prefix.length++] = (byte) floorLeadLabel;
      }

      return new PendingBlock(prefix, startFP, hasTerms, isFloor, floorLeadLabel, subIndices);
    }

```
termsOut封装了.tim文件的输出流，其实是FSIndexOutput，其getFilePointer函数返回的startFP保存了该文件可以插入的指针。
writeBlock函数首先提取相同的前缀，例如需要写为一个block的Term有aaa，aab，aac，则相同的前缀为aa，保存在类型为BytesRef的prefix中，BytesRef用于封装一个byte数组。
numEntries保存了本次需要写入多少个Term或者Block，code封装了numEntries的信息，并在最后一个bit表示后面是否还有。然后将code写入.tim文件中。
isLeafBlock表示是否是叶子节点。bytesWriter、suffixWriter、statsWriter、metaWriter在内存中模拟文件。
writeBlock函数接下来遍历需要写入的Term或者Block，suffix表示最后取出的不同字幕的长度，例如aaa，aab，aac则suffix为1，首先写入该长度suffix，最终写入suffixWriter中的为a、b、c。再往下往statsWriter中写入词频和文档频率。
再往下postingsWriter是Lucene50PostingsWriter，encodeTerm函数在longs中保存了.doc、.pos和.pay中文件指针的偏移，然后singletonDocID、lastPosBlockOffset、skipOffset等信息保存在bytesWriter中，再将longs的指针写入metaWriter中，最后把其余信息写入bytesWriter中。
再往下调用bytesWriter、suffixWriter、statsWriter、metaWriter的writeTo函数将内存中的数据写入.tim文件中。
writeBlock函数最后创建PendingBlock并返回，PendingBlock封装了本次写入的各个Term或者子Block的信息。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->writeBlock

```
    private PendingBlock writeBlock(int prefixLength, boolean isFloor, int floorLeadLabel, int start, int end, boolean hasTerms, boolean hasPrefixTerms, boolean hasSubBlocks) throws IOException {

      long startFP = termsOut.getFilePointer();

      boolean hasFloorLeadLabel = isFloor && floorLeadLabel != -1;

      final BytesRef prefix = new BytesRef(prefixLength + (hasFloorLeadLabel ? 1 : 0));
      System.arraycopy(lastTerm.get().bytes, 0, prefix.bytes, 0, prefixLength);
      prefix.length = prefixLength;

      int numEntries = end - start;
      int code = numEntries << 1;
      if (end == pending.size()) {
        code |= 1;
      }
      termsOut.writeVInt(code);

      boolean isLeafBlock = hasSubBlocks == false && hasPrefixTerms == false;
      final List<FST<BytesRef>> subIndices;
      boolean absolute = true;

      if (isLeafBlock) {
        ...
      } else {
        subIndices = new ArrayList<>();
        boolean sawAutoPrefixTerm = false;
        for (int i=start;i<end;i++) {
          PendingEntry ent = pending.get(i);
          if (ent.isTerm) {
            PendingTerm term = (PendingTerm) ent;
            BlockTermState state = term.state;
            final int suffix = term.termBytes.length - prefixLength;
            if (minItemsInAutoPrefix == 0) {
              suffixWriter.writeVInt(suffix << 1);
              suffixWriter.writeBytes(term.termBytes, prefixLength, suffix);
            } else {
              code = suffix<<2;
              int floorLeadEnd = -1;
              if (term.prefixTerm != null) {
                sawAutoPrefixTerm = true;
                PrefixTerm prefixTerm = term.prefixTerm;
                floorLeadEnd = prefixTerm.floorLeadEnd;

                if (prefixTerm.floorLeadStart == -2) {
                  code |= 2;
                } else {
                  code |= 3;
                }
              }
              suffixWriter.writeVInt(code);
              suffixWriter.writeBytes(term.termBytes, prefixLength, suffix);
              if (floorLeadEnd != -1) {
                suffixWriter.writeByte((byte) floorLeadEnd);
              }
            }

            statsWriter.writeVInt(state.docFreq);
            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS) {
              statsWriter.writeVLong(state.totalTermFreq - state.docFreq);
            }
            postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
            for (int pos = 0; pos < longsSize; pos++) {
              metaWriter.writeVLong(longs[pos]);
            }
            bytesWriter.writeTo(metaWriter);
            bytesWriter.reset();
            absolute = false;
          } else {
            PendingBlock block = (PendingBlock) ent;
            final int suffix = block.prefix.length - prefixLength;
            if (minItemsInAutoPrefix == 0) {
              suffixWriter.writeVInt((suffix<<1)|1);
            } else {
              suffixWriter.writeVInt((suffix<<2)|1);
            }
            suffixWriter.writeBytes(block.prefix.bytes, prefixLength, suffix);
            suffixWriter.writeVLong(startFP - block.fp);
            subIndices.add(block.index);
          }
        }
      }

      termsOut.writeVInt((int) (suffixWriter.getFilePointer() << 1) | (isLeafBlock ? 1:0));
      suffixWriter.writeTo(termsOut);
      suffixWriter.reset();

      termsOut.writeVInt((int) statsWriter.getFilePointer());
      statsWriter.writeTo(termsOut);
      statsWriter.reset();

      termsOut.writeVInt((int) metaWriter.getFilePointer());
      metaWriter.writeTo(termsOut);
      metaWriter.reset();

      if (hasFloorLeadLabel) {
        prefix.bytes[prefix.length++] = (byte) floorLeadLabel;
      }

      return new PendingBlock(prefix, startFP, hasTerms, isFloor, floorLeadLabel, subIndices);
    }
```
第二种情况表示要写入的不是叶子节点，如果是Term，和第一部分一样，如果是一个子block，写入子block的相应信息，最后创建的PendingBlock需要封装每个Block对应的FST结构，即subIndices。

writeBlocks函数调用完writeBlock函数后将pending列表中的Term或者Block写入.tim文件中，接下来要通过PendingBlock的compileIndex函数针对刚刚写入.tim文件中的Term创建索引信息，最后要将这些信息写入.tip文件中，用于查找。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex
```
    public void compileIndex(List<PendingBlock> blocks, RAMOutputStream scratchBytes, IntsRefBuilder scratchIntsRef) throws IOException {

      scratchBytes.writeVLong(encodeOutput(fp, hasTerms, isFloor));
      if (isFloor) {
        scratchBytes.writeVInt(blocks.size()-1);
        for (int i=1;i<blocks.size();i++) {
          PendingBlock sub = blocks.get(i);
          scratchBytes.writeByte((byte) sub.floorLeadByte);
          scratchBytes.writeVLong((sub.fp - fp) << 1 | (sub.hasTerms ? 1 : 0));
        }
      }

      final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
      final Builder<BytesRef> indexBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
                                                           0, 0, true, false, Integer.MAX_VALUE,
                                                           outputs, false,
                                                           PackedInts.COMPACT, true, 15);

      final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()];
      scratchBytes.writeTo(bytes, 0);
      indexBuilder.add(Util.toIntsRef(prefix, scratchIntsRef), new BytesRef(bytes, 0, bytes.length));
      scratchBytes.reset();

      for(PendingBlock block : blocks) {
        if (block.subIndices != null) {
          for(FST<BytesRef> subIndex : block.subIndices) {
            append(indexBuilder, subIndex, scratchIntsRef);
          }
          block.subIndices = null;
        }
      }
      index = indexBuilder.finish();
    }
```
fp是对应.tim文件的指针，encodeOutput函数将fp、hasTerms和isFloor信息封装到一个长整型中，然后将该长整型存入scratchBytes中。compileIndex函数接下来创建Builder，用于构造索引树，再往下将scratchBytes中的数据存入byte数组bytes中。
compileIndex最核心的部分是通过Builder的add函数依次将Term或者Term的部分前缀添加到一颗树中，由frontier数组维护，进而添加到FST中。compileIndex最后通过Builder的finish函数将add添加后的FST树中的信息写入缓存中，后续添加到.tip文件里。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::Builder
```
  public Builder(FST.INPUT_TYPE inputType, int minSuffixCount1, int minSuffixCount2, boolean doShareSuffix, boolean doShareNonSingletonNodes, int shareMaxTailLength, Outputs<T> outputs, boolean doPackFST, float acceptableOverheadRatio, boolean allowArrayArcs, int bytesPageBits) {

    this.minSuffixCount1 = minSuffixCount1;
    this.minSuffixCount2 = minSuffixCount2;
    this.doShareNonSingletonNodes = doShareNonSingletonNodes;
    this.shareMaxTailLength = shareMaxTailLength;
    this.doPackFST = doPackFST;
    this.acceptableOverheadRatio = acceptableOverheadRatio;
    this.allowArrayArcs = allowArrayArcs;
    fst = new FST<>(inputType, outputs, doPackFST, acceptableOverheadRatio, bytesPageBits);
    bytes = fst.bytes;
    if (doShareSuffix) {
      dedupHash = new NodeHash<>(fst, bytes.getReverseReader(false));
    } else {
      dedupHash = null;
    }
    NO_OUTPUT = outputs.getNoOutput();

    final UnCompiledNode<T>[] f = (UnCompiledNode<T>[]) new UnCompiledNode[10];
    frontier = f;
    for(int idx=0;idx<frontier.length;idx++) {
      frontier[idx] = new UnCompiledNode<>(this, idx);
    }
  }
```
Builder的构造函数主要是创建了一个FST，并初始化frontier数组，frontier数组中的每个元素UnCompiledNode代表树中的每个节点。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::add
```
  public void add(IntsRef input, T output) throws IOException {

    ...

    int pos1 = 0;
    int pos2 = input.offset;
    final int pos1Stop = Math.min(lastInput.length(), input.length);
    while(true) {
      frontier[pos1].inputCount++;
      if (pos1 >= pos1Stop || lastInput.intAt(pos1) != input.ints[pos2]) {
        break;
      }
      pos1++;
      pos2++;
    }
    final int prefixLenPlus1 = pos1+1;

    if (frontier.length < input.length+1) {
      final UnCompiledNode<T>[] next = ArrayUtil.grow(frontier, input.length+1);
      for(int idx=frontier.length;idx<next.length;idx++) {
        next[idx] = new UnCompiledNode<>(this, idx);
      }
      frontier = next;
    }

    freezeTail(prefixLenPlus1);

    for(int idx=prefixLenPlus1;idx<=input.length;idx++) {
      frontier[idx-1].addArc(input.ints[input.offset + idx - 1],
                             frontier[idx]);
      frontier[idx].inputCount++;
    }

    final UnCompiledNode<T> lastNode = frontier[input.length];
    if (lastInput.length() != input.length || prefixLenPlus1 != input.length + 1) {
      lastNode.isFinal = true;
      lastNode.output = NO_OUTPUT;
    }

    for(int idx=1;idx<prefixLenPlus1;idx++) {
      final UnCompiledNode<T> node = frontier[idx];
      final UnCompiledNode<T> parentNode = frontier[idx-1];

      final T lastOutput = parentNode.getLastOutput(input.ints[input.offset + idx - 1]);

      final T commonOutputPrefix;
      final T wordSuffix;

      if (lastOutput != NO_OUTPUT) {
        commonOutputPrefix = fst.outputs.common(output, lastOutput);
        wordSuffix = fst.outputs.subtract(lastOutput, commonOutputPrefix);
        parentNode.setLastOutput(input.ints[input.offset + idx - 1], commonOutputPrefix);
        node.prependOutput(wordSuffix);
      } else {
        commonOutputPrefix = wordSuffix = NO_OUTPUT;
      }

      output = fst.outputs.subtract(output, commonOutputPrefix);
    }

    if (lastInput.length() == input.length && prefixLenPlus1 == 1+input.length) {
      lastNode.output = fst.outputs.merge(lastNode.output, output);
    } else {
      frontier[prefixLenPlus1-1].setLastOutput(input.ints[input.offset + prefixLenPlus1-1], output);
    }
    lastInput.copyInts(input);
  }
```
add函数首先计算和上一个字符串的共同前缀，prefixLenPlus1表示FST数中的相同前缀的长度，如果存在，后面就需要进行相应的合并。接下来通过for循环调用addArc函数依次添加input即Term中的每个byte至frontier中，形成一个FST树，由frontier数组维护，然后设置frontier数组中的最后一个UnCompiledNode，将isFinal标志位设为true。add函数最后将output中的数据（文件指针等信息）存入本次frontier数组中最前面的一个UnCompiledNode中，并设置lastInput为本次的input。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::add->freezeTail
```
  private void freezeTail(int prefixLenPlus1) throws IOException {
    final int downTo = Math.max(1, prefixLenPlus1);
    for(int idx=lastInput.length(); idx >= downTo; idx--) {

      boolean doPrune = false;
      boolean doCompile = false;

      final UnCompiledNode<T> node = frontier[idx];
      final UnCompiledNode<T> parent = frontier[idx-1];

      if (node.inputCount < minSuffixCount1) {
        doPrune = true;
        doCompile = true;
      } else if (idx > prefixLenPlus1) {
        if (parent.inputCount < minSuffixCount2 || (minSuffixCount2 == 1 && parent.inputCount == 1 && idx > 1)) {
          doPrune = true;
        } else {
          doPrune = false;
        }
        doCompile = true;
      } else {
        doCompile = minSuffixCount2 == 0;
      }

      if (node.inputCount < minSuffixCount2 || (minSuffixCount2 == 1 && node.inputCount == 1 && idx > 1)) {
        for(int arcIdx=0;arcIdx<node.numArcs;arcIdx++) {
          final UnCompiledNode<T> target = (UnCompiledNode<T>) node.arcs[arcIdx].target;
          target.clear();
        }
        node.numArcs = 0;
      }

      if (doPrune) {
        node.clear();
        parent.deleteLast(lastInput.intAt(idx-1), node);
      } else {

        if (minSuffixCount2 != 0) {
          compileAllTargets(node, lastInput.length()-idx);
        }
        final T nextFinalOutput = node.output;

        final boolean isFinal = node.isFinal || node.numArcs == 0;

        if (doCompile) {
          parent.replaceLast(lastInput.intAt(idx-1),
                             compileNode(node, 1+lastInput.length()-idx),
                             nextFinalOutput,
                             isFinal);
        } else {
          parent.replaceLast(lastInput.intAt(idx-1),
                             node,
                             nextFinalOutput,
                             isFinal);
          frontier[idx] = new UnCompiledNode<>(this, idx);
        }
      }
    }
  }
```
freezeTail函数的核心功能是将不会再变化的节点通过compileNode函数添加到FST结构中。
replaceLast函数设置父节点对应的参数，例如其子节点在bytes中的位置target，是否为最后一个节点isFinal等等。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::add->freezeTail->compileNode
```
  private CompiledNode compileNode(UnCompiledNode<T> nodeIn, int tailLength) throws IOException {
    final long node;
    long bytesPosStart = bytes.getPosition();
    if (dedupHash != null && (doShareNonSingletonNodes || nodeIn.numArcs <= 1) && tailLength <= shareMaxTailLength) {
      if (nodeIn.numArcs == 0) {
        node = fst.addNode(this, nodeIn);
        lastFrozenNode = node;
      } else {
        node = dedupHash.add(this, nodeIn);
      }
    } else {
      node = fst.addNode(this, nodeIn);
    }

    long bytesPosEnd = bytes.getPosition();
    if (bytesPosEnd != bytesPosStart) {
      lastFrozenNode = node;
    }

    nodeIn.clear();

    final CompiledNode fn = new CompiledNode();
    fn.node = node;
    return fn;
  }
```
compileNode的核心部分是调用FST的addNode函数添加节点。dedupHash是一个hash缓存，这里不管它。如果bytesPosEnd不等于bytesPosStart，表示有节点写入bytes中了，设置lastFrozenNode为当前node（其实是bytes中的缓存指针位置）。compileNode函数最后创建CompiledNode，设置其中的node并返回。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::add->freezeTail->compileNode->FST::addNode
```
  long addNode(Builder<T> builder, Builder.UnCompiledNode<T> nodeIn) throws IOException {
    T NO_OUTPUT = outputs.getNoOutput();

    if (nodeIn.numArcs == 0) {
      if (nodeIn.isFinal) {
        return FINAL_END_NODE;
      } else {
        return NON_FINAL_END_NODE;
      }
    }

    final long startAddress = builder.bytes.getPosition();

    final boolean doFixedArray = shouldExpand(builder, nodeIn);
    if (doFixedArray) {
      if (builder.reusedBytesPerArc.length < nodeIn.numArcs) {
        builder.reusedBytesPerArc = new int[ArrayUtil.oversize(nodeIn.numArcs, 1)];
      }
    }

    builder.arcCount += nodeIn.numArcs;

    final int lastArc = nodeIn.numArcs-1;

    long lastArcStart = builder.bytes.getPosition();
    int maxBytesPerArc = 0;
    for(int arcIdx=0;arcIdx<nodeIn.numArcs;arcIdx++) {
      final Builder.Arc<T> arc = nodeIn.arcs[arcIdx];
      final Builder.CompiledNode target = (Builder.CompiledNode) arc.target;
      int flags = 0;

      if (arcIdx == lastArc) {
        flags += BIT_LAST_ARC;
      }

      if (builder.lastFrozenNode == target.node && !doFixedArray) {
        flags += BIT_TARGET_NEXT;
      }

      if (arc.isFinal) {
        flags += BIT_FINAL_ARC;
        if (arc.nextFinalOutput != NO_OUTPUT) {
          flags += BIT_ARC_HAS_FINAL_OUTPUT;
        }
      } else {

      }

      boolean targetHasArcs = target.node > 0;

      if (!targetHasArcs) {
        flags += BIT_STOP_NODE;
      } else if (inCounts != null) {
        inCounts.set((int) target.node, inCounts.get((int) target.node) + 1);
      }

      if (arc.output != NO_OUTPUT) {
        flags += BIT_ARC_HAS_OUTPUT;
      }

      builder.bytes.writeByte((byte) flags);
      writeLabel(builder.bytes, arc.label);

      if (arc.output != NO_OUTPUT) {
        outputs.write(arc.output, builder.bytes);
      }

      if (arc.nextFinalOutput != NO_OUTPUT) {
        outputs.writeFinalOutput(arc.nextFinalOutput, builder.bytes);
      }

      if (targetHasArcs && (flags & BIT_TARGET_NEXT) == 0) {
        builder.bytes.writeVLong(target.node);
      }

    }

    final long thisNodeAddress = builder.bytes.getPosition()-1;
    builder.bytes.reverse(startAddress, thisNodeAddress);

    builder.nodeCount++;
    final long node;
    node = thisNodeAddress;

    return node;
  }
```
首先判断如果是最后的节点，直接返回。接下来累加numArcs至arcCount中，统计节点arc个数。addNode函数接下来计算并设置标志位flags，然后将flags和label写入bytes中，label就是Term中的某个字母或者byte。addNode函数最后返回bytes即BytesStore中的位置。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::add->freezeTail->compileNode->NodeHash::addNode
```
  public long add(Builder<T> builder, Builder.UnCompiledNode<T> nodeIn) throws IOException {
    final long h = hash(nodeIn);
    long pos = h & mask;
    int c = 0;
    while(true) {
      final long v = table.get(pos);
      if (v == 0) {
        final long node = fst.addNode(builder, nodeIn);
        count++;
        table.set(pos, node);
        if (count > 2*table.size()/3) {
          rehash();
        }
        return node;
      } else if (nodesEqual(nodeIn, v)) {
        return v;
      }
      pos = (pos + (++c)) & mask;
    }
  }
```
dedupHash的add函数首先通过hash函数获得该node的hash值，遍历node内的每个arc，计算hash值。
该函数内部也是使用了FST的addNode函数添加节点，并在必要的时候通过rehash扩展hash数组。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::add->UnCompiledNode::addArc
```
    public void addArc(int label, Node target) {
      if (numArcs == arcs.length) {
        final Arc<T>[] newArcs = ArrayUtil.grow(arcs, numArcs+1);
        for(int arcIdx=numArcs;arcIdx<newArcs.length;arcIdx++) {
          newArcs[arcIdx] = new Arc<>();
        }
        arcs = newArcs;
      }
      final Arc<T> arc = arcs[numArcs++];
      arc.label = label;
      arc.target = target;
      arc.output = arc.nextFinalOutput = owner.NO_OUTPUT;
      arc.isFinal = false;
    }
```
addArc用来将一个Term里的字母或者byte添加到该节点UnCompiledNode的arcs数组中，开头的if语句用来扩充arcs数组，然后按照顺序获取arcs数组中的Arc，并存入label，传入的参数target指向下一个UnCompiledNode节点。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::finish
```
  public FST<T> finish() throws IOException {

    final UnCompiledNode<T> root = frontier[0];

    freezeTail(0);

    if (root.inputCount < minSuffixCount1 || root.inputCount < minSuffixCount2 || root.numArcs == 0) {
      if (fst.emptyOutput == null) {
        return null;
      } else if (minSuffixCount1 > 0 || minSuffixCount2 > 0) {
        return null;
      }
    } else {
      if (minSuffixCount2 != 0) {
        compileAllTargets(root, lastInput.length());
      }
    }
    fst.finish(compileNode(root, lastInput.length()).node);

    if (doPackFST) {
      return fst.pack(this, 3, Math.max(10, (int) (getNodeCount()/4)), acceptableOverheadRatio);
    } else {
      return fst;
    }
  }
```
finish函数开头的freezeTail函数传入的参数0，代表要处理frontier数组维护的所有节点，compileNode函数最后向bytes中写入根节点。最后的finish函数将FST的信息缓存到成员变量blocks中去，blocks是一个byte数组列表。

BlockTreeTermsWriter::write->TermsWriter::write->pushTerm->writeBlocks->PendingBlock::compileIndex->Builder::finish->FST::finish
```
  void finish(long newStartNode) throws IOException {
    startNode = newStartNode;
    bytes.finish();
    cacheRootArcs();
  }

  public void finish() {
    if (current != null) {
      byte[] lastBuffer = new byte[nextWrite];
      System.arraycopy(current, 0, lastBuffer, 0, nextWrite);
      blocks.set(blocks.size()-1, lastBuffer);
      current = null;
    }
  }
```
回到BlockTreeTermsWriter的write函数中，接下来通过TermsWriter的finish函数将FST中的信息写入.tip文件中。

BlockTreeTermsWriter::write->TermsWriter::write->finish
```
    public void finish() throws IOException {
      if (numTerms > 0) {
        pushTerm(new BytesRef());
        pushTerm(new BytesRef());
        writeBlocks(0, pending.size());

        final PendingBlock root = (PendingBlock) pending.get(0);
        indexStartFP = indexOut.getFilePointer();
        root.index.save(indexOut);

        BytesRef minTerm = new BytesRef(firstPendingTerm.termBytes);
        BytesRef maxTerm = new BytesRef(lastPendingTerm.termBytes);

        fields.add(new FieldMetaData(fieldInfo,
                                     ((PendingBlock) pending.get(0)).index.getEmptyOutput(),
                                     numTerms,
                                     indexStartFP,
                                     sumTotalTermFreq,
                                     sumDocFreq,
                                     docsSeen.cardinality(),
                                     longsSize,
                                     minTerm, maxTerm));
      } else {

      }
    }
```
root.index.save(indexOut)就是将信息写入.tip文件中。

## 倒排索引的读过程
读过程，重点分析SegmentTermsEnum的seekExact函数。
首先看几个构造函数，先看SegmentCoreReaders的构造函数，在Lucene50PostingFormat的fieldsProducer函数中创建。

BlockTreeTermsReader::BlockTreeTermsReader
```
  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {
    boolean success = false;
    IndexInput indexIn = null;

    this.postingsReader = postingsReader;
    this.segment = state.segmentInfo.name;

    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);
    try {
      termsIn = state.directory.openInput(termsName, state.context);
      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
      ...
      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);
      indexIn = state.directory.openInput(indexName, state.context);
      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);
      CodecUtil.checksumEntireFile(indexIn);

      postingsReader.init(termsIn, state);
      CodecUtil.retrieveChecksum(termsIn);

      seekDir(termsIn, dirOffset);
      seekDir(indexIn, indexDirOffset);

      final int numFields = termsIn.readVInt();

      for (int i = 0; i < numFields; ++i) {
        final int field = termsIn.readVInt();
        final long numTerms = termsIn.readVLong();
        final int numBytes = termsIn.readVInt();
        final BytesRef rootCode = new BytesRef(new byte[numBytes]);
        termsIn.readBytes(rootCode.bytes, 0, numBytes);
        rootCode.length = numBytes;
        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();
        final long sumDocFreq = termsIn.readVLong();
        final int docCount = termsIn.readVInt();
        final int longsSize = termsIn.readVInt();
        BytesRef minTerm = readBytesRef(termsIn);
        BytesRef maxTerm = readBytesRef(termsIn);
        final long indexStartFP = indexIn.readVLong();
        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn, minTerm, maxTerm));
      }

      indexIn.close();
      success = true;
    } finally {

    }
  }
```
BlockTreeTermsReader的核心功能是打开.tim和.tip文件并创建输出流，然后创建FiledReader用于读取数据。
函数中的segment为段名，例如”_0”，state.segmentSuffix假设返回Lucene50_0，TERMS_EXTENSION默认为tim，因此segmentFileName构造文件名_0_Lucene50_0.tim。
directory对于cfs文件，返回Lucene50CompoundReader。
openInput函数返回SingleBufferImpl或者MultiBufferImpl，下面假设为SingleBufferImpl，termsIn封装了_0_Lucene50_0.tim文件的输出流。
checkIndexHeader检查头信息，和写过程的writeIndexHeader函数对应。
和.tim文件的打开过程类似，BlockTreeTermsReader的构造函数接下来打开_0_Lucene50_0.tip文件，检查头信息，同样调用openInput返回的indexIn封装了_0_Lucene50_0.tip文件的输出流。
seekDir最终调用SingleBufferImpl的父类ByteBufferIndexInput的seek函数，改变DirectByteBufferR的position指针的位置，用于略过一些头信息。然后从tim文件中读取并设置域的相应信息。最后创建FieldReader并返回。
BlockTreeTermsReader::BlockTreeTermsReader->FieldReader::FieldReader
```
  FieldReader(BlockTreeTermsReader parent, FieldInfo fieldInfo, long numTerms, BytesRef rootCode, long sumTotalTermFreq, long sumDocFreq, int docCount, long indexStartFP, int longsSize, IndexInput indexIn, BytesRef minTerm, BytesRef maxTerm) throws IOException {
    this.fieldInfo = fieldInfo;
    this.parent = parent;
    this.numTerms = numTerms;
    this.sumTotalTermFreq = sumTotalTermFreq;
    this.sumDocFreq = sumDocFreq;
    this.docCount = docCount;
    this.indexStartFP = indexStartFP;
    this.rootCode = rootCode;
    this.longsSize = longsSize;
    this.minTerm = minTerm;
    this.maxTerm = maxTerm;

    rootBlockFP = (new ByteArrayDataInput(rootCode.bytes, rootCode.offset, rootCode.length)).readVLong() >>> BlockTreeTermsReader.OUTPUT_FLAGS_NUM_BITS;

    if (indexIn != null) {
      final IndexInput clone = indexIn.clone();
      clone.seek(indexStartFP);
      index = new FST<>(clone, ByteSequenceOutputs.getSingleton());

    } else {
      index = null;
    }
  }
```
FieldReader函数的核心部分是创建一个FST，FST，全称Finite State Transducer，用有限状态机实现对词典中单词前缀和后缀的重复利用，压缩存储空间，在上一章已经介绍了如何将FST中的信息写入.tip文件，这一章后面介绍的过程相反，要将.tip文件中的数据读取出来。
rootBlockFP被创建为ByteArrayDataInput，ByteArrayDataInput对应的每个存储结构的最高位bit用来表示是否后面的位置信息有用。例如10000001（高位1表示后面的数据和前面的数据组成一个数据）+00000001最终其实为10000001。
seek函数调整ByteBufferIndexInput中当前ByteBuffer中的position位置为indexStartFP。
最后创建FST赋值给成员变量index。

BlockTreeTermsReader::BlockTreeTermsReader->FieldReader::FieldReader->FST::FST
```
  public FST(DataInput in, Outputs<T> outputs) throws IOException {
    this(in, outputs, DEFAULT_MAX_BLOCK_BITS);
  }

  public FST(DataInput in, Outputs<T> outputs, int maxBlockBits) throws IOException {
    this.outputs = outputs;

    version = CodecUtil.checkHeader(in, FILE_FORMAT_NAME, VERSION_PACKED, VERSION_NO_NODE_ARC_COUNTS);
    packed = in.readByte() == 1;
    if (in.readByte() == 1) {
      BytesStore emptyBytes = new BytesStore(10);
      int numBytes = in.readVInt();
      emptyBytes.copyBytes(in, numBytes);

      BytesReader reader;
      if (packed) {
        reader = emptyBytes.getForwardReader();
      } else {
        reader = emptyBytes.getReverseReader();
        if (numBytes > 0) {
          reader.setPosition(numBytes-1);
        }
      }
      emptyOutput = outputs.readFinalOutput(reader);
    } else {
      emptyOutput = null;
    }
    final byte t = in.readByte();
    switch(t) {
      case 0:
        inputType = INPUT_TYPE.BYTE1;
        break;
      case 1:
        inputType = INPUT_TYPE.BYTE2;
        break;
      case 2:
        inputType = INPUT_TYPE.BYTE4;
        break;
    default:
      throw new IllegalStateException("invalid input type " + t);
    }
    if (packed) {
      nodeRefToAddress = PackedInts.getReader(in);
    } else {
      nodeRefToAddress = null;
    }
    startNode = in.readVLong();
    if (version < VERSION_NO_NODE_ARC_COUNTS) {
      in.readVLong();
      in.readVLong();
      in.readVLong();
    }

    long numBytes = in.readVLong();
    if (numBytes > 1 << maxBlockBits) {
      bytes = new BytesStore(in, numBytes, 1<<maxBlockBits);
      bytesArray = null;
    } else {
      bytes = null;
      bytesArray = new byte[(int) numBytes];
      in.readBytes(bytesArray, 0, bytesArray.length);
    }

    cacheRootArcs();
  }
```
FST的构造函数简而言之就是从.tip文件中读取写入的各个索引，并进行初始化。

传入的参数DEFAULT_MAX_BLOCK_BITS表示读取文件时每个块的大小，默认为30个bit。
checkHeader检查.tip文件的合法性。getForwardReader和getReverseReader返回FST.BytesReader。getForwardReader返回的BytesReader从缓存中向前读取数据，getReverseReader向后读取数据。
读取数据类型至inputType，即一个Term中的每个元素占多少字节。
最后读取了.tip文件最核心的内容并存储至bytesArray中，即倒排索引写过程中写入树的每个节点的信息。
cacheRootArcs函数对bytesArray中的数据进行解析并缓存根节点。

BlockTreeTermsReader::BlockTreeTermsReader->FieldReader::FieldReader->FST::FST->cacheRootArcs
```
  private void cacheRootArcs() throws IOException {
    final Arc<T> arc = new Arc<>();
    getFirstArc(arc);
    if (targetHasArcs(arc)) {
      final BytesReader in = getBytesReader();
      Arc<T>[] arcs = (Arc<T>[]) new Arc[0x80];
      readFirstRealTargetArc(arc.target, arc, in);
      int count = 0;
      while(true) {
        if (arc.label < arcs.length) {
          arcs[arc.label] = new Arc<T>().copyFrom(arc);
        } else {
          break;
        }
        if (arc.isLast()) {
          break;
        }
        readNextRealArc(arc, in);
        count++;
      }

      int cacheRAM = (int) ramBytesUsed(arcs);

      if (count >= FIXED_ARRAY_NUM_ARCS_SHALLOW && cacheRAM < ramBytesUsed()/5) {
        cachedRootArcs = arcs;
        cachedArcsBytesUsed = cacheRAM;
      }
    }
  }
```
cacheRootArcs函数首先创建Arc，并调用getFirstArc对第一个节点进行初始化。targetHasArcs函数判断是否有可读信息，即在.tip文件中，一个节点是否有下一个节点。接着调用readFirstRealTargetArc读取第一个节点也即根节点的信息，这里就不往下看了，其中最重要的是读取该节点的内容label和下一个节点在bytesArray缓存中的位置。
再往下看cacheRootArcs函数，接下来通过一个while循环读取其他的根节点，如果读取的内容label大于128或者已经读取到最后的一个叶子节点，就退出循环，否则将读取到的节点信息存入arcs中，最后根据条件缓存到cachedRootArcs和cachedArcsBytesUsed成员变量里。

BlockTreeTermsReader::BlockTreeTermsReader->FieldReader::FieldReader->FST::FST->cacheRootArcs->getFirstArc
```
  public Arc<T> getFirstArc(Arc<T> arc) {
    T NO_OUTPUT = outputs.getNoOutput();

    if (emptyOutput != null) {
      arc.flags = BIT_FINAL_ARC | BIT_LAST_ARC;
      arc.nextFinalOutput = emptyOutput;
      if (emptyOutput != NO_OUTPUT) {
        arc.flags |= BIT_ARC_HAS_FINAL_OUTPUT;
      }
    } else {
      arc.flags = BIT_LAST_ARC;
      arc.nextFinalOutput = NO_OUTPUT;
    }
    arc.output = NO_OUTPUT;

    arc.target = startNode;
    return arc;
  }
```
getFirstArc函数用来初始化第一个节点，最重要的是设置了最后的arc.target，标识了一会从.tip核心内容的缓存bytesArray的哪个位置开始读。

下面开始分析SegmentTermsEnum的seekExact函数，先看一下SegmentTermsEnum的构造函数。

SegmentTermsEnum::SegmentTermsEnum

```
  public SegmentTermsEnum(FieldReader fr) throws IOException {
    this.fr = fr;

    stack = new SegmentTermsEnumFrame[0];
    staticFrame = new SegmentTermsEnumFrame(this, -1);

    if (fr.index == null) {
      fstReader = null;
    } else {
      fstReader = fr.index.getBytesReader();
    }

    for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
      arcs[arcIdx] = new FST.Arc<>();
    }

    currentFrame = staticFrame;
    validIndexPrefix = 0;
  }
```
根据前面的分析，FieldReader的成员变量index是前面构造的FST，其构造函数读取了.tip文件，缓存了其核心内容到bytesArray中，并标记了起始位置为startNode。如果该index不为null，接下来的getBytesReader返回的就是bytesArray。

SegmentTermsEnum::seekExact
```
  public boolean seekExact(BytesRef target) throws IOException {

    term.grow(1 + target.length);

    FST.Arc<BytesRef> arc;
    int targetUpto;
    BytesRef output;

    targetBeforeCurrentLength = currentFrame.ord;

    if (currentFrame != staticFrame) {

      ...

    } else {

      targetBeforeCurrentLength = -1;
      arc = fr.index.getFirstArc(arcs[0]);
      output = arc.output;
      currentFrame = staticFrame;

      targetUpto = 0;
      currentFrame = pushFrame(arc, BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.nextFinalOutput), 0);
    }

    ...

  }
```
这里的fr是前面创建的FieldReader，index是FST，内部分装了从.tip文件读取的信息，FST_OUTPUTS是ByteSequenceOutputs。ByteSequenceOutputs的add函数合并arc.output和arc.nextFinalOutput两个BytesRef。
currentFrame和staticFrame不相等的情况不是第一次调用seekExact，if里省略的代码会利用之前的查找结果，本章不分析这种情况。
如果currentFrame和staticFrame相等，就调用getFirstArc初始化第一个Arc，最后pushFrame获得对应位置上（这里是第一个）的SegmentTermsEnumFrame并进行相应的设置。一个SegmentTermsEnumFrame代表的是一层节点，并不是一个节点，一层节点表示树中大于1个以上叶子节点到下一个该种节点间的所有节点。

SegmentTermsEnum::seekExact->SegmentTermsEnumFrame::pushFrame
```
  SegmentTermsEnumFrame pushFrame(FST.Arc<BytesRef> arc, BytesRef frameData, int length) throws IOException {
    scratchReader.reset(frameData.bytes, frameData.offset, frameData.length);
    final long code = scratchReader.readVLong();
    final long fpSeek = code >>> BlockTreeTermsReader.OUTPUT_FLAGS_NUM_BITS;
    final SegmentTermsEnumFrame f = getFrame(1+currentFrame.ord);
    f.hasTerms = (code & BlockTreeTermsReader.OUTPUT_FLAG_HAS_TERMS) != 0;
    f.hasTermsOrig = f.hasTerms;
    f.isFloor = (code & BlockTreeTermsReader.OUTPUT_FLAG_IS_FLOOR) != 0;
    if (f.isFloor) {
      f.setFloorData(scratchReader, frameData);
    }
    pushFrame(arc, fpSeek, length);

    return f;
  }
```
frameData保存了从.tip文件中读取的该节点对应的下一层节点的所有信息，即Arc结构中的nextFinalOutput。getFrame函数从SegmentTermsEnumFrame数组stack中获取对应位置上的SegmentTermsEnumFrame结构，然后调用pushFrame对其设置记录信息。

继续看seekExact函数的后一部分。

SegmentTermsEnum::seekExact
```
  public boolean seekExact(BytesRef target) throws IOException {

    ...

    while (targetUpto < target.length) {

      final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;

      final FST.Arc<BytesRef> nextArc = fr.index.findTargetArc(targetLabel, arc, getArc(1+targetUpto), fstReader);

      if (nextArc == null) {

        validIndexPrefix = currentFrame.prefix;

        currentFrame.scanToFloorFrame(target);

        if (!currentFrame.hasTerms) {
          termExists = false;
          term.setByteAt(targetUpto, (byte) targetLabel);
          term.setLength(1+targetUpto);
          return false;
        }

        currentFrame.loadBlock();

        final SeekStatus result = currentFrame.scanToTerm(target, true);            
        if (result == SeekStatus.FOUND) {
          return true;
        } else {
          return false;
        }
      } else {
        arc = nextArc;
        term.setByteAt(targetUpto, (byte) targetLabel);
        if (arc.output != BlockTreeTermsReader.NO_OUTPUT) {
          output = BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.output);
        }

        targetUpto++;

        if (arc.isFinal()) {
          currentFrame = pushFrame(arc, BlockTreeTermsReader.FST_OUTPUTS.add(output, arc.nextFinalOutput), targetUpto);
        }
      }
    }

    validIndexPrefix = currentFrame.prefix;
    currentFrame.scanToFloorFrame(target);
    if (!currentFrame.hasTerms) {
      termExists = false;
      term.setLength(targetUpto);
      return false;
    }

    currentFrame.loadBlock();

    final SeekStatus result = currentFrame.scanToTerm(target, true);            
    if (result == SeekStatus.FOUND) {
      return true;
    } else {
      return false;
    }
  }
```
getArc函数每次在SegmentTermsEnum的成员变量Arc数组arcs中分配一个Arc结构，用于存放下一个节点信息，例如查询“abc”，如果当前查找“a”，有可能下一个节点即为“b”。
findTargetArc查找byte对应节点。
if部分表示找到了最后一层节点，或者没找到节点，scanToFloorFrame函数首先从.tip文件读取的结果中获取.tim文件中的指针。如果currentFrame.hasTerms为false，则表示没有找到Term，此时就直接返回了。如果找到了，则首先通过loadBlock函数从.tim文件中读取余下的信息，再调用scanToTerm进行比较，返回最终的结果。
这个举个例子，假设lucene索引中存储了“aab”、“aac”两个Term，在调用loadBlock前，已经找到了“aa”在.tip文件中信息，loadBlock函数就是根据“aa”在.tip文件中提供的指针位置，在.tim文件中获取到了b、c。
else部分表示找到了节点，则将查找到的label缓存到term中，如果到达了该层的最后一个节点，就调用pushFrame函数创建一个SegmentTermsEnumFrame记录下一层节点的信息。

SegmentTermsEnum::seekExact->getArc->findTargetArc
```
  public Arc<T> findTargetArc(int labelToMatch, Arc<T> follow, Arc<T> arc, BytesReader in) throws IOException {
    return findTargetArc(labelToMatch, follow, arc, in, true);
  }

  private Arc<T> findTargetArc(int labelToMatch, Arc<T> follow, Arc<T> arc, BytesReader in, boolean useRootArcCache) throws IOException {

    ...

    in.setPosition(getNodeAddress(follow.target));
    arc.node = follow.target;

    ...

    readFirstRealTargetArc(follow.target, arc, in);

    while(true) {
      if (arc.label == labelToMatch) {
        return arc;
      } else if (arc.label > labelToMatch) {
        return null;
      } else if (arc.isLast()) {
        return null;
      } else {
        readNextRealArc(arc, in);
      }
    }
  }
```
省略的部分代码处理两种情况，一种情况是要查询的byte是个结束字符-1，另一种是直接从缓存cachedRootArcs查找。
第二部分省略的代码是当节点数量相同时采用二分法查找。
剩下的代码就是线性搜索了，传入的参数in就是对应.tip文件核心内容的缓存，即前面读取到的bytesArray，follow的target变量存储了第一个节点在.tip文件缓存中的指针位置，调用setPosition调整指针位置。
如果是线性搜索，则首先调用readFirstRealTargetArc读取根节点信息到arc，读取的信息最重要的一是根节点的label，二是根节点的下一个节点。如果匹配到要查找的labelToMatch就直接返回该节点，否则继续读取下一个节点直到匹配到或返回。

进入seekExact函数的if部分，scanToFloorFrame根据.tip文件中的信息获取最后的叶子节点在.tim文件中的指针，loadBlock则从.tim文件中读取最后的信息。

SegmentTermsEnum::seekExact->SegmentTermsEnumFrame::loadBlock
```
  void loadBlock() throws IOException {

    ste.initIndexInput();

    ste.in.seek(fp);
    int code = ste.in.readVInt();
    entCount = code >>> 1;
    isLastInFloor = (code & 1) != 0;

    code = ste.in.readVInt();
    isLeafBlock = (code & 1) != 0;
    int numBytes = code >>> 1;
    if (suffixBytes.length < numBytes) {
      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
    }
    ste.in.readBytes(suffixBytes, 0, numBytes);
    suffixesReader.reset(suffixBytes, 0, numBytes);

    numBytes = ste.in.readVInt();
    if (statBytes.length < numBytes) {
      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
    }
    ste.in.readBytes(statBytes, 0, numBytes);
    statsReader.reset(statBytes, 0, numBytes);
    metaDataUpto = 0;

    state.termBlockOrd = 0;
    nextEnt = 0;
    lastSubFP = -1;

    numBytes = ste.in.readVInt();
    if (bytes.length < numBytes) {
      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
    }
    ste.in.readBytes(bytes, 0, numBytes);
    bytesReader.reset(bytes, 0, numBytes);

    fpEnd = ste.in.getFilePointer();

  }
```
ste为SegmentTermsEnum。initIndexInput函数设置SegmentTermsEnum的成员变量in为BlockTreeTermsReader中的termsIn，对应.tim文件的输出流。fp为文件中的指针位置，在.tip文件中读取出来的。seek调整termsIn的读取位置。然后从tim文件读取数据到suffixBytes中，在封装到suffixBytes中。读取数据到statBytes中，封装到statsReader中。读取数据到bytes中，封装到bytesReader中。其中suffixBytes中封装的是待比较的数据。

SegmentTermsEnum::seekExact->SegmentTermsEnumFrame::scanToTerm
```
  public SeekStatus scanToTerm(BytesRef target, boolean exactOnly) throws IOException {
    return isLeafBlock ? scanToTermLeaf(target, exactOnly) : scanToTermNonLeaf(target, exactOnly);
  }

  public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {

    ste.termExists = true;
    subCode = 0;

    if (nextEnt == entCount) {
      if (exactOnly) {
        fillTerm();
      }
      return SeekStatus.END;
    }


    nextTerm: while (true) {
      nextEnt++;

      suffix = suffixesReader.readVInt();

      final int termLen = prefix + suffix;
      startBytePos = suffixesReader.getPosition();
      suffixesReader.skipBytes(suffix);

      final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
      int targetPos = target.offset + prefix;

      int bytePos = startBytePos;
      while(true) {
        final int cmp;
        final boolean stop;
        if (targetPos < targetLimit) {
          cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
          stop = false;
        } else {
          cmp = termLen - target.length;
          stop = true;
        }

        if (cmp < 0) {
          if (nextEnt == entCount) {
            break nextTerm;
          } else {
            continue nextTerm;
          }
        } else if (cmp > 0) {
          fillTerm();
          return SeekStatus.NOT_FOUND;
        } else if (stop) {
          fillTerm();
          return SeekStatus.FOUND;
        }
      }
    }

    if (exactOnly) {
      fillTerm();
    }

    return SeekStatus.END;
  }
```
suffixesReader中有多个可以匹配的term，外层的while循环依次取出每个term，其中prefix是已经匹配的长度，不需要再匹配的，因为该长度已经对应到一个block中了（block下面包含多个suffix）。suffix保存了term的长度，startBytePos保存了该term在suffixesReader也即在suffixBytes中的偏移。内层的while循环依次比对每个字节，直到每个字节都相等，targetPos会等于targetLimit，stop被设为true。其他情况下，例如遍历了所有suffix都没找到，或者cmp大于0（suffix中的字节按顺序排序），意味着该block中找不到匹配的term，则也返回。
最后如果找到了，就返回SeekStatus.FOUND。

假设索引文件中存储了“abc”“abd”两个字符串，待查找的字符串为“abc”，首先从.tip文件中按层次按节点查找“a”节点、再查找“b”节点（findTargetArc函数），获得“b”节点后继续从.tip文件中读取剩下的部分在.tim文件中的指针（scanToFloorFrame函数），然后从.tim文件中读取了“c”和“d”（loadBlock函数），最后比较获得最终结果（scanToTerm函数）。













