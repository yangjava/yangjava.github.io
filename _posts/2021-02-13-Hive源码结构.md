---
layout: post
categories: [Hive]
description: none
keywords: Hive
---
# Hive源码结构

## 整体架构
如何下载源码？
源码下载，登陆github官网，搜索hive然后下载。https://github.com/apache/hive

下载后导入idea工具，hive源码工程结构如下：

## 源码入口在哪？
我们安装完hive之后，我们通过命令hive来启动hive客户端。所以，我们从hive客户端命令hive脚本入手分析，hive的运行机制。当然也可以从beeline入手。

## hive脚本走读
执行hive-env.sh
如果hive-env.sh存在，则初始化hive环境配置。
```
if [ -f "${HIVE_CONF_DIR}/hive-env.sh" ]; then
  . "${HIVE_CONF_DIR}/hive-env.sh"
fi
```
然后做了好多的检查，包括依赖包，spark环境，hadoop环境等。

逐一执行ext目录和/ext/util下的.sh脚本。
```
for i in "$bin"/ext/*.sh ; do
  . $i
done

for i in "$bin"/ext/util/*.sh ; do
  . $i
done
```
整体来看，hive脚本还是很简单粗暴的。更多的细节，我们后面从ext下的脚本内容来看。

beeline.sh:beeline相关
cli.sh:客户端相关
debug.sh:debug相关
help.sh:帮助相关
hiveserver2.sh:hiveserver2相关

## Hive主要模块及调用关系

Hive的编译器
编译器主要有这么几个模块：Parser，SemanticAnalyzer，LogicPlanGenerator。

首先这些模块都是在编译器中，具体来说就是在
org.apache.hadoop.hive.ql.Driver#compile(java.lang.String, boolean)中。

Parser 在哪调用？

Parser是在org.apache.hadoop.hive.ql.Driver#compile(java.lang.String, boolean)中调用的，换句话说是在编译期调用的。
```
public int compile(String command, boolean resetTaskIds) {
...
      ParseDriver pd = new ParseDriver();
      ASTNode tree = pd.parse(command, ctx);
      tree = ParseUtils.findRootNonNullToken(tree);
...
```

有哪些模块？
首先parse在org.apache.hadoop.hive.ql.parse包下：

org.apache.hadoop.hive.ql.parse.ParseDriver（解析器引擎）
org.apache.hadoop.hive.ql.parse.HiveParser（hive解析器）
org.apache.hadoop.hive.ql.parse.ParseUtils（解析工具）

各个模块有哪些功能？
ParserDriver：

Hive采用Antlr实现SQL的词法语法解析，其中语法规则定义在.g的文件中（在org.apache.hadoop.hive.ql.parse包下，可以看到.g文件）。在0.10版本之前，Hive.g文件是一个，随着语法规则的越来越复杂，由语法规则生成的Java解析类可能超过Java类文件的最大上限，0.11版本将Hive.g文件拆成了5个文件（词法规则HiveLexer.g和4个语法规则文件）

FromClauseParser.g	org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser	from语法解析器
HiveLexer.g	org.apache.hadoop.hive.ql.parse.HiveLexer	Hive词法规则
HiveParser.g	org.apache.hadoop.hive.ql.parse.HiveParser	解析规则
IdentifiersParser.g	org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser	标示语法解析
SelectClauseParser.g	org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser	select语法解析

功能是什么？

## 工程解读
accumulo-handler（不关注）
hive-accumulo-handler工程。hive对accumulo对支持。

Apache Accumulo 是一个可靠的、可伸缩的、高性能的排序分布式的 Key-Value 存储解决方案，基于单元访问控制以及可定制的服务器端处理。使用 Google BigTable 设计思路，基于 Apache Hadoop、Zookeeper 和 Thrift 构建 。

ant（不关注）
hive-ant工程
应该是一个打包的工具，没细研究。

beeline（重点）
*hive-beeline工程
HS2的beeline客户端。用户可以通过beeline的方式连接hive。

bin（重点）
bin目录
这个目录下是hive相关的命令脚本，包括hive，beeline等等。

checkstyle
checkstyle目录
这里是代码规范相关的文件。

cli（重点）
hive-cli工程
该工程下是Hive Cli相关。

common
hive-common工程
通用common包在这个工程下。

conf
conf目录
这个目录下是配置文件模版，如hive-site.xml。

contrib（不关注）
hive-contrib工程
这里有一些hive贡献出来的实现，如行自增函数UDFRowSequence等。

data（不关注）
data目录
这里有配置还有一些测试文件等。

dev-support（不关注）
dev-support目录
这里是一些开发相关的工具，如eclipse风格配置文件等。

docs（不关注）
docs目录
这里是一些文档相关，如hive的logo图片，css样式等。

findbugs（不关注）
findbugs目录
插件FindBugs的配置文件。

hbase-handler
hive-hbase-handler工程
hive对Hbase的相关支持。

hcatalog
hive-hcatalog工程
hive对pig的支持。

hwi
hive-hwi工程
hwi是Hive Web Ui的缩写，顾名思义就是Hive的web界面相关。

itests
jdbc（重点）
hive-jdbc工程
hive对jdbc访问方式的支持。

lib
lib目录
用于存放第三方jar

metastore（重点）
metastore服务工程
hive的元数据存储服务，metastore不负责元数据的存储，只负责元数据的查询服务，元数据存储通过mysql等。

odbc（重点）
odbc工程
hive对odbc连接的支持

packaging
hive-packaging工程
打包工程，编译完的包在packaging路径下。

ql（重点）
hive-exec工程
这是hive的核心工程，Driver等都在这个工程中。

serde
hive-serde
hive对序列化相关实现。

service（重点）
hive-service
这里是通常所说的hiveserver2服务工程。

shims
hive-shims-aggregator工程
这里是hive和多版本hadoop兼容的支持。

spark-client
spark-client工程
hive对spark的支持。

testutils
testutils工程
该工程主要是一些测试项。






