---
layout: post
categories: [Python]
description: none
keywords: Python
---
# Python爬虫入门

## 爬虫原理
网络连接需要计算机一次Request请求和服务器端的Response回应。爬虫也是需要做两件事：
- 模拟计算机对服务器发起Request请求。
- 接收服务器端的Response内容并解析、提取所需的信息。

## 我的第一个爬虫程序

### Requests库
Requests库的官方文档指出：让HTTP服务人类。细心的读者会发现，Requests库的作用就是请求网站获取网页数据的。让我们从简单的实例开始，讲解Requests库的使用方法。
```
import requests
#网站为百度网站
res = requests.get('https://www.baidu.com/')
print(res)
#返回结果为<Response [200]>，说明请求网址成功，若为404,400则请求网址失败
print(res.text)
```
有时爬虫需要加入请求头来伪装成浏览器，以便更好地抓取数据。在Chrome浏览器中按F12键打开Chrome开发者工具，刷新网页后找到User-Agent进行复制
```
import requests
headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36"
}
#网站为百度网站
res = requests.get('https://www.baidu.com/',headers=headers)
print(res)
#返回结果为<Response [200]>，说明请求网址成功，若为404,400则请求网址失败
print(res.text)

```


