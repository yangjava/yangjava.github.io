---
layout: post
categories: [Python]
description: none
keywords: Python
---
# 中文分词工具Jieba

## Jieba简介
近年来，随着NLP技术的日益成熟，开源实现的分词工具越来越多，如Ansj、盘古分词等。在本书中，我们选取了Jieba进行介绍和案例展示，主要基于以下考虑：

●社区活跃。在本书写作的时候，Jieba在Github上已经有将近10000的star数目。社区活跃度高，代表着该项目会持续更新，实际生产实践中遇到的问题能够在社区反馈并得到解决，适合长期使用。

●功能丰富。Jieba其实并不是只有分词这一个功能，其是一个开源框架，提供了很多在分词之上的算法，如关键词提取、词性标注等。

●提供多种编程语言实现。Jieba官方提供了Python、C++、Go、R、iOS等多平台多语言支持，不仅如此，还提供了很多热门社区项目的扩展插件，如ElasticSearch、solr、lucene等。在实际项目中，进行扩展十分容易。

●使用简单。Jieba的API总体来说并不多，且需要进行的配置并不复杂，方便上手。

Jieba分词官网地址是：https://github.com/fxsjy/jieba，可以采用如下方式进行安装。

Jieba分词结合了基于规则和基于统计这两类方法。首先基于前缀词典进行词图扫描，前缀词典是指词典中的词按照前缀包含的顺序排列，例如词典中出现了“上”，之后以“上”开头的词都会出现在这一部分，例如“上海”，进而会出现“上海市”，从而形成一种层级包含结构。如果将词看作节点，词和词之间的分词符看作边，那么一种分词方案则对应着从第一个字到最后一个字的一条分词路径。因此，基于前缀词典可以快速构建包含全部可能分词结果的有向无环图，这个图中包含多条分词路径，有向是指全部的路径都始于第一个字、止于最后一个字，无环是指节点之间不构成闭环。基于标注语料，使用动态规划的方法可以找出最大概率路径，并将其作为最终的分词结果。对于未登录词，Jieba使用了基于汉字成词的HMM模型，采用了Viterbi算法进行推导。

## Jieba的三种分词模式
Jieba提供了三种分词模式：

- 精确模式：试图将句子最精确地切开，适合文本分析。
- 全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。
- 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

