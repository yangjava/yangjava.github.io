---
layout: post
categories: [Python]
description: none
keywords: Python
---
# Scrapy爬虫

## Scrapy框架简介
Scrapy是一个为了爬取网站信息，提取结构性数据而编写的应用框架。Scrapy用途广泛，可用于数据挖掘、监测和自动化测试等。

## Scrapy框架结构
Scrapy框架结构和流程设计遵循网络爬虫的基本原理。通过组件封装不同的功能模块；通过请求和响应类封装数据流；通过引擎指挥整个系统协调运行。

### 组件
下面简单介绍一下Scrapy框架结构中包含的组件。
- 引擎（Engine）
引擎犹如总指挥，是整个系统的“大脑”，指挥其他组件协同工作。
- 调度器（Scheduler）
调度器接收引擎发过来的请求，按照先后顺序，压入队列中，同时去除重复的请求。
- 下载器（Downloader）
下载器用于下载网页内容，并将网页内容返回给爬虫（Scrapy下载器是建立在twisted这个高效的异步模型上的）。
- 爬虫（Spiders）
爬虫作为最核心的组件，用于从特定的网页中提取需要的信息，即所谓的实体（Item）。用户也可以从中提取出链接，让Scrapy继续抓取下一个页面。
- 项目管道（Item Pipelines）
项目管道负责处理爬虫从网页中抽取的实体。主要的功能是持久化实体、验证实体的有效性、清除不需要的信息等。
- 下载器中间件（Downloader Middlewares）
下载器中间件介于引擎和下载器之间，主要处理Scrapy引擎与下载器之间的请求及响应。
- 爬虫中间件（Spider Middlewares）
爬虫中间件介于引擎和爬虫之间，主要工作是处理爬虫的响应输入和请求输出。

## 依赖包
Scrapy是用纯python编写的，主要依赖几个关键的python包：
- lxml 一个高效的XML和HTML解析器
- parsel ，一个写在lxml上面的html/xml数据提取库,
- w3lib ，用于处理URL和网页编码的多用途帮助程序
- twisted 异步网络框架
- cryptography 和 pyOpenSSL ，处理各种网络级安全需求

### 数据流
Scrapy框架结构中传递和处理的数据主要有以下3种：
- 向网站服务器发送的请求数据；
- 网站服务器返回的响应数据；
- 解析后的结构数据。

Scrapy中定义的Request和Response类，用于保存请求和响应数据；Item类保存解析后的结构数据。

## Scrapy执行流程
- 爬虫（Spider）使用URL（要爬取页面的网址）构造一个请求（Request）对象，提交给引擎（Engine）。如果请求要伪装成浏览器，或者设置代理IP，可以先在爬虫中间件中设置，再发送给引擎。
- 引擎将请求安排给调度器，调度器根据请求的优先级确定执行顺序。
- 引擎从调度器获取即将要执行的请求。
- 引擎通过下载器中间件，将请求发送给下载器下载页面。
- 页面完成下载后，下载器会生成一个响应（Response）对象并将其发送给引擎。下载后的数据会保存于响应对象中。
- 引擎接收来自下载器的响应对象后，通过爬虫中间件，将其发送给爬虫（Spider）进行处理。
- 爬虫将抽取到的一条数据实体（Item）和新的请求（如下一页的链接）发送给引擎。
- 引擎将从爬虫获取到的Item发送给项目管道（Item Pipelines），项目管道实现数据持久化等功能。同时将新的请求发送给调度器，再从第②步开始重复执行，直到调度器中没有更多的请求，引擎关闭该网站。

## Scrapy安装
Scrapy作为一个强大的爬虫框架，需要依赖于很多库。

使用pip安装Scrapy
```
pip install scrapy
```

如果使用pip安装失败，可以试着使用Conda安装Scrapy，执行如下命令：
```
conda install -c scrapinghub scrapy
```

## 验证安装
Scrapy安装完成后，需要验证安装是否成功。在Python解释器界面，输入如下代码：
```
import scrapy
```

## 创建项目
下面来创建一个example项目；

语法：scrapy startproject 项目名称
```
scrapy startproject  example
```

项目文件结构：
```
example/  
scrapy.cfg        # 项目配置细信息  
  
example/          # 项目模块以及所有代码位置  
__init__.py  
items.py          # 设置数据存储模板  
middlewares.py    # 中间件模型  
pipelines.py      # 数据持久化处理  
settings.py       # 配置文件，如：并发数，延迟下载等  
spiders/          # 爬虫目录  
__init__.py
```

## 第一只爬虫
爬虫是你定义的类，Scrapy用来从网站获取信息，必须是子类Spider定义要发出的初始请求，可以选择跟踪页面中的链接，以及解析获取到的内容以提取有效的数据。

如下我们的第一只爬虫，保存在example/spiders项目目录下，文件名为mySpider.py：





