---
layout: post
categories: [Kafka]
description: none
keywords: Kafka
---
# Kafka源码消费者实战
消费者
消费者分为高级和简单两种消费者
高级：消费者组形式消费，更加简单高效


## 多线程消费者demo
可以使用shell命名消费topic
```
可以使用消费者组命令查看，监控工具也可以使用
./kafka-consumer-groups.sh --zookeeper hdp01:2181,hdp02:2181,hdp03:2181 --list
/kafka-consumer-groups.sh --zookeeper hdp01:2181,hdp02:2181,hdp03:2181 --group cons --describe
```

使用客户端消费
```
public class consumer  {
    private final ConsumerConnector consumer;
	priate final String topic="spark01";
    public consumer(){
        Properties props = new Properties();

        //zookeeper 配置，通过zk 可以负载均衡的获取broker
        props.put("group.id", "cons");
        props.put("enable.auto.commit", "true");
        props.put("zookeeper.connect","hdp01:2181,hdp02:2181,hdp03:2181");
        props.put("auto.commit.interval.ms", "1000");
        props.put("partition.assignment.strategy", "Range");
        props.put("auto.offset.reset", "smallest");
        // props.put("zookeeper.connect","hdp01:2181,hdp02:2181,hdp03:2181");
        /**指定键的反序列化方式*/
        props.put("key.deserializer",
                "org.apache.kafka.common.serialization.StringDeserializer");
        /**指定值得反序列化方式*/
        props.put("value.deserializer",
                "org.apache.kafka.common.serialization.StringDeserializer");
      
        //构建consumer connection 对象
        consumer = Consumer.createJavaConsumerConnector(new ConsumerConfig(props));
    }

    public void consume(){
        //指定需要订阅的topic
        Map<String ,Integer> topicCountMap = new HashMap<String, Integer>();
        //value值建议和partititon个数一样。指定featch消息的线程数，从而实现多线程获取消息，若此处value!=1,但是没有多线程featch,会造成正常运行但是消费不到消息的现象。具体看源码分析，会并行获取分区的消息
        topicCountMap.put(topic, new Integer(numThreads));

        //指定key的编码格式
        Decoder<String> keyDecoder = new kafka.serializer.StringDecoder(new VerifiableProperties());
        //指定value的编码格式
        Decoder<String> valueDecoder = new kafka.serializer.StringDecoder(new VerifiableProperties());

        //获取topic 和 接受到的stream 集合，针对每一个消费者线程创建一个BlockingQueue队列，队列中存储的
        //是FetchedDataChunk数据块，每一个数据块中包括多条记录。
        Map<String, List<KafkaStream<String, String>>> map = consumer.createMessageStreams(topicCountMap, keyDecoder, valueDecoder);

        //根据指定的topic 获取 stream 集合
        List<KafkaStream<String, String>> kafkaStreams = map.get(topic);
		//创建多线程消费者kafkaStream，线程数和topicCountMap 中的value值必须保持一致。若value为1，则没有必要运行线程池创建多线程
        ExecutorService executor = Executors.newFixedThreadPool(numThreads);
        
        //因为是多个 message组成 message set ， 所以要对stream 进行拆解遍历
        for(final KafkaStream<String, String> kafkaStream : kafkaStreams){
        //是否需要多线程根据topicCountMap 中的value值是否>1
            executor.submit(new Runnable() {
                @Override
                public void run() {
                    //拆解每个的 stream，一般根据消费获取某几个分区的数据。
                    ConsumerIterator<String, String> iterator = kafkaStream.iterator();
                    while (iterator.hasNext()) {
                       //messageAndMetadata 包括了 message ， topic ， partition等metadata信息
                        MessageAndMetadata<String, String> messageAndMetadata = iterator.next();
                        System.out.println("message : " + messageAndMetadata.message() + "  partition :  " + messageAndMetadata.partition());
                        //偏移量管理：提交由此连接器连接的所有代理分区的偏移量。
                        consumer.commitOffsets();
                    }
                }
            });
        }
    }
    
    public static void main(String[] args) {
        new consumer().consume();
    }
}

```

## 源码解析


























