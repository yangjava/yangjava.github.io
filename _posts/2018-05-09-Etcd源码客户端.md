---
layout: post
categories: etcd
description: none
keywords: etcd
---
# etcd源码客户端

## etcd客户端详解
在etcd v2版本中，客户端采用的是HTTP+JSON的方式与Server端进行交互。在etcd v3版本中，客户端采用的是GRPC的方式与Server端进行交互。相比之下，Client v2的交互方式更加简单快捷，我们甚至可以直接使用浏览器或Post Man等工具与etcd v2集群进行交互。Client v3使用GRPC进行通信，底层的协议消息通过Google Protocol Buffer定义，它可以简化RPC客户端存根代码的生成和管理。另外，GRPC 在处理网络连接方面优势比较明显，GRPC 使用单一连接的HTTP2，实现多路复用的RPC调用，而HTTP+JSON的客户端实现则必须为每个请求建立一个连接。

## GRPC基础
在开始介绍etcd对GRPC的应用之前，我们先来介绍一下GRPC的基础使用方式。在本节中，我们将简单介绍在Golang中使用GRPC的基本流程，并介绍GRPC自带的route_guide示例。使用GRPC编写一个服务分为如下三步：

（1）编写一个“.proto”文件，在该“.proto”文件中定义相关服务。

（2）用Protocol Buffer编译器生成服务端和客户端的存根代码。

（3）使用GRPC的Go API为服务实现一个简单的客户端和服务器。

8.1.1 定义proto文件
这里我们以GRPC自带的route_guide示例介绍GRPC的基本使用方法。首先，我们需要获取GRPC的代码，具体命令如下：


之后，我们可以跳转到route_guide实例的目录中，具体命令如下：


在 route_guide 目录下，我们可以看到一个名为“route_guide.proto”的文件，在其中定义RouteGuide服务，具体内容如下所示。


在route_guide.proto文件中，除了定义RouteGuide服务，还定义了该服务中使用到的消息，如下所示。



完成“.proto”文件的定义之后，我们可以通过 protoc（Protocol Buffer 的编译器）和一个特殊的GRPC-Go插件来生成GRPC客户端和服务器端的接口。在Mac系统上安装protoc的步骤如下，其他系统的读者可以查阅相关资料进行学习。

（1）从Protocol Buffers官方网站（https：//github.com/google/protobuf/releases）下载最新的Protocol Buffers压缩包，并解压缩。

（2）将命令行跳转到解压后的目录中，并执行“./configure”命令。

（3）依次执行“make”命令、“make check”命令、“sudo make install”命令。

（4）通过“protoc-version”命令检测安装是否成功。

完成 protoc 的安装之后，我们回到前面介绍的 route_guide 目录下并执行“protoc--go_out=plugins=grpc：.route_guide.proto”命令，即可在当前目录中得到一个名为“route_guide.pb.go”文件。在该文件中包括如下内容：

· 所有用于填充、序列化和获取上述定义的请求和响应消息类型的Protocol Buffers代码。

· 一个为客户端调用定义的接口（RouteGuideClient接口）。

· 一个为服务端定义的接口，其中定义了前面介绍的 RouteGuide 服务中的全部方法（RouteGuideServer接口）。

8.1.2 服务端
接下来，我们需要创建一个 RouteGuide 服务器。首先需要做的就是实现前面生成的RouteGuideServer接口。在route_guide示例的server/server.go文件中，我们可以看到routeGuideServer结构体，它就是我们需要的 RouteGuideServer 接口实现。这里简单介绍一下其中的几个方法实现。

首先是routeGuideServer.GetFeature（）方法，该方法的第一个参数是RPC的上下文对象，第二个参数是客户端传递的Point消息，返回了一个包含响应信息的Feature消息和一个错误信息。该方法的大致实现如下：


下面再看一下 routeGuideServer.ListFeatures（）方法，它是一个流式 RPC，可以返回多个Feature 消息给客户端。该方法的第一个参数是客户端发送的 Rectangle 消息；第二个参数是RouteGuide_ListFeaturesServer实例，它是用来写入响应消息的，在该方法中我们可以创建任意个数的Feature实例，然后通过Send（）方法把它们写入RouteGuide_ListFeaturesServer中。在该方法的最后，我们返回了一个 nil 告诉GRPC响应的写入已经完成。如果在调用过程中发生任何错误，我们可以返回一个非nil的错误信息通知GRPC层。routeGuideServer.ListFeatures（）方法的具体实现如下：


接下来看一下 routeGuideServer.RecordRoute（）方法，这个方法只有一个 RouteGuide_RecordRouteServer 流作为其参数，服务端可以用它来读写消息。服务端可以用其RouteGuide_RecordRouteServer.Recv（）方法接收客户端的消息，也可以用其SendAndClose（）方法返回它的单个响应。RecordRoute（）方法的具体实现如下：



最后看一下 routeGuideServer.RouteChat（）方法，这个方法与上面的介绍的 RecordRoute（）方法很类似，该方法的唯一参数是 RouteGuide_RouteChatServer 流，它可以用来读写消息。与RecordRoute（）方法的区别在于，当客户端还在往其中写入消息时，服务端同时可以通过 Send（）方法返回消息，这样就可以返回多个消息。RouteChat（）方法的具体实现如下：


了解完routeGuideServer结构体的实现，为服务端生成代码之后，我们还需要启动一个GRPC服务器，这样客户端才可以使用上述服务。在route_guide示例中的server/server.go文件中，我们可以看到main（）函数，其大致实现如下所示。



8.1.3 创建客户端
在route_guide.pb.go 文件中，除了有 RouteGuideServer 接口，还有另一个供客户端使用的RouteGuideClient 接口，并且提供了相应实现ü　结构体 routeGuideClient，其具体实现中涉及的GRPC使用方式与前面介绍的routeGuideServer有很多类似之处，这里不再展开描述。

接下来，我们开始搭建RouteGuide 服务的Go客户端，在route_guide示例中的client/client.go文件中，我们可以看到其客户端的完整实现。这里重点关注其中的 main（）方法，为了调用服务端方法，需要创建一个GRPC channel 和服务端进行交互，具体实现如下：


到此为止，GRPC的基础使用和route_guide示例的实现就全部介绍完了。在后面介绍etcd的Client v3时，还会提到GRPC的相关内容。

8.2 Client v3
正如前面介绍的那样，在etcd 3.2中的客户端使用GRPC实现与服务端的交互。我们依旧按照上一节的介绍 GRPC 使用流程，先来看一下 proto 文件文件的定义，如图 8-1 所示，在etcdserver/etcdserverpb目录中的rpc.proto文件中里定义了六个服务。


图8-1

下面先分析KV服务，其中定义了五个客户端最基本、最常用的RPC方法，分别是Range（）方法、Put（）方法、Txn（）方法、DeleteRange（）方法和 Compact（）方法，这些方法都是普通的 RPC方法，没有使用前面介绍的流式操作，其具体定义如下所示。


在 rpc.proto 文件的同一目录下，有一个名为“rpc.pb.go”文件，其中就包含了通过 protoc命令生成的服务端代码，定义了 KVServer 接口及该接口使用到的消息等内容。在 etcd-server模块中提供了KVServer接口实现，如图8-2所示。


图8-2

8.2.1 kvServer
接下来，我们深入分析结构体kvServer的具体实现，其中封装了两个字段，具体含义如下所示。

· hdr（header类型）：用于填充响应消息的头信息。

· kv（etcdserver.RaftKV类型）：如图8-2所示，该RaftKV接口继承了KVServer接口。在 NewKVServer（）函数中我们可以看到，kvServer.kv 字段实际指向了前面介绍的EtcdServer实例。

kvServer 结构体处理请求的大致步骤如下：首先会对请求消息进行各方面的检查，检查完成之后会将所有的请求交给其内封装的 RaftKV 接口实现进行处理，待处理完成得到响应消息之后，会通过 header.fill（）方法填充响应的头信息，最终将完整的响应消息返回给客户端，整个请求的处理流程结束。下面以kvServer.Range（）方法为例进行介绍，其具体实现如下所示。


kvServer中其他方法的实现与Range（）方法的处理流程基本一致，这里不再逐个展开描述了，感兴趣的读者可以参考源码进行学习。

8.2.2 EtcdServer
前面介绍的EtcdServer结构体是RaftKV接口的实现之一，在结构体kvServer中kv字段就指向了EtcdServer示例。在本节中，我们将重点介绍EtcdServer对RaftKV接口的实现。

1.Range方法

我们先来看一下EtcdServer.Range（）方法，它首先会将根据RangeRequest请求决定此次请求是serializable类型还是linearizable类型，这两种类型的读请求在前面也多次提到过，这里简单回顾一下，serializable read请求会直接读取当前节点的数据并返回给客户端，它并不保证返回给客户端的数据是集群中最新的，例如，当前出现了网络分区，响应请求的节点是上一个Term的Leader节点。而linearizable read请求的处理过程会通过Raft协议保证返回给客户端最新数据，具体的过程在前面已经介绍过了，这里不再展开描述。之后，Range（）会进行权限检查，检查通过之后会调用 EtcdServer.applyV3Base.Range（）方法完成键值对的查询，最后，将键值对数据封装成RangeResponse消息返回给客户端。EtcdServer.Range（）方法的具体实现如下所示。


在EtcdServer.doSerialize（）方法中，会使用Range（）方法中定义的回调函数，完成权限检测以及键值对数据的查询，其具体实现如下：


2.Put&DeleteRange

接下来，我们简单分析EtcdServer.Put（）和DeleteRange（）方法的具体实现。两者都会直接调用EtcdServer.raftRequest（）方法向集群发送MsgProp消息，并阻塞等待其中的Entry记录被应用。raftRequest（）方法底层是调用raftRequestOnce（）方法实现的，如下：


raftRequestOnce（）方法是通过调用 processInternalRaftRequestOnce（）方法实现的，而processInternalRaftRequestOnce（）方法的具体实现在上一章中已经详细分析过了，这里不再展开介绍。

3.Txn

在介绍applierV3backend.Txn（）方法时也提到，在TxnRequest中可以封装多个操作，这些操作将会批量执行。EtcdServer.Txn（）方法就用来处理客户端发送的 TxnRequest，其中会根据TxnRequest中封装的操作类型进行分类处理。如果TxnRequest中封装的都是只读操作，则其处理流程与前面介绍 Range（）方法类似；如果TxnRequest 中包含写操作，则其处理流程与前面介绍的Put（）方法类似。Txn（）方法的具体实现如下：


4.Compact

在前面的章节中提到，如果待应用的 Entry 记录中封装了 CompactionRequest，会调用applierV3backend.Compact（）方法处理，在其中会将压缩操作放入FIFO调度器中执行。该方法还会返回一个通道，当压缩操作真正完成之后，会关闭该通道实现通知的效果。

回顾完 applierV3 接口中压缩相关的操作之后，下面看一下 EtcdServer.Compact（）方法的具体实现：



5. 启动服务

介绍完EtcdServer对RaftKV接口的实现之后，我们看一下服务端的启动流程，其中就包含了启动GRPC服务端的相关代码。

在 etcd.go 文件中的 StartEtcd（）函数负责启动 etcd 的服务端，每个节点都会对外提供两组URL地址，一组是与集群中其他节点交互的URL地址（Peer URL），另一组是与客户端交互的URL地址（Client URL）。StartEtcd（）函数首先会为每个URL地址创建相应的Listener实例并记录到指定的字段中，然后调用前文介绍的 etcdserver.NewServer（）函数创建 EtcdServer 实例，之后调用EtcdServer.Start（）方法启动该实例，最后调用Etcd.serve（）方法对外提供服务。StartEtcd（）函数的具体实现如下所示。



接下来介绍startPeerListeners（）函数，该函数中会为每个Peer URL创建相应的Listener实例。后面会使用这些 Listener 创建相应的 http.Server 实例，从而监听 Peer URL 上的请求。startPeerListeners（）函数的具体实现如下：


下面看一下startClientListeners（）函数，其中会为每个Client URL地址创建相应的serverCtx实例。在serverCtx实例中记录了对应的Client URL、相应的Listener实例和用户自定义Handler等信息。startClientListeners（）函数的具体实现如下所示。


最后是Etcd.serve（）方法，该方法中会启动Peer URL对应的http.Server实例，开始监听来自集群中其他节点的请求，另外还会启动GRPC服务，开始接收客户端的请求，该方法的具体实现如下所示。



下面我们进入 serveCtx.serve（）方法进行进一步分析。在上一章介绍 EtcdServer 时提到，当前其完全初始化之后会关闭 EtcdServer.readych 通道，serveCtx.serve（）方法在开始处就会阻塞监听该通道是否关闭，从而决定是否继续对外提供服务。serveCtx.serve（）方法的具体实现如下所示。



最后简单介绍一下v3rpc.Server（）函数，这里是完成gRPC服务的注册的地方，在该函数中不仅完成了KVServer服务的注册，还完成了WatcherServer和LeaseServer多个其他服务的注册，其具体实现如下：


8.2.3 Client
在本节前面的内容中，我们介绍了protoc生成的服务端代码，以及我们如何使用这些生成的代码搭建一个GRPC服务。在此我们将来介绍protoc生成的客户端代码，以及如何使用这些代码搭建一个客户端。

首先，在上一节介绍的 rpc.pb.go 文件中会为客户端生一个名为 KVClient 的接口，同时也会生成该接口的实现ü　结构体kVClient，如图8-3所示。在etcd-clientv3中还有提供了另外两个KVClient接口的实现，在后面我们会一一介绍。


图8-3

etcd V3版本的客户端中的核心结构体是Client，在其中嵌套了多个接口，这里简单介绍每个接口的含义。

· KV接口：负责处理键值对操作，例如，增删改查键值对数据、触发压缩操作等。

· Cluster接口：负责完成集群节点的管理操作，例如，增删改查集群中的节点信息。

· Lease接口：负责租约相关的操作，例如，新建和回收租约、为租约续期等。

· Watcher接口：负责为指定的Key添加Watcher。

· Auth接口：负责权限相关的操作，例如，添加用户或角色、为用户分配角色等。

· Maintenance接口：提供了获取Alarm列表、关闭 Alarm的功能，该接口还提供了读取指定节点状态的功能、触发指定节点进行压缩的功能，以及读取指定节点快照的功能。

在本节中，我们重点以KV接口为例进行介绍，其具体定义如下：



结构体Client中其他关键字段的具体含义如下所示。

· conn（*grpc.ClientConn类型）：与服务端交互的连接，在前面使用过，不再赘述。

· cfg（Config类型）：Client的相关配置都封装在该字段中。

· balancer（*simpleBalancer类型）：simpleBalancer继承了GRPC中的Balancer接口，它会选择合适的地址并相应的gRPC连接，实现了负载均衡的功能。

· retryWrapper（retryRpcFunc类型）：retryRpcFunc主要负责重试。

· Username 和 Password（string 类型）：当前客户端的用户名和密码，当客户端正式发起请求之前，会先通过用户名和密码检测相应权限。

1. 初始化

Client 实例的初始化过程是在 newClient（）函数中完成的，该方法首先会创建 Client 实例，然后创建grpc.ClientConn实例，最后初始化前面介绍的GRPC服务的客户端。newClient（）函数的具体实现如下所示。



接下来我们分析Client.dial（）方法是如何建立网络连接的，其具体实现如下：



最后，我们来看一下 newClient（）函数中启动的后台 goroutine，该 goroutine 执行的是Client.autoSync（）方法，具体实现如下：


在 Client.Sync（）方法中会向集群请求当前的节点列表，并更新本地的缓存，其具体实现如下所示。



2.kv

前面提到，Client中内嵌了KV接口，在Client实例的初始化过程可以看到，其中由NewKV（）函数创建了kv实例。在kv中只有一个remote字段，该字段实际上指向一个RetryKVClient实例（RetryKVClient实现了KV接口）。

结构体kv中的Put（）、Get（）和Delete（）三个方法的实现比较简单，直接调用kv.Do（）方法实现。在kv.Do（）方法的实现中会循环调用kv.do（）方法，具体实现如下所示。


在kv.do（）方法中会根据此次操作的类型，分别调用KVClient相应的方法向集群发送请求，具体实现如下所示。



kv.Compact（）方法的实现比较简单，是直接调用了KVClient.Compact（）方法实现的，这里不再赘述。kv.Txn（）方法的实现也比较简单，直接返回了一个txn实例，在txn实例中封装了多个操作。结构体txn实现了Txn接口，Txn接口的定义如下：


接下来看一下结构体txn中的各个字段的含义。

· kv（*kv类型）：关联的kv实例。

· cmps（[]*pb.Compare类型）：用于记录Compare条件的集合。

· sus（[]*pb.RequestOp类型）：全部Compare条件通过之后执行的操作列表。

· fas（[]*pb.RequestOp类型）：任一Compare条件检测失败之后执行的操作列表。

· cif（bool类型）：是否设置Compare条件，设置一次则该字段就会被设置为true。

· cthen（bool类型）：是否向上面的sus集合中添加了操作。

· celse（bool类型）：是否向上面的fas集合中添加操作。

· isWrite（bool类型）：如果当前txn实例中记录的操作全部是读操作，则该字段为false，否则为true。

txn.If（）、Then（）和Else（）方法会将操作添加到指定的集合中，其中If（）方法会将Compare条件添加到cmps集合中，Then（）方法会将操作添加到sus集合中并同时更新isWrite字段，Else（）方法会将操作添加到fas集合中并同时更新isWrite字段。

这里重点看一下txn.Commit（）方法，该方法最终会调用KVClient.Txn（）方法进行处理，具体实现如下所示。


3.RetryKVClient

在上一小节的介绍中提到，kv.remote 字段实际上指向一个 RetryKVClient 实例。结构体RetryKVClient中内嵌了retryWriteKVClient，而在结构体retryWriteKVClient中内嵌了KVClient接口并且封装了一个retryRpcFunc回调函数（retryf字段）。

在RetryKVClient实例初始化时，会将retryRpcFunc字段指向Client.retryWrapper字段，而 Client.retryWrapper 字段指向的回调函数则是在 newRetryWrapper（）中定义的，具体实现如下所示。



在retryWriteKVClient的实现中，都是通过上述回调函数完成RPC调用的，这样就可以实现重试的功能，这里以Put（）方法为例进行介绍：


retryWriteKVClient中实现的其他方法与Put（）方法类似，这里不再赘述，感兴趣的读者可以分析相应代码。

到目前为止，Client v3中KV服务的整个运作流程，以及涉及的具体实现就全部介绍完了。除了KV服务的其他服务，例如，Cluster、Auth、Lease、Maintenance和Watch等，与KV服务的运转流程很类似，只不过是服务端底层调用了不同的组件，支持了不同方面的功能而已，这里不再一一展开介绍。在了解了KV服务的整个流程之后，读者可以很轻松地完成其他服务的代码分析。

8.3 Client v2
etcd v2版本的Client的代码位于client包中，它是通过HTTP+JSON方式实现的。在上一节介绍etcd服务端启动的过程中，我们可以也看到其中专门添加了一个Handler用于Client v2请求，读者可以回顾上一章对etcd服务端的介绍。

8.3.1 KeysAPI接口
我们首先需要了解的是KeysAPI接口，该接口中定义了Client v2对键值对操作的方法，其具体定义如下所示。


结构体httpKeysAPI是KeysAPI接口的实现，其中封装了httpClient实例（client字段），该httpKeysAPI 中所有方法的实现都是通过委托给 httpClient.Do（）方法实现的。这里以httpKeysAPI.Set（）方法为例进行介绍，其具体实现如下：



httpKeysAPI其他方法与Set（）方法类似，这里不再一一展开介绍。

8.3.2 httpClient接口
httpClient接口中只定义了一个Do（）方法，正如上一小节所示，所有的请求都会经过该方法处理。httpClient接口的相关实现如图8-4所示。


图8-4

1.httpClusterClient

首先分析结构体 httpClusterClient，httpKeysAPI.client 字段实际上也指向 httpClusterClient实例。httpClusterClient中各个字段的含义如下所示。

· clientFactory（httpClientFactory类型）：工厂函数，用于创建底层的httpClient实例。

· endpoints（[]url.URL类型）：记录集群中所有节点的暴露给客户端的URL地址。

· pinned（int类型）：用于选择重试的URL地址。

· rand（*rand.Rand类型）：随机数，用于选择重试的URL地址。

· selectionMode（EndpointSelectionMode 类型）：更新 endpoints 字段的模式，目前有两个可选值，分别是EndpointSelectionRandom和EndpointSelectionPrioritizeLeader。

下面看一下httpClusterClient.Do（）方法，其中会从endpoints字段中选择合适的URL地址并建立连接，然后发送相应的请求，具体实现如下所示。



在Client v2中也有与Client v3类似的同步机制，其主要实现在httpClusterClient.Sync（）方法中。Sync（）方法首先会请求当前集群中所有节点提供的 ClientURL 地址，然后根据当前selectionMode字段指定的模式修改pinned字段值，最后更新endpoints字段中记录的ClientURL地址。Sync（）方法发具体实现如下所示。



httpClusterClient.clientFactory字段指向了一个newHTTPClientFactory（）工厂函数，该函数负责根据指定的ClientURL创建httprClient实例，具体实现如下：


2.redirectFollowingHTTPClient

httpClusterClient.clientFactory 工厂函数返回的是 redirectFollowingHTTPClient 实例，redirectFollowingHTTPClient也实现了httpClient接口，其Do（）方法主要处理Redirect跳转的响应，具体实现如下：


3.simpleHTTPClient

在前面介绍的 newHTTPClientFactory（）工厂函数中可以看到，redirectFollowingHTTP-Client.client字段指向了一个simpleHTTPClient实例，结构体simpleHTTPClient也是httpClient接口的实现之一。simpleHTTPClient 是功能最简单，也是最基础的 httpClient 接口实现。simpleHTTPClient中各个字段的含义如下所示。

· transport（CancelableTransport 类型）：内嵌了 http.RoundTripper，用于发送 HTTP请求并获取相应的响应。

· endpoint（url.URL类型）：请求的URL地址。

· headerTimeout（time.Duration类型）：请求的超时时间。

接下来看一下simpleHTTPClient.Do（）方法的具体实现如下：


除了KeysAPI，Client v2中其他的服务，例如，MembersAPI等，底层也是依赖httpClient接口向服务端发送HTTP请求并获取响应的，这里不再一一展开介绍。

本章小结
本章主要介绍了etcd客户端的具体实现，其中包括v3和v2两个版本的客户端实现。Client v3的具体实现在client v3包中，Client v3是通过GRPC的方式与服务端进行交互的。在本章开始，我们介绍了GRPC的基本使用方式，其中包括proto文件的定义、protoc编译器的使用，以及在生成代码的基础上完成客户端和服务端。

随后我们详细分析了Client v3中KV服务的内容，首先是proto文件的服务定义，以及生成的客户端和服务端的接口，然后介绍了服务端的EtcdServer结构体对KV接口等的实现，最后详细分析了客户端的实现。

在本章的最后一节，我们简单介绍了 Client v2 的大致实现。v2 版本的客户端是通过HTTP+JSON的方式与服务端进行通信的。这里我们只是简单介绍了KeysAPI接口、底层使用的httpClient接口及具体实现，其他的接口留给读者自行分析。

希望通过本章的介绍，读者不仅了解如何使用etcd的客户端，还能够深入了解其工作原理。







---------------------
grpc通过etcd实现服务发现
前言
项目中使用etcd实现了grpc的服务户注册和服务发现，这里来看下如何实现的服务注册和服务发现

先来看下使用的demo，demo中的代码discovery

服务注册
package discovery

import (
"context"
"encoding/json"
"errors"
"net/http"
"strconv"
"strings"
"time"

	clientv3 "go.etcd.io/etcd/client/v3"
	"go.uber.org/zap"
)

// Register for grpc server
type Register struct {
EtcdAddrs   []string
DialTimeout int

	closeCh     chan struct{}
	leasesID    clientv3.LeaseID
	keepAliveCh <-chan *clientv3.LeaseKeepAliveResponse

	srvInfo Server
	srvTTL  int64
	cli     *clientv3.Client
	logger  *zap.Logger
}

// NewRegister create a register base on etcd
func NewRegister(etcdAddrs []string, logger *zap.Logger) *Register {
return &Register{
EtcdAddrs:   etcdAddrs,
DialTimeout: 3,
logger:      logger,
}
}

// Register a service
func (r *Register) Register(srvInfo Server, ttl int64) (chan<- struct{}, error) {
var err error

	if strings.Split(srvInfo.Addr, ":")[0] == "" {
		return nil, errors.New("invalid ip")
	}

	if r.cli, err = clientv3.New(clientv3.Config{
		Endpoints:   r.EtcdAddrs,
		DialTimeout: time.Duration(r.DialTimeout) * time.Second,
	}); err != nil {
		return nil, err
	}

	r.srvInfo = srvInfo
	r.srvTTL = ttl

	if err = r.register(); err != nil {
		return nil, err
	}

	r.closeCh = make(chan struct{})

	go r.keepAlive()

	return r.closeCh, nil
}

// Stop stop register
func (r *Register) Stop() {
r.closeCh <- struct{}{}
}

// register 注册节点
func (r *Register) register() error {
leaseCtx, cancel := context.WithTimeout(context.Background(), time.Duration(r.DialTimeout)*time.Second)
defer cancel()

	leaseResp, err := r.cli.Grant(leaseCtx, r.srvTTL)
	if err != nil {
		return err
	}
	r.leasesID = leaseResp.ID
	if r.keepAliveCh, err = r.cli.KeepAlive(context.Background(), leaseResp.ID); err != nil {
		return err
	}

	data, err := json.Marshal(r.srvInfo)
	if err != nil {
		return err
	}
	_, err = r.cli.Put(context.Background(), BuildRegPath(r.srvInfo), string(data), clientv3.WithLease(r.leasesID))
	return err
}

// unregister 删除节点
func (r *Register) unregister() error {
_, err := r.cli.Delete(context.Background(), BuildRegPath(r.srvInfo))
return err
}

// keepAlive
func (r *Register) keepAlive() {
ticker := time.NewTicker(time.Duration(r.srvTTL) * time.Second)
for {
select {
case <-r.closeCh:
if err := r.unregister(); err != nil {
r.logger.Error("unregister failed", zap.Error(err))
}
if _, err := r.cli.Revoke(context.Background(), r.leasesID); err != nil {
r.logger.Error("revoke failed", zap.Error(err))
}
return
case res := <-r.keepAliveCh:
if res == nil {
if err := r.register(); err != nil {
r.logger.Error("register failed", zap.Error(err))
}
}
case <-ticker.C:
if r.keepAliveCh == nil {
if err := r.register(); err != nil {
r.logger.Error("register failed", zap.Error(err))
}
}
}
}
}

// UpdateHandler return http handler
func (r *Register) UpdateHandler() http.HandlerFunc {
return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) {
wi := req.URL.Query().Get("weight")
weight, err := strconv.Atoi(wi)
if err != nil {
w.WriteHeader(http.StatusBadRequest)
w.Write([]byte(err.Error()))
return
}

		var update = func() error {
			r.srvInfo.Weight = int64(weight)
			data, err := json.Marshal(r.srvInfo)
			if err != nil {
				return err
			}
			_, err = r.cli.Put(context.Background(), BuildRegPath(r.srvInfo), string(data), clientv3.WithLease(r.leasesID))
			return err
		}

		if err := update(); err != nil {
			w.WriteHeader(http.StatusInternalServerError)
			w.Write([]byte(err.Error()))
			return
		}
		w.Write([]byte("update server weight success"))
	})
}

func (r *Register) GetServerInfo() (Server, error) {
resp, err := r.cli.Get(context.Background(), BuildRegPath(r.srvInfo))
if err != nil {
return r.srvInfo, err
}
info := Server{}
if resp.Count >= 1 {
if err := json.Unmarshal(resp.Kvs[0].Value, &info); err != nil {
return info, err
}
}
return info, nil
}
来分析下上面的代码实现

当启动一个grpc的时候我们注册到etcd中

	etcdRegister := discovery.NewRegister(config.Etcd.Addrs, log.Logger)
	node := discovery.Server{
		Name: app,
		Addr: utils.InternalIP() + config.Port.GRPC,
	}

	if _, err := etcdRegister.Register(node, 10); err != nil {
		panic(fmt.Sprintf("server register failed: %v", err))
	}
调用服务注册的时候首先分配了一个租约

func (l *lessor) Grant(ctx context.Context, ttl int64) (*LeaseGrantResponse, error) {
r := &pb.LeaseGrantRequest{TTL: ttl}
resp, err := l.remote.LeaseGrant(ctx, r, l.callOpts...)
if err == nil {
gresp := &LeaseGrantResponse{
ResponseHeader: resp.GetHeader(),
ID:             LeaseID(resp.ID),
TTL:            resp.TTL,
Error:          resp.Error,
}
return gresp, nil
}
return nil, toErr(ctx, err)
}
然后通过KeepAlive保活

// KeepAlive尝试保持给定的租约永久alive
func (l *lessor) KeepAlive(ctx context.Context, id LeaseID) (<-chan *LeaseKeepAliveResponse, error) {
ch := make(chan *LeaseKeepAliveResponse, LeaseResponseChSize)

	l.mu.Lock()
	// ensure that recvKeepAliveLoop is still running
	select {
	case <-l.donec:
		err := l.loopErr
		l.mu.Unlock()
		close(ch)
		return ch, ErrKeepAliveHalted{Reason: err}
	default:
	}
	ka, ok := l.keepAlives[id]
	if !ok {
		// create fresh keep alive
		ka = &keepAlive{
			chs:           []chan<- *LeaseKeepAliveResponse{ch},
			ctxs:          []context.Context{ctx},
			deadline:      time.Now().Add(l.firstKeepAliveTimeout),
			nextKeepAlive: time.Now(),
			donec:         make(chan struct{}),
		}
		l.keepAlives[id] = ka
	} else {
		// add channel and context to existing keep alive
		ka.ctxs = append(ka.ctxs, ctx)
		ka.chs = append(ka.chs, ch)
	}
	l.mu.Unlock()

	go l.keepAliveCtxCloser(ctx, id, ka.donec)
	// 使用once只在第一次调用
	l.firstKeepAliveOnce.Do(func() {
		// 500毫秒一次，不断的发送保持活动请求
		go l.recvKeepAliveLoop()
		// 删除等待太久没反馈的租约
		go l.deadlineLoop()
	})

	return ch, nil
}

// deadlineLoop获取在租约TTL中没有收到响应的任何保持活动的通道
func (l *lessor) deadlineLoop() {
for {
select {
case <-time.After(time.Second):
// donec 关闭，当 recvKeepAliveLoop 停止时设置 loopErr
case <-l.donec:
return
}
now := time.Now()
l.mu.Lock()
for id, ka := range l.keepAlives {
if ka.deadline.Before(now) {
// 等待响应太久；租约可能已过期
ka.close()
delete(l.keepAlives, id)
}
}
l.mu.Unlock()
}
}

func (l *lessor) recvKeepAliveLoop() (gerr error) {
defer func() {
l.mu.Lock()
close(l.donec)
l.loopErr = gerr
for _, ka := range l.keepAlives {
ka.close()
}
l.keepAlives = make(map[LeaseID]*keepAlive)
l.mu.Unlock()
}()

	for {
		// resetRecv 打开一个新的lease stream并开始发送保持活动请求。
		stream, err := l.resetRecv()
		if err != nil {
			if canceledByCaller(l.stopCtx, err) {
				return err
			}
		} else {
			for {
				// 接收lease stream的返回返回
				resp, err := stream.Recv()
				if err != nil {
					if canceledByCaller(l.stopCtx, err) {
						return err
					}

					if toErr(l.stopCtx, err) == rpctypes.ErrNoLeader {
						l.closeRequireLeader()
					}
					break
				}
				// 根据LeaseKeepAliveResponse更新租约
				// 如果租约过期删除所有alive channels
				l.recvKeepAlive(resp)
			}
		}

		select {
		case <-time.After(retryConnWait):
			continue
		case <-l.stopCtx.Done():
			return l.stopCtx.Err()
		}
	}
}

// resetRecv 打开一个新的lease stream并开始发送保持活动请求。
func (l *lessor) resetRecv() (pb.Lease_LeaseKeepAliveClient, error) {
sctx, cancel := context.WithCancel(l.stopCtx)
// 建立服务端和客户端连接的lease stream
stream, err := l.remote.LeaseKeepAlive(sctx, l.callOpts...)
if err != nil {
cancel()
return nil, err
}

	l.mu.Lock()
	defer l.mu.Unlock()
	if l.stream != nil && l.streamCancel != nil {
		l.streamCancel()
	}

	l.streamCancel = cancel
	l.stream = stream

	go l.sendKeepAliveLoop(stream)
	return stream, nil
}

// sendKeepAliveLoop 在给定流的生命周期内发送保持活动请求
func (l *lessor) sendKeepAliveLoop(stream pb.Lease_LeaseKeepAliveClient) {
for {
var tosend []LeaseID

		now := time.Now()
		l.mu.Lock()
		for id, ka := range l.keepAlives {
			if ka.nextKeepAlive.Before(now) {
				tosend = append(tosend, id)
			}
		}
		l.mu.Unlock()

		for _, id := range tosend {
			r := &pb.LeaseKeepAliveRequest{ID: int64(id)}
			if err := stream.Send(r); err != nil {
				// TODO do something with this error?
				return
			}
		}

		select {
		// 每500毫秒执行一次
		case <-time.After(500 * time.Millisecond):
		case <-stream.Context().Done():
			return
		case <-l.donec:
			return
		case <-l.stopCtx.Done():
			return
		}
	}
}

// 撤销给定的租约，所有附加到租约的key将过期并被删除  
func (l *lessor) Revoke(ctx context.Context, id LeaseID) (*LeaseRevokeResponse, error) {
r := &pb.LeaseRevokeRequest{ID: int64(id)}
resp, err := l.remote.LeaseRevoke(ctx, r, l.callOpts...)
if err == nil {
return (*LeaseRevokeResponse)(resp), nil
}
return nil, toErr(ctx, err)
}
总结：

1、每次注册一个服务的分配一个租约；

2、KeepAlive通过从客户端到服务器端的流化的keep alive请求和从服务器端到客户端的流化的keep alive应答来维持租约；

3、KeepAlive会500毫秒进行一次lease stream的发送；

4、然后接收到KeepAlive发送信息回执，处理更新租约，服务处于活动状态；

5、如果在租约TTL中没有收到响应的任何保持活动的请求，删除租约；

6、Revoke撤销一个租约，所有附加到租约的key将过期并被删除。

服务发现
我们只需实现grpc在resolver中提供了Builder和Resolver接口，就能完成gRPC客户端的服务发现和负载均衡

// 创建一个resolver用于监视名称解析更新
type Builder interface {
Build(target Target, cc ClientConn, opts BuildOption) (Resolver, error)
Scheme() string
}
Build方法：为给定目标创建一个新的resolver，当调用grpc.Dial()时执行；

Scheme方法：返回此resolver支持的方案,可参考Scheme定义

// 监视指定目标的更新，包括地址更新和服务配置更新
type Resolver interface {
ResolveNow(ResolveNowOption)
Close()
}
ResolveNow方法：被 gRPC 调用，以尝试再次解析目标名称。只用于提示，可忽略该方法;

Close方法：关闭resolver。

接下来看下具体的实现

package discovery

import (
"context"
"time"

	"go.uber.org/zap"

	"go.etcd.io/etcd/api/v3/mvccpb"
	clientv3 "go.etcd.io/etcd/client/v3"
	"google.golang.org/grpc/resolver"
)

const (
schema = "etcd"
)

// Resolver for grpc client
type Resolver struct {
schema      string
EtcdAddrs   []string
DialTimeout int

	closeCh      chan struct{}
	watchCh      clientv3.WatchChan
	cli          *clientv3.Client
	keyPrifix    string
	srvAddrsList []resolver.Address

	cc     resolver.ClientConn
	logger *zap.Logger
}

// NewResolver create a new resolver.Builder base on etcd
func NewResolver(etcdAddrs []string, logger *zap.Logger) *Resolver {
return &Resolver{
schema:      schema,
EtcdAddrs:   etcdAddrs,
DialTimeout: 3,
logger:      logger,
}
}

// Scheme returns the scheme supported by this resolver.
func (r *Resolver) Scheme() string {
return r.schema
}

// Build creates a new resolver.Resolver for the given target
func (r *Resolver) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) {
r.cc = cc

	r.keyPrifix = BuildPrefix(Server{Name: target.Endpoint, Version: target.Authority})
	if _, err := r.start(); err != nil {
		return nil, err
	}
	return r, nil
}

// ResolveNow resolver.Resolver interface
func (r *Resolver) ResolveNow(o resolver.ResolveNowOptions) {}

// Close resolver.Resolver interface
func (r *Resolver) Close() {
r.closeCh <- struct{}{}
}

// start
func (r *Resolver) start() (chan<- struct{}, error) {
var err error
r.cli, err = clientv3.New(clientv3.Config{
Endpoints:   r.EtcdAddrs,
DialTimeout: time.Duration(r.DialTimeout) * time.Second,
})
if err != nil {
return nil, err
}
resolver.Register(r)

	r.closeCh = make(chan struct{})

	if err = r.sync(); err != nil {
		return nil, err
	}

	go r.watch()

	return r.closeCh, nil
}

// watch update events
func (r *Resolver) watch() {
ticker := time.NewTicker(time.Minute)
r.watchCh = r.cli.Watch(context.Background(), r.keyPrifix, clientv3.WithPrefix())

	for {
		select {
		case <-r.closeCh:
			return
		case res, ok := <-r.watchCh:
			if ok {
				r.update(res.Events)
			}
		case <-ticker.C:
			if err := r.sync(); err != nil {
				r.logger.Error("sync failed", zap.Error(err))
			}
		}
	}
}

// update
func (r *Resolver) update(events []*clientv3.Event) {
for _, ev := range events {
var info Server
var err error

		switch ev.Type {
		case mvccpb.PUT:
			info, err = ParseValue(ev.Kv.Value)
			if err != nil {
				continue
			}
			addr := resolver.Address{Addr: info.Addr, Metadata: info.Weight}
			if !Exist(r.srvAddrsList, addr) {
				r.srvAddrsList = append(r.srvAddrsList, addr)
				r.cc.UpdateState(resolver.State{Addresses: r.srvAddrsList})
			}
		case mvccpb.DELETE:
			info, err = SplitPath(string(ev.Kv.Key))
			if err != nil {
				continue
			}
			addr := resolver.Address{Addr: info.Addr}
			if s, ok := Remove(r.srvAddrsList, addr); ok {
				r.srvAddrsList = s
				r.cc.UpdateState(resolver.State{Addresses: r.srvAddrsList})
			}
		}
	}
}

// sync 同步获取所有地址信息
func (r *Resolver) sync() error {
ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)
defer cancel()
res, err := r.cli.Get(ctx, r.keyPrifix, clientv3.WithPrefix())
if err != nil {
return err
}
r.srvAddrsList = []resolver.Address{}

	for _, v := range res.Kvs {
		info, err := ParseValue(v.Value)
		if err != nil {
			continue
		}
		addr := resolver.Address{Addr: info.Addr, Metadata: info.Weight}
		r.srvAddrsList = append(r.srvAddrsList, addr)
	}
	r.cc.UpdateState(resolver.State{Addresses: r.srvAddrsList})
	return nil
}
总结：

1、watch会监听前缀的信息变更，有变更的通知，及时更新srvAddrsList的地址信息；

2、sync会定时的同步etcd中的可用的服务地址到srvAddrsList中；

3、使用UpdateState更新ClientConn的Addresses；

4、然后grpc客户端就能根据配置的具体策略发送请求到grpc的server中。

这里使用gRPC内置的负载均衡策略round_robin，根据负载均衡地址，以轮询的方式进行调用服务，来测试下服务的发现和简单的服务负载

package discovery

import (
"context"
"fmt"
"log"
"net"
"testing"
"time"

	"go.uber.org/zap"
	"google.golang.org/grpc/balancer/roundrobin"
	"google.golang.org/grpc/resolver"

	"etcd-learning/discovery/helloworld"

	"google.golang.org/grpc"
)

var etcdAddrs = []string{"127.0.0.1:2379"}

func TestResolver(t *testing.T) {
r := NewResolver(etcdAddrs, zap.NewNop())
resolver.Register(r)

	// etcd中注册5个服务
	go newServer(t, ":1001", "1.0.0", 1)
	go newServer(t, ":1002", "1.0.0", 1)
	go newServer(t, ":1003", "1.0.0", 1)
	go newServer(t, ":1004", "1.0.0", 1)
	go newServer(t, ":1006", "1.0.0", 10)

	conn, err := grpc.Dial("etcd:///hello", grpc.WithInsecure(), grpc.WithBalancerName(roundrobin.Name))
	if err != nil {
		t.Fatalf("failed to dial %v", err)
	}
	defer conn.Close()

	c := helloworld.NewGreeterClient(conn)

	// 进行十次数据请求
	for i := 0; i < 10; i++ {
		resp, err := c.SayHello(context.Background(), &helloworld.HelloRequest{Name: "abc"})
		if err != nil {
			t.Fatalf("say hello failed %v", err)
		}
		log.Println(resp.Message)
		time.Sleep(100 * time.Millisecond)
	}

	time.Sleep(10 * time.Second)
}

type server struct {
Port string
}

// SayHello implements helloworld.GreeterServer
func (s *server) SayHello(ctx context.Context, in *helloworld.HelloRequest) (*helloworld.HelloReply, error) {
return &helloworld.HelloReply{Message: fmt.Sprintf("Hello From %s", s.Port)}, nil
}

func newServer(t *testing.T, port string, version string, weight int64) {
register := NewRegister(etcdAddrs, zap.NewNop())
defer register.Stop()

	listen, err := net.Listen("tcp", port)
	if err != nil {
		log.Fatalf("failed to listen %v", err)
	}

	s := grpc.NewServer()
	helloworld.RegisterGreeterServer(s, &server{Port: port})

	info := Server{
		Name:    "hello",
		Addr:    fmt.Sprintf("127.0.0.1%s", port),
		Version: version,
		Weight:  weight,
	}

	register.Register(info, 10)

	if err := s.Serve(listen); err != nil {
		log.Fatalf("failed to server %v", err)
	}
}
这里注册了5个服务，端口号是1001到1006，循环调用10次

=== RUN   TestResolver
2021/07/24 22:44:52 Hello From :1001
2021/07/24 22:44:52 Hello From :1006
2021/07/24 22:44:53 Hello From :1001
2021/07/24 22:44:53 Hello From :1002
2021/07/24 22:44:53 Hello From :1003
2021/07/24 22:44:53 Hello From :1004
2021/07/24 22:44:53 Hello From :1006
2021/07/24 22:44:53 Hello From :1001
2021/07/24 22:44:53 Hello From :1002
2021/07/24 22:44:53 Hello From :1003
发现每次的请求会发送到不同的服务中

负载均衡
集中式LB（Proxy Model）grpc
在服务消费者和服务提供者之间有一个独立的LB，通常是专门的硬件设备如 F5，或者基于软件如LVS，HAproxy等实现。LB上有所有服务的地址映射表，通常由运维配置注册，当服务消费方调用某个目标服务时，它向LB发起请求，由LB以某种策略，比如轮询（Round-Robin）做负载均衡后将请求转发到目标服务。LB一般具备健康检查能力，能自动摘除不健康的服务实例。

该方案主要问题：

1、单点问题，所有服务调用流量都经过LB，当服务数量和调用量大的时候，LB容易成为瓶颈，且一旦LB发生故障影响整个系统；

2、服务消费方、提供方之间增加了一级，有一定性能开销。

进程内LB（Balancing-aware Client）grpc
针对第一个方案的不足，此方案将LB的功能集成到服务消费方进程里，也被称为软负载或者客户端负载方案。服务提供方启动时，首先将服务地址注册到服务注册表，同时定期报心跳到服务注册表以表明服务的存活状态，相当于健康检查，服务消费方要访问某个服务时，它通过内置的LB组件向服务注册表查询，同时缓存并定期刷新目标服务地址列表，然后以某种负载均衡策略选择一个目标服务地址，最后向目标服务发起请求。LB和服务发现能力被分散到每一个服务消费者的进程内部，同时服务消费方和服务提供方之间是直接调用，没有额外开销，性能比较好。

该方案主要问题：

1、开发成本，该方案将服务调用方集成到客户端的进程里头，如果有多种不同的语言栈，就要配合开发多种不同的客户端，有一定的研发和维护成本；

2、另外生产环境中，后续如果要对客户库进行升级，势必要求服务调用方修改代码并重新发布，升级较复杂。

独立 LB 进程（External Load Balancing Service）grpc
该方案是针对第二种方案的不足而提出的一种折中方案，原理和第二种方案基本类似。

不同之处是将LB和服务发现功能从进程内移出来，变成主机上的一个独立进程。主机上的一个或者多个服务要访问目标服务时，他们都通过同一主机上的独立LB进程做服务发现和负载均衡。该方案也是一种分布式方案没有单点问题，一个LB进程挂了只影响该主机上的服务调用方，服务调用方和LB之间是进程内调用性能好，同时该方案还简化了服务调用方，不需要为不同语言开发客户库，LB的升级不需要服务调用方改代码。

该方案主要问题：部署较复杂，环节多，出错调试排查问题不方便。

上面通过etcd实现服务发现，使用的及时第二种 进程内LB（Balancing-aware Client）。

参考
【Load Balancing in gRPC】https://github.com/grpc/grpc/blob/master/doc/load-balancing.md
【文中的代码示例】https://github.com/boilingfrog/etcd-learning/tree/main/discovery