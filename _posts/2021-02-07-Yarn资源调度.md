---
layout: post
categories: [Hadoop]
description: none
keywords: Hadoop
---
# Yarn资源调度


## Yarn产生背景
MapReduce1.0固有的缺陷，如JobTasker任务过重，存在单点故障等问题，Hadoop2.0以后的版本针对MapReduce1.0进行体系架构重新设计，这就有了YARN。在Hadoop2.0版本中，YARN提供资源管理调度服务，MapReduce不再负责资源调度管理，只是运行在YARN之上的一个纯粹的计算框架。

MapReduce 1.0架构的缺陷，最严重的限制主要是可伸缩性、资源利用和对与MapReduce不同的工作负载的支持。

如图6-1所示，在MapReduce1.0框架中，有一个称为JobTracker的主要进程，它协调在集群上运行的所有作业，分配要在TaskTracker上运行的map和reduce任务；而称为TaskTracker的下级进程，它们运行分配的任务并定期向JobTracker报告进度。JobTracker既要负责作业调度和状态监控，又要负责资源管理分配。JobTracker需要巨大的内存开销，当存在非常多的MapReduce任务时，就会造成JobTracker失败。

大型的Hadoop集群显现出了由单个JobTracker导致的可扩展瓶颈。JobTracker是集群事务的集中处理点，存在单点故障。JobTracker需要完成的任务太多，既要承载客户端提交job的分发和调度，又要管理所有job的失败、重启，监视每个DataNode的资源利用情况，造成过多的资源消耗。在TaskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到CPU、内存等资源情况。

## YARN框架介绍
为了解决可扩展性问题，一个绝妙的想法应运而生，这就是责任解耦。我们减少了单个JobTracker的职责，将部分职责委派给TaskTracker，因为集群中有许多TaskTracker。在新设计中，这个概念通过将JobTracker的双重职责（集群资源管理和任务协调）分开。新一代的资源管理调度框架YARN出现了，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度。在YARN框架中，ResourceManager负责整个集群的资源管理和分配，而任务协调工作交给ApplicationMaster，如图6-2所示。

其中要点如下：
- ResourceManager（简称RM），ResourceManager是YARN的核心组件，它一般分配在主节点上，其主要功能是负责系统资源的管理和分配。
- ApplicationMaster代替了原来的JobTracker，每当用户提交了一个应用程序，就会为这个应用程序产生一个对应的ApplicationMaster，并且这个单独进程是在其中一个子节点上运行的。它的主要功能：为运行应用向ResourceManager申请资源、在job中对Task实行调度、与NodeManager通信以启动或者停止任务、监控所有任务的运行情况，并且在任务失败的情况下，重新为任务申请资源并且重启任务。

（3）NodeManager（简称NM）代替原来的TaskTracker。NM是每个子节点上的资源和任务管理器，一方面，它会定向通过心跳信息向RM汇报本节点上的资源使用情况和各个Container的运行情况；另一方面，它会接收并且处理来自AM的Container启动和停止的各种请求。

（4）Container是YARN中对系统资源的抽象，同时它也是系统资源分配的基本单位，它封装节点上多维度资源，其中包括CPU、内存、磁盘、网络等。YARN会为每个任务分配一个Container，并且该任务只能够使用该Container中所描述的资源。值得注意的是，YARN中的Container是一个动态的资源划分单位，它是根据实际提交的应用程序所需求的资源自动生成的，换句话说，Container里边所描述的CPU、内存等资源是根据实际应用程序需求而变化的。

（5）一个分布式应用程序代替一个MapReduce作业。

在整个YARN资源管理系统当中，ResourceManager作为Master，各个节点的NodeManager作为Slave。ResorceManager组件和HDFS的名称节点NameNode部署在一个节点上，YARN的ApplicationMaster以及NodeManager是和HDFS的数据节点DataNode部署在一起的，YARN中的Container容器（代表计算资源）也是和HDFS的数据节点DataNode在一起。各个节点上NodeManager的资源由ResourceManager统计进行管理和调度。当应用程序提交后，会有一个单独的Application来对该应用程序进行跟踪和管理，同时该Application还会为该应用程序向Resource申请资源，并要求NodeManager启动该应用程序占用一定资源的任务。

## YARN工作原理
当用户给YARN提交了一个应用程序后，YARN的主要工作流程如图6-3所示（图中Yarn即YARN）。

YARN的主要工作流程说明如下：

步骤01　用户编写客户端应用程序，向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。

步骤02　ResourceManager接到客户端应用程序的请求，会为该应用程序分配一个Container，同时ResourceManager的Application Manager会与该容器所在的NodeManager通信，要求它在这个Container中启动一个ApplicationMaster。

步骤03　ApplicationMaster被创建后首先向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4到步骤7。

步骤04　ApplicationMaster采用轮询的方式，通过RPC协议向ResourceManager申请和领取资源。

步骤05　一旦ApplicationMaster申请到资源后，就会与该容器所在的NodeManager通信，要求它启动任务。

步骤06　NodeManager会为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，最后通过在容器中运行该脚本来启动任务。

步骤07　各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，从而让ApplicationMaster随时掌握各个任务的运行状态，以便可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。

步骤08　应用程序运行完成后，ApplicationMaster向ResourceManager的Application Manager注销并关闭自己。若ApplicationMaster因故失败，ResourceManager中Application Manager会监测到失败，然后将其重新启动，直到所有的任务执行完毕。

## YARN框架和MapReduce1.0框架对比
两个框架最大的区别在于原来MapReduce框架中的JobTracker和TaskTracker不见了，取而代之的是ResourceManager、NodeManager和Application Master这三个组件。

NodeManager功能比较专一，就是负责Container状态的维护，并向ResourceManger保持心跳。Application Master负责一个Job生命周期内的所有工作，类似老的框架中JobTracker。但注意每一个Job都有一个Application Master，它可以运行在ResourceManager以外的机器上。

ResourceManager只需负责资源管理，而任务调度和监控重启任务就交给Application Master做了。ResourceManager中有一个模块叫ApplicationsManager，它用于监测Application Master的运行状况，如果出问题，会将其在其他机器上重启。

总而言之，YARN相对于MapReduce1.0有如下优势：

（1）ResourceManager比JobTracker大大减少了资源消耗。ResourceManager起到了JobTracker的资源分配的作用，它做的关于作业调度的工作就只有启动、监控每个作业所属的Application Master，并重启故障的Application Master。它不再负责作业里面的不同任务的监控、调度和重启每个Task。这样使得单点故障的影响变小，恢复更加容易。

（2）MapReduce1.0既是计算框架又是资源管理调度框架，但只能支持MapReduce编程模型。而在新框架中，Application Master是可变的，可以为不同的计算框架编写自己的Application Master，使得更多的计算框架可以运行在Hadoop集群上。YARN上面可以运行各种计算框架（包括MapReduce、Spark、Storm等）。

（3）YARN的资源管理更加高效，YARN采用Container为单位进行资源管理。Container很好地起到了资源隔离的作用，让资源更好地被利用起来。

## CDH集群的YARN参数调整
YARN内存分配与管理优化，主要涉及ResourceManager、ApplicationMaster、NodeManager、Container这几个概念，CDH5版本中，同时集成了MapReduceV1和MapReduceV2（YARN）两个版本，如果集群中需要使用YARN做统一的资源调度，建议使用YARN。

根据CPU和内存公平调度资源。CDH动态资源池默认采用的DRF（即Dominant Resource Fairness）计划策略。简单地理解，就是内存不够的时候，多余的CPU就不会分配任务了，就让它空闲着；CPU不够的时候，多出来的内存也不会再启动任务了。

YARN启动任务时资源相关的参数，有如下几个参数可能会产生影响：

mapreduce.map.memory.mb，map任务内存。
mapreduce.map.cpu.vcores，map任务虚拟CPU核数。
mapreduce.reduce.memory.mb，reduce任务内存。
mapreduce.reduce.cpu.vcores，reduce任务虚拟CPU核数。
yarn.nodemanager.resource.memory-mb，容器内存。
yarn.nodemanager.resource.cpu-vcores，容器虚拟CPU核数。
yarn.scheduler.maximum-allocation-mb,，分配给容器可申请的最大内存。
yarn.scheduler.maximum-allocation-vcores，分配给容器最大虚拟CPU核数。
比如将yarn.nodemanager.resource.memory-mb配置成了16GB，甚至更大些（机器内存128GB）

重启YARN后，再次启动MapReduce任务，测试结果会发现MapReduce任务确实快了很多。

Spark的on Yarn模式，其资源分配是交给YARN的ResourceManager来进行管理的，比如，曾经有一个项目，启动Spark时报错，如图6-5所示。

解决方法如下：

从图6-5中可以看到验证Application需要的内存超过Container的最大内存，所以配置YARN中的yarn.scheduler.maximum-allocation-mb这个参数，使得Container的最大内存达到1024+384 MB以上。修改参数，如图6-6所示。


