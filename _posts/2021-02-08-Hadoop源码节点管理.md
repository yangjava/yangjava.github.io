---
layout: post
categories: [Hadoop]
description: none
keywords: Hadoop
---
# Hadoop源码
熟练掌握Hadoop基础知识，能够提升管理Hadoop集群和开发Hadoop应用程序的效率。通过阅读Hadoop源代码，能够更好地理解Hadoop的原理和运行机制。

## 基础准备与源码编译
由于Hadoop是一个分布式系统，分别由NameNode、DataNode、ResourceManager、NodeManager等多个守护进程构成，具有一定的复杂性，如果希望深入学习它的原理，还需要掌握一些调试工具的使用方法，方便对Hadoop源代码进行调试（Debug）、运行（Run），活学活用。

## 加载源码
Hadoop源代码托管在Github开源社区，任何人都可以免费获取。遵循Apache协议，用户可以阅读、修改Hadoop源代码。Mac OS操作系统和Linux操作系统自带Git环境，可以直接使用Git命令下载Hadoop源代码。Windows操作系统需要额外下载一个Git安装包，在Windows操作系统中安装好Git环境后，也可以使用Git命令进行下载。

## 编译源码
Hadoop源代码是由Maven进行管理的。各个模块分工明确，阅读和调试源代码都很方便。在Hadoop源代码根目录中通过pom.xml文件进行描述。

Hadoop模块描述
```
<!-- 各个子模块描述 -->
<modules>
     <module>hadoop-project</module>
     <module>hadoop-project-dist</module>
     <module>hadoop-assemblies</module>
     <module>hadoop-maven-plugins</module>
     <module>hadoop-common-project</module>
     <module>hadoop-hdfs-project</module>
     <module>hadoop-yarn-project</module>
     <module>hadoop-mapreduce-project</module>
     <module>hadoop-tools</module>
     <module>hadoop-dist</module>
     <module>hadoop-client</module>
     <module>hadoop-minicluster</module>
</modules>
```
其中比较核心的Project目录分别为：hadoop-common-project、hadoop-mapreduce-project、hadoop-hdfs-project和hadoop-yarn-project。
它们所承担的责任如下：
- Hadoop基础公共库
在Hadoop源代码中，hadoop-common-project目录为一个基础类库，其他模块公共的依赖类库都可以在该目录下获取。
- MapReduce算法实现
实现MapReduce算法是由hadoop-mapreduce-project目录来完成的。
- 分布式文件系统
Hadoop分布式文件系统（HDFS）的实现是由hadoop-hdfs-project目录来完成的。在Hadoop 2之后支持多个NameNode（Active&Standby），解决了之前Hadoop 1版本中存在的单点问题。
- 资源管理系统
Hadoop的资源管理与调度是由hadoop-yarn-project目录实现的，通过YARN来统一管理集群中的资源，并按照制定的策略给每个应用程序分配资源。

明确Hadoop源代码的目录结构，能使读者在编译Hadoop源代码时做到心中有数。进入Hadoop源代码编译工作之前，先确保编译环境是否准备就绪。它所依赖gcc、gcc-c++、cmake、openssl-devel、ncurses-devel等环境。

编译之后，成功生成的Hadoop二进制软件安装包在hadoop-dist/target/目录下。

## 源码结构
Hadoop 2源代码结构由几大核心子模块构成，分别是hadoop-common、hadoop-hdfs、hadoop-mapreduce、hadoop-yarn。

而每个子模块下又包含若干个功能模块
- 基础类库（Common）
Hadoop基础类库由hadoop-common子项目来实现，其内容包含读取Hadoop系统配置文件（如hdfs-site.xml、core-site.xml、yarn-site.xml等）、抽象文件系统类和Hadoop系统命令等。

例如基础类库中的conf包，该包用于读取Hadoop系统配置文件，但是读取又需要依赖文件系统来实现，而文件系统的部分功能又被抽象在fs中。

- 分布式文件系统（HDFS）
Hadoop的分布式文件系统主要由hadoop-hdfs子项目来实现，其内容包含NameNode、DataNode、Mover和Balancer等。

以读取分布式文件系统中的内容为示例来说。在客户端执行读取操作时，需要先调用基础类库中的工具类。通过和NameNode建立通信获得操作DataNode的块信息，然后再从指定的块信息中读取数据。

- 分布式计算框架（MapReduce）
Hadoop实现分布式计算框架主要由hadoop-mapreduce子项目来实现，其内容包含应用的提交、算法的实现、执行的历史记录等。

例如，以执行WordCount算法为示例来说明。在Hadoop集群中提交一个作业，用于执行WordCount算法。通过基础类库来读取配置文件信息，从分布式文件系统中获取数据源，从资源管理调度系统中获得执行所需要的资源（内存、CPU）。作业在执行过程中会产生一系列的任务，任务产生的信息会由HistoryServer记录。

- 资源调度系统（YARN）
Hadoop实现资源调度主要由hadoop-yarn子项目来实现，其内容包含资源管理与调配、应用接口（API）、资源监控等。

用户提交一个应用到Hadoop集群，由基础类库读取配置文件，由MapReduce算法实现业务逻辑，分布式文件系统提供数据源，由YARN统一管理资源的调度。

Hadoop 2对比Hadoop 1做了很多改进，如Hadoop 2实现了高可用（HA），解决了Hadoop 1的单点问题；使用新的资源调度和管理系统（YARN）；重新改进MapReduce等。

## 第一代MapReduce框架
MapReduce是Hadoop的核心算法。编写完一个MapReduce作业（Job）后，需要通过JobClient提交该Job。提交的信息会发送到JobTracker模块。

JobTracker是第一代MapReduce计算框架的核心之一，它负责与集群中的其他节点维持心跳，并负责给提交的作业（Job）分配资源，同时管理提交作业的运行状态，包含失败、重启。

TaskTracker是第一代MapReduce的另一个核心。TaskTracker的主要功能是负责监控当前节点的资源使用情况及Tasks的运行状态。

TaskTracker主要包含Map Task和Reduce Task。当Map Task执行完成后会进入Reduce Task阶段，Reduce Task执行完成后进入到Reduce阶段，将最终结果输出到Hadoop的分布式文件系统中进行保存。

TaskTracker在监控期间，需要将收集的信息通过心跳发送给JobTracker。JobTracker收到TaskTracker上报的信息后，会给新提交的作业（Job）分配专属资源。

综上所述，第一代MapReduce的架构简单、清晰，在刚面世时也是备受青睐。但随着分布式集群的规模扩展以及企业业务的增长，第一代MapReduce框架所存在的问题也逐渐暴露出来，具体问题如下：

- JobTracker是第一代MapReduce应用的入口点，如果JobTracker服务异常将导致整个集群服务不可用，存在单点问题。
- JobTracker负责的事情太多，处理了太多的任务，导致占用了过多的资源。而内存使用率会随着提交作业（Job）数的目增加而增加，这样很容易出现性能瓶颈。
- 对于TaskTracker来说，Task担当的角色过于简单，没有考虑内存和CPU的使用情况。如果存在多个消耗内存很大的Task被集中调度，容易出现内存溢出（OutOfMemoryError，简称OOM）。
- TaskTracker把资源强制拆分为Map Task Slot和Reduce Task Slot。如果在执行一个MapReduce作业时，只存在Map Task阶段，那么会出现资源浪费的情况，导致资源利用率低下。
- 对于开发者来说，阅读Hadoop 1的源代码时，可阅读性不好且代码量庞大，而且任务不够清晰明了，开发者在修复Hadoop 1的bug和维护Hadoop 1的源代码时难度会很大。

## 第二代MapReduce框架
在Hadoop 2中加入了资源管理系统这一新特性。同时，对比第一代MapReduce框架，第二代MapReduce框架重新进行了改进。

将第一代MapReduce框架中的JobTracker的两个核心功能分离成了独立的组件，并对分离后的系统组件进行了重新命名，分别是应用管理器（ApplicationsManager，AM）和资源调度器（ResouceScheduler，简称RS）。

新的资源管理器（ResourceManager，RM）将管理整个Hadoop系统的资源分配，而每一个计算节点将由YARN的代理节点NodeManager（简称NM）进行管理。NodeManager负责与ResourceManager保持通信，监管Container的生命周期以及监控每一个Container的内存、CPU、I/O等使用情况。

每一个NodeManager中的ApplicationMaster负责对应的调度和协调工作。Application Master从Resource Manager中获取资源（如内存、CPU），让NodeManager进行协同工作和任务监控。

ApplicationMaster负责向资源调度器动态地申请资源，对应用程序的状态进行监控并处理异常情况。如果期间出现问题，会在其他计算节点上进行重启。

在Hadoop集群中，YARN的ResourceManager是支持队列分层的。配置后的队列可以从集群中获取配置比例的资源，可以说ResourceManager算得上是一个资源调度器。

## HDFS源码
HDFS的核心组成就是NameNode和DataNode，开始剖析源码之前，我们要先理解HDFS的本质，HDFS是一个分布式文件系统，那么就意味着核心功能必然就是读数据和写数据，读写数据的过程会涉及到的另外一个问题，就是数据如何读，怎么写，写到哪里去，怎么取出来，这就是另外一个核心功能：元数据管理，所以我们只需要解决以上问题，很大程度上就算是精通了HDFS。

对于分布式框架，首先我们要了解它底层的网络通信架构。

Hadoop各个节点之间和客户端之间的通信是基于RPC协议，RPC（Remote Procedure Call Protocol）即远程调用协议。不同进程的方法调用，本质上就是客户端调用服务端的方法，方法的执行在服务端。

在pom.xml引入依赖：
```xml
    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <hadoop.version>2.6.0</hadoop.version>
    </properties>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
```
定义通信协议接口：
```
/**
 * 网络协议
 */
public interface Protocol {
    //定义版本号，可自定义
    long versionID=1L;
    void hello(String msg);
}
```
定义服务端实现类：
```
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.ipc.RPC;
import org.apache.hadoop.ipc.Server;


import java.io.IOException;


/**
 * RPC 服务端
 */
public class NameNodeServer implements Protocol {

    public static void main(String[] args) throws IOException {
        Server server = new RPC.Builder(new Configuration())
                .setBindAddress("localhost")
                .setPort(9999)
                .setProtocol(Protocol.class)
                .setInstance(new NameNodeServer())
                .build();
        //启动服务端
        System.out.println("我是RPC服务端，我准备启动了");
        server.start();
        System.out.println("启动完成");
    }


    @Override
    public void hello(String msg) {
        System.out.println(" hello " + msg);
    }


}
```
定义客户端类
```
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.ipc.RPC;

import java.io.IOException;
import java.net.InetSocketAddress;


/**
 * RPC 客户端
 */
public class NameNodeClient {
    public static void main(String[] args) throws IOException {
        Protocol namenode = RPC.getProxy(Protocol.class,
                Protocol.versionID,
                new InetSocketAddress("localhost", 9999),
                new Configuration());


        namenode.hello("hadoop architechure");
    }
}
```
启动NameNodeRPCServer，到服务器控制台执行jps，你会发现多了一个NameNodeRPCServer进程，所以不管是NameNode还是DataNode，其实都是一个RPC进程，于是我们可以从NameNode和DataNode这两个类入手

### 走进NameNode
我们以NameNode的启动初始化流程为例来说明。 剖析一个类，首先看类的注释说明，其次找 main() 方法，结合流程图阅读。
```
* NameNode serves as both directory namespace manager and
* "inode table" for the Hadoop DFS.  There is a single NameNode
* running in any DFS deployment.  (Well, except when there
* is a second backup/failover NameNode, or when using federated NameNodes.)
*
* The NameNode controls two critical tables:
*   1)  filename-> blocksequence (namespace)
*   2)  block-> machinelist ("inodes")
*
*
*
* NameNode服务既管理了HDFS的集群的命名空间和 "inode table"。
* 一个HDFS集群里面只有一个NameNode.(除了HA方案，或者是联邦)
*
*  Namenode管理了两张极其重要的表：
*  1）一张表管理了文件与block之间的映射关系。
*  2）另一张表管理了block文件块与 DataNode主机之间的映射关系。
*
* The first table is stored on disk and is very precious.
* The second table is rebuilt every time the NameNode comes up.
* 
* 第一张表存储到了磁盘上面。(因为文件与block块之间的关系是不会发生变化的)
* 每次NameNode重启的时候重新构建第二张映射表。
*
* 'NameNode' refers to both this class as well as the 'NameNode server'.
* The 'FSNamesystem' class actually performs most of the filesystem
* management.  The majority of the 'NameNode' class itself is concerned
* with exposing the IPC interface and the HTTP server to the outside world,
* plus some configuration management.
*
* 
* Namenode服务是由三个重要的类支撑的：
* 1）NameNode类：
*    管理配置的参数
* 2）NameNode server：
*     IPC Server:
*        NameNodeRPCServer:开放端口，等待别人调用.比如：8020/9000
*     HTTP Server:
*        NameNodeHttpServer：开放50070界面，我们可以通过这个界面了解HDFS的情况
* 3) FSNameSystem:
*    这个类非常重要，管理了HDFS的元数据。
 
找到NameNode的 main()方法入口，我们只取关键代码剖析，避免钻牛角尖，不然只会让自己越陷越深。
public static void main(String argv[]) throws Exception {
  //解析参数
  if (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, true)) {
    //如果参数异常则退出进程
   System.exit(0);
  }
 
 
 
 
  try {
    StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);
    //创建NameNode的核心代码
    NameNode namenode = createNameNode(argv, null);
    if (namenode != null) {
     //让线程阻塞在这儿。这也就是为什么大家敲jps命令的时候能一直看到NameNode进程
      namenode.join();
    }
  } catch (Throwable e) {
    LOG.error("Failed to start namenode.", e);
    terminate(1, e);
  }
}
```
在 main() 中，我们整体看下来，可以发现下面这一行是关键代码，开始创建NameNode后台进程，所以我们主要看createNameNode() 这个方法。
```
NameNode namenode = createNameNode(argv, null);
```
跳转到 createNameNode() 方法，首先会判断启动HDFS时传入的参数是否合法，然后针对参数执行不同的操作。
```

public static NameNode createNameNode(String argv[], Configuration conf)
    throws IOException {
  LOG.info("createNameNode " + Arrays.asList(argv));
  if (conf == null)
    conf = new HdfsConfiguration();
  /**
   *  我们操作HDFS集群的时候会传进来如下的参数：
   *
   *  hdfs namenode -format
   *
   *  hadoop-daemon.sh start namenode
   */
  StartupOption startOpt = parseArguments(argv);
  if (startOpt == null) {
    printUsage(System.err);
    return null;
  }
  setStartupOption(conf, startOpt);
 
 
 
 
  switch (startOpt) {
    case FORMAT: {
      boolean aborted = format(conf, startOpt.getForceFormat(),
          startOpt.getInteractiveFormat());
      terminate(aborted ? 1 : 0);
      return null; // avoid javac warning
    }
    case GENCLUSTERID: {
      System.err.println("Generating new cluster id:");
      System.out.println(NNStorage.newClusterID());
      terminate(0);
      return null;
    }
    case FINALIZE: {
      System.err.println("Use of the argument '" + StartupOption.FINALIZE +
          "' is no longer supported. To finalize an upgrade, start the NN " +
          " and then run `hdfs dfsadmin -finalizeUpgrade'");
      terminate(1);
      return null; // avoid javac warning
    }
    case ROLLBACK: {
      boolean aborted = doRollback(conf, true);
      terminate(aborted ? 1 : 0);
      return null; // avoid warning
    }
    case BOOTSTRAPSTANDBY: {
      String toolArgs[] = Arrays.copyOfRange(argv, 1, argv.length);
      int rc = BootstrapStandby.run(toolArgs, conf);
      terminate(rc);
      return null; // avoid warning
    }
    case INITIALIZESHAREDEDITS: {
      boolean aborted = initializeSharedEdits(conf,
          startOpt.getForceFormat(),
          startOpt.getInteractiveFormat());
      terminate(aborted ? 1 : 0);
      return null; // avoid warning
    }
    case BACKUP:
    case CHECKPOINT: {
      NamenodeRole role = startOpt.toNodeRole();
      DefaultMetricsSystem.initialize(role.toString().replace(" ", ""));
      return new BackupNode(conf, role);
    }
    case RECOVER: {
      NameNode.doRecovery(startOpt, conf);
      return null;
    }
    case METADATAVERSION: {
      printMetadataVersion(conf);
      terminate(0);
      return null; // avoid javac warning
    }
    case UPGRADEONLY: {
      DefaultMetricsSystem.initialize("NameNode");
      new NameNode(conf);
      terminate(0);
      return null;
    }
    default: {
      DefaultMetricsSystem.initialize("NameNode");
      //关键代码
      return new NameNode(conf);
    }
  }
}
```
我们会发现createNameNode最关键的操作就是最后返回 NameNode() 方法，加载配置。
```
public NameNode(Configuration conf) throws IOException {
  this(conf, NamenodeRole.NAMENODE);
}
```

```
protected NameNode(Configuration conf, NamenodeRole role) 
    throws IOException { 
  this.conf = conf;
  this.role = role;
  setClientNamenodeAddress(conf);
  String nsId = getNameServiceId(conf);
  String namenodeId = HAUtil.getNameNodeId(conf, nsId);
  this.haEnabled = HAUtil.isHAEnabled(conf, nsId);
  state = createHAState(getStartupOption(conf));
  this.allowStaleStandbyReads = HAUtil.shouldAllowStandbyReads(conf);
  this.haContext = createHAContext();
  try {
    initializeGenericKeys(conf, nsId, namenodeId);
    //我们在阅读分析源码的时候，我们一定要留意关键的方法。
    //初始化的配置方法
    initialize(conf);
    try {
      haContext.writeLock();
      state.prepareToEnterState(haContext);
      state.enterState(haContext);
    } finally {
      haContext.writeUnlock();
    }
  } catch (IOException e) {
    this.stop();
    throw e;
  } catch (HadoopIllegalArgumentException e) {
    this.stop();
    throw e;
  }
  this.started.set(true);
}
```
NameNode() 方法看完，发现比较敏感的方法只有initialize(conf)，其他的都可以暂时不看，这就是场景驱动阅读源码的好处，千军万马直取将领首级，从不拖泥带水，当然啦，这需要我们对代码有一定的敏感度，所以剖析源码是需要一定的基础才能做到的。
```
**
** Initialize name-node. 
* * @param conf the configuration */
protected void initialize(Configuration conf) throws IOException {  
    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) == null) {    
        String intervals = conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);   
        if (intervals != null) {      
            conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,        intervals);    
        } 
    }
 
  UserGroupInformation.setConfiguration(conf);  
    loginAsNameNodeUser(conf);
 
  NameNode.initMetrics(conf, this.getRole());  
  StartupProgressMetrics.register(startupProgress);  
  /**   * namenode的启动流程   *    服务端：   *      RPCServer   *        9000/8020   *      HttpServer   *        50070   */  
  if (NamenodeRole.NAMENODE == role) {   
  //启动HTTPServer    startHttpServer(conf);  
  }
 
  this.spanReceiverHost = SpanReceiverHost.getInstance(conf);  
  //加载元数据  
  //加载元数据这个事，目前对集群刚启动的时候，我们不做重点分析。  
  //在后面源码剖析系列会专门来讲元数据的管理。  
  //根据场景驱动，集群刚初始化启动，所以其实没什么元数据。  
  loadNamesystem(conf);    //这里就是Hadoop RPC  
  //NameNodeRPCServer里面有两个主要的RPC服务：  
  //1)ClientRPCServer: 主要管理的协议是：hdfs的客户端（用户）去操作HDFS的方法  
  //2)ServiceRPCServer: 主要管理的协议：服务之间互相进行的方法的调用（注册，心跳等）  
  rpcServer = createRpcServer(conf);  if (clientNamenodeAddress == null) {    
  // This is expected for MiniDFSCluster. Set it now using     
  // the RPC server's bind address.    
  clientNamenodeAddress =         NetUtils.getHostPortString(rpcServer.getRpcAddress());    
  LOG.info("Clients are to use " + clientNamenodeAddress + " to access"        + " this namenode/service.");  
  }  
  if (NamenodeRole.NAMENODE == role) {   
   httpServer.setNameNodeAddress(getNameNodeAddress());  
     httpServer.setFSImage(getFSImage());  
     }    
     pauseMonitor = new JvmPauseMonitor(conf);  
     pauseMonitor.start();  
     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);  
     //启动一些公共的服务。NameNode RPC的服务就是在里面启动的  
     //1)进行资源检查，检查是否有磁盘足够存储元数据  
     //2)进入安全模式检查，检查是否可以退出安全模式。  
     startCommonServices(conf);
```
可以看到 startHttpServer() 启动了HTTPServer进程，对外提供了50070访问入口。
```
private void startHttpServer(final Configuration conf) throws IOException {
  //getHttpServerBindAddress 里面设置了主机名和端口号
  httpServer = new NameNodeHttpServer(conf, this, getHttpServerBindAddress(conf));
  httpServer.start();
  httpServer.setStartupProgress(startupProgress);
}
 
 
 
 
protected InetSocketAddress getHttpServerBindAddress(Configuration conf) {
  //里面设置的是50070端口
  InetSocketAddress bindAddress = getHttpServerAddress(conf);
  // If DFS_NAMENODE_HTTP_BIND_HOST_KEY exists then it overrides the
  // host name portion of DFS_NAMENODE_HTTP_ADDRESS_KEY.
  final String bindHost = conf.getTrimmed(DFS_NAMENODE_HTTP_BIND_HOST_KEY);
  if (bindHost != null && !bindHost.isEmpty()) {
    bindAddress = new InetSocketAddress(bindHost, bindAddress.getPort());
  }
  return bindAddress;
}
 
 
 
 
protected InetSocketAddress getHttpServerAddress(Configuration conf) {
  return getHttpAddress(conf);
}
 
 
 
 
/** @return the NameNode HTTP address. 
* public static final int     DFS_NAMENODE_HTTP_PORT_DEFAULT = 50070;
* public static final String  DFS_NAMENODE_HTTP_ADDRESS_KEY =   "dfs.namenode.http-address";
* public static final String  DFS_NAMENODE_HTTP_ADDRESS_DEFAULT = "0.0.0.0:" + DFS_NAMENODE_HTTP_PORT_DEFAULT;
*/
public static InetSocketAddress getHttpAddress(Configuration conf) {
  return  NetUtils.createSocketAddr(
      conf.getTrimmed(DFS_NAMENODE_HTTP_ADDRESS_KEY, DFS_NAMENODE_HTTP_ADDRESS_DEFAULT));
```
如果有细心的小伙伴继续深究 httpserver.start() 这个方法，会发现方法里面绑定了好多的servelet，绑定的servelet越多就意味着功能越多，在这里不是我们的重点，就不做展开了。

代码继续往下走，NameNode启动了HttpServer之后，同时执行了 createRpcServer(conf) 方法，启动了NameNodeRPCServer。
```
protected NameNodeRpcServer createRpcServer(Configuration conf)
    throws IOException {
  return new NameNodeRpcServer(conf, this);
}
 
 
 
 
//由于NameNodeRpcServer(conf)方法的内容过长，在这里只取关键部门进行说明
public NameNodeRpcServer(Configuration conf, NameNode nn)
      throws IOException {
    this.nn = nn;
    this.namesystem = nn.getNamesystem();
    this.retryCache = namesystem.getRetryCache();
    this.metrics = NameNode.getNameNodeMetrics();
    
    int handlerCount = 
      conf.getInt(DFS_NAMENODE_HANDLER_COUNT_KEY, 
                  DFS_NAMENODE_HANDLER_COUNT_DEFAULT);
 
 
 
 
    RPC.setProtocolEngine(conf, ClientNamenodeProtocolPB.class,
        ProtobufRpcEngine.class);
      /**
       * 如下就是一堆协议，协议里面就会有一堆方法
       */
    
    //客户端调用namenode的那些方法，都在这个协议里面。
    ClientNamenodeProtocolServerSideTranslatorPB 
       clientProtocolServerTranslator = 
         new ClientNamenodeProtocolServerSideTranslatorPB(this);
     BlockingService clientNNPbService = ClientNamenodeProtocol.
         newReflectiveBlockingService(clientProtocolServerTranslator);
 
 
 
 
    //datanode之间需要互相调用的协议。
    DatanodeProtocolServerSideTranslatorPB dnProtoPbTranslator = 
        new DatanodeProtocolServerSideTranslatorPB(this);
    BlockingService dnProtoPbService = DatanodeProtocolService
        .newReflectiveBlockingService(dnProtoPbTranslator);
    //namenode之间互相调用的协议
    NamenodeProtocolServerSideTranslatorPB namenodeProtocolXlator = 
        new NamenodeProtocolServerSideTranslatorPB(this);
    BlockingService NNPbService = NamenodeProtocolService
          .newReflectiveBlockingService(namenodeProtocolXlator);
    ......
    //看到这里是不是觉得很熟悉，启动serviceRpcServer服务，用来监控DataNode发送过来的请求
    this.serviceRpcServer = new RPC.Builder(conf)
          .setProtocol(            org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB.class)
          .setInstance(clientNNPbService)
          .setBindAddress(bindHost)      .setPort(serviceRpcAddr.getPort()).setNumHandlers(serviceHandlerCount)
          .setVerbose(false)          .setSecretManager(namesystem.getDelegationTokenSecretManager())
          .build();
//然后又添加了很多的通信协议，最后都会被namenode继承，从而实现更多的方法
// Add all the RPC protocols that the namenode implements
      DFSUtil.addPBProtocol(conf, HAServiceProtocolPB.class, haPbService,
          serviceRpcServer);
      DFSUtil.addPBProtocol(conf, NamenodeProtocolPB.class, NNPbService,
          serviceRpcServer);
      DFSUtil.addPBProtocol(conf, DatanodeProtocolPB.class, dnProtoPbService,
          serviceRpcServer);
......
 
 
 
 
  //启动了clientRpcServer
  //这个服务是主要服务于 用户使用的客户端发送过来的请求的
this.clientRpcServer = new RPC.Builder(conf)
    .setProtocol(
    .apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB.class)
    .setInstance(clientNNPbService).setBindAddress(bindHost)
    .setPort(rpcAddr.getPort()).setNumHandlers(handlerCount)
    .setVerbose(false)
.setSecretManager(namesystem.getDelegationTokenSecretManager()).buil
```
NameNode启动完ClientRPCServer和ServiceRPCServer之后就会执行startCommonServices() 方法启动公共服务，进行磁盘资源的检查，确认是否满足元数据存储条件以及安全模式检查。
```

/** Start the services common to active and standby states */
 private void startCommonServices(Configuration conf) throws IOException {
   //元数据管理
   namesystem.startCommonServices(conf, haContext);
   registerNNSMXBean();
   if (NamenodeRole.NAMENODE != role) {
     startHttpServer(conf);
     httpServer.setNameNodeAddress(getNameNodeAddress());
     httpServer.setFSImage(getFSImage());
   }
   //RPC服务端启动起来了
   rpcServer.start();
   plugins = conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,
       ServicePlugin.class);
   for (ServicePlugin p: plugins) {
     try {
       p.start(this);
     } catch (Throwable t) {
       LOG.warn("ServicePlugin " + p + " could not be started", t);
     }
   }
   LOG.info(getRole() + " RPC up at: " + rpcServer.getRpcAddress());
   if (rpcServer.getServiceRpcAddress() != null) {
     LOG.info(getRole() + " service RPC up at: "
         + rpcServer.getServiceRpcAddress());
   }
 }
```

资源检查
```

/** 
 * Start services common to both active and standby states
 *
 * namenode启动的时候，存储资源检查
  */
void startCommonServices(Configuration conf, HAContext haContext) throws IOException {
  this.registerMBean(); // register the MBean for the FSNamesystemState
  writeLock();
  this.haContext = haContext;
  try {
    //NameNode资源检查 通过core-site.xml  hdfs-site.xml两个文件，就知道了元数据存在哪儿？
    //需要检查三个目录，因为这三个目录都涉及到了元数据
    //（1）NameNode的两个目录：存储fsiamge的目录，存储editlog的目录。但是一般情况下，或者默认情况这两个使用的是同一个目录。
    //加载了配置文件，配置文件里面有存储元数据的目录。
    nnResourceChecker = new NameNodeResourceChecker(conf);
    //检查是否有足够的磁盘存储元数据
    checkAvailableResources();
    assert safeMode != null && !isPopulatingReplQueues();
    StartupProgress prog = NameNode.getStartupProgress();
    prog.beginPhase(Phase.SAFEMODE);
    prog.setTotal(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS,
            getCompleteBlocksTotal());
    //HDFS的安全模式
    setBlockTotal();
    //启动重要心跳服务
    blockManager.activate(conf);
  } finally {
    writeUnlock();
  }
  
  registerMXBean();
  DefaultMetricsSystem.instance().register(this);
  if (inodeAttributeProvider != null) {
    inodeAttributeProvider.start();
    dir.setINodeAttributeProvider(inodeAttributeProvider);
  }
  snapshotManager.registerMXBean();
}
 
 
//检查元数据目录阈值，100M
//public static final String  DFS_NAMENODE_DU_RESERVED_KEY = "dfs.namenode.resource.du.reserved";
//public static final long DFS_NAMENODE_DU_RESERVED_DEFAULT = 1024 * 1024 * 100; // 100 MB
public NameNodeResourceChecker(Configuration conf) throws IOException {
  this.conf = conf;
  volumes = new HashMap<String, CheckedVolume>();
  
  //计算磁盘阈值
  duReserved = conf.getLong(DFSConfigKeys.DFS_NAMENODE_DU_RESERVED_KEY,
      DFSConfigKeys.DFS_NAMENODE_DU_RESERVED_DEFAULT);
}
```

- 检查磁盘资源是否足够
```
1.检查磁盘资源是否足够
 
/** * Perform resource checks and cache the results. */void checkAvailableResources() {  Preconditions.checkState(nnResourceChecker != null,      "nnResourceChecker not initialized");  //检查是否有足够的磁盘空间  //如果资源不够那么就返回hasResourcesAvailable = false，则不满足NameNode初始化条件  hasResourcesAvailable = nnResourceChecker.hasAvailableDiskSpace();}
 
/** * Set the total number of blocks in the system.  */public void setBlockTotal() {  // safeMode is volatile, and may be set to null at any time  SafeModeInfo safeMode = this.safeMode;  if (safeMode == null)    return;  //设置安全模式  //getCompleteBlocksTotal()获取所有正常的block的个数  safeMode.setBlockTotal((int)getCompleteBlocksTotal());}
```

- 检查NameNode是否处理安全模式
```
/**
 * Set the total number of blocks in the system. 
 */
public void setBlockTotal() {
  // safeMode is volatile, and may be set to null at any time
  SafeModeInfo safeMode = this.safeMode;
  if (safeMode == null)
    return;
  //设置安全模式
  //在HDFS中Block有两种状态，一种是正常的complete block数据块，一种是正在构建中的underconstruction block
  //getCompleteBlocksTotal()获取所有正常的block的个数
  safeMode.setBlockTotal((int)getCompleteBlocksTotal());
}
 
 
 
 
/**
 * Set total number of blocks.
 * 
 * total:complete状态的block的个数，也就是我们正常的block的个数
 *
 * 假设有1000 block
 * 
 */
private synchronized void setBlockTotal(int total) {
  this.blockTotal = total;
  //计算阈值
  //1000 * 0.999  = 999
  this.blockThreshold = (int) (blockTotal * threshold);
  //999
  this.blockReplQueueThreshold = 
    (int) (blockTotal * replQueueThreshold);
  if (haEnabled) {
    // After we initialize the block count, any further namespace
    // modifications done while in safe mode need to keep track
    // of the number of total blocks in the system.
    this.shouldIncrementallyTrackBlocks = true;
  }
  if(blockSafe < 0)
    this.blockSafe = 0;
  //检查安全模式
  checkMode();
}
/**
 * Check and trigger safe mode if needed. 
 */
private void checkMode() {
  // Have to have write-lock since leaving safemode initializes
  // repl queues, which requires write lock
  assert hasWriteLock();
  if (inTransitionToActive()) {
    return;
  }
  // if smmthread is already running, the block threshold must have been 
  // reached before, there is no need to enter the safe mode again
  //判断是否进入安全模式
  if (smmthread == null && needEnter()) {
    //进入安全模式
    enter();
    // check if we are ready to initialize replication queues
    if (canInitializeReplQueues() && !isPopulatingReplQueues()
        && !haEnabled) {
      initializeReplQueues();
    }
    reportStatus("STATE* Safe mode ON.", false);
    return;
  }
}
什么时候回进入安全模式：
1.当datanode汇报的block小于0.999阈值算出来的block总数
2.当存活的datanode小于一定初始化集群datanode的数量的情况
3.当NameNode元数据目录空间小于100M时
以上三种情况会导致HDFS进入安全模式
private boolean needEnter() {
  //1000 * 0.999 =999
  return (threshold != 0 && blockSafe < blockThreshold) ||
    (datanodeThreshold != 0 && getNumLiveDataNodes() < datanodeThreshold) ||
    (!nameNodeHasResourcesAvailable());
}
```
最后执行BlockManager.activate(conf)方法去启动心跳服务，执行完这一步，我们基本上完成了NameNode初始化的流程。

### DataNode
DataNode相对NameNode更加复杂一些，剖析难度较大，阅读源码的时候养成好习惯，我们在阅读核心类的时候首先还是要先看类注释，理解剖析类扮演的角色和功能。

DataNode剖析
```

 /**********************************************************
 * DataNode is a class (and program) that stores a set of
 * blocks for a DFS deployment.  A single deployment can
 * have one or many DataNodes.  Each DataNode communicates
 * regularly with a single NameNode.  It also communicates
 * with client code and other DataNodes from time to time.
 * 
 * 
 * DataNodes store a series of named blocks.  The DataNode
 * allows client code to read these blocks, or to write new
 * block data.  The DataNode may also, in response to instructions
 * from its NameNode, delete blocks or copy blocks to/from other
 * DataNodes.
 * 
 *
 * The DataNode maintains just one critical table:
 *   block-> stream of bytes (of BLOCK_SIZE or less)
 * This info is stored on a local disk.  The DataNode
 * reports the table's contents to the NameNode upon startup
 * and every so often afterwards.
 *
 * DataNodes spend their lives in an endless loop of asking
 * the NameNode for something to do.  A NameNode cannot connect
 * to a DataNode directly; a NameNode simply returns values from
 * functions invoked by a DataNode.
 *
 *
 * DataNodes maintain an open server socket so that client code 
 * or other DataNodes can read/write data.  The host/port for
 * this server is reported to the NameNode, which then sends that
 * information to clients or other DataNodes that might be interested.
 *
 * 总结：
 * 1）一个集群里面可以有很多个DataNode，这些DataNode就是用来存储数据的。
 * 2）DataNode启动了以后会周期性的跟NameNode进行通信（心跳，块汇报）,执行NameNode发送过来的指令，比如删除block、创建block等等
 * 3）NameNode不能直接操作DataNode.而是通过心跳返回值指令的方式去操作DataNode的.
 * 4) DataNode启动了以后开放了一个socket的服务（RPC）,等待别人去调用它。
 *
 **********************************************************/
 public static void main(String args[]) {
  //解析参数，参数异常则退出线程
  //USAGE: java DataNode [-regular | -rollback]
  if (DFSUtil.parseHelpArgument(args, DataNode.USAGE, System.out, true)) {
    System.exit(0);
  }
  //核心代码
  secureMain(args, null);
}
```
可以看到DataNode的 _main(...) 方法只有短短的几行代码，这样我们的目标就很明确， 核心方法就是secureMain(...)
```
public static void secureMain(String args[], SecureResources resources) {
  int errorCode = 0;
  try {
    StringUtils.startupShutdownMessage(DataNode.class, args, LOG);
    //初始化DataNode,看到这里是不是很熟悉，NameNode中也有这个方法
    DataNode datanode = createDataNode(args, null, resources);
    //等待datanode初始化完成再继续往下执行
    if (datanode != null) {
     //线程阻塞
      datanode.join();
    } else {
      errorCode = 1;
    }
  } catch (Throwable e) {
    LOG.fatal("Exception in secureMain", e);
    terminate(1, e);
  } finally {
    // We need to terminate the process here because either shutdown was called
    // or some disk related conditions like volumes tolerated or volumes required
    // condition was not met. Also, In secure mode, control will go to Jsvc
    // and Datanode process hangs if it does not exit.
    LOG.warn("Exiting Datanode");
    terminate(errorCode);
  }
}
```
上面的代码很明显，执行完 createDataNode(...) 方法之后就完成了初始化流程，这里还巧妙地运用了线程阻塞的设计，等待datanode完成初始化，我们在开发中也可以借鉴一下。
```

/** Instantiate & Start a single datanode daemon and wait for it to finish.
 *  If this thread is specifically interrupted, it will stop waiting.
 */
@VisibleForTesting
@InterfaceAudience.Private
public static DataNode createDataNode(String args[], Configuration conf,
    SecureResources resources) throws IOException {
  //初始化DataNode，返回DataNode对象
  DataNode dn = instantiateDataNode(args, conf, resources);
  //DataNode初始化完成之后，以守护进程的方式运行
  if (dn != null) {
    //启动DataNode后台线程
    dn.runDatanodeDaemon();
  }
  return dn;
}
```
初始化
```

/** Instantiate a single datanode object, along with its secure resources. 
 * This must be run by invoking{@link DataNode#runDatanodeDaemon()} 
 * subsequently. 
 */
 //初始化datanode，最终要调用runDataNodeDaemon()方法以守护进程的方式后台运行
public static DataNode instantiateDataNode(String args [], Configuration conf,
    SecureResources resources) throws IOException {
  if (conf == null)
    conf = new HdfsConfiguration();
  
  if (args != null) {
    // parse generic hadoop options
    GenericOptionsParser hParser = new GenericOptionsParser(conf, args);
    args = hParser.getRemainingArgs();
  }
  
  if (!parseArguments(args, conf)) {
    printUsage(System.err);
    return null;
  }
  Collection<StorageLocation> dataLocations = getStorageLocations(conf);
  UserGroupInformation.setConfiguration(conf);
  SecurityUtil.login(conf, DFS_DATANODE_KEYTAB_FILE_KEY,
      DFS_DATANODE_KERBEROS_PRINCIPAL_KEY);
  //重要的代码,上面都在解析参数
  return makeInstance(dataLocations, conf, resources);
}
```

```

/**
 * Make an instance of DataNode after ensuring that at least one of the
 * given data directories (and their parent directories, if necessary)
 * can be created.
 * @param dataDirs List of directories, where the new DataNode instance should
 * keep its files.
 * @param conf Configuration instance to use.
 * @param resources Secure resources needed to run under Kerberos
 * @return DataNode instance for given list of data dirs and conf, or null if
 * no directory from this directory list can be created.
 * @throws IOException
 */
static DataNode makeInstance(Collection<StorageLocation> dataDirs,
    Configuration conf, SecureResources resources) throws IOException {
  LocalFileSystem localFS = FileSystem.getLocal(conf);
  FsPermission permission = new FsPermission(
      conf.get(DFS_DATANODE_DATA_DIR_PERMISSION_KEY,
               DFS_DATANODE_DATA_DIR_PERMISSION_DEFAULT));
  DataNodeDiskChecker dataNodeDiskChecker =
      new DataNodeDiskChecker(permission);
  List<StorageLocation> locations =
      checkStorageLocations(dataDirs, localFS, dataNodeDiskChecker);
  DefaultMetricsSystem.initialize("DataNode");
  assert locations.size() > 0 : "number of data directories should be > 0";
  //重要代码,跟NameNode是一样的套路
  return new DataNode(conf, locations, resources);
}
```

```

/**
 * Create the DataNode given a configuration, an array of dataDirs,
 * and a namenode proxy
 */
//解析DataNode配置hdfs-site.xml和core-site.xml,启动DataNode
DataNode(final Configuration conf,
final List<StorageLocation> dataDirs,
final SecureResources resources) throws IOException {
        super(conf);
        this.blockScanner = new BlockScanner(this, conf);
        this.lastDiskErrorCheck = 0;
        this.maxNumberOfBlocksToLog = conf.getLong(DFS_MAX_NUM_BLOCKS_TO_LOG_KEY,
        DFS_MAX_NUM_BLOCKS_TO_LOG_DEFAULT);
        this.usersWithLocalPathAccess = Arrays.asList(
        conf.getTrimmedStrings(DFSConfigKeys.DFS_BLOCK_LOCAL_PATH_ACCESS_USER_KEY));
        this.connectToDnViaHostname = conf.getBoolean(
        DFSConfigKeys.DFS_DATANODE_USE_DN_HOSTNAME,
        DFSConfigKeys.DFS_DATANODE_USE_DN_HOSTNAME_DEFAULT);
        this.getHdfsBlockLocationsEnabled = conf.getBoolean(
        DFSConfigKeys.DFS_HDFS_BLOCKS_METADATA_ENABLED,
        DFSConfigKeys.DFS_HDFS_BLOCKS_METADATA_ENABLED_DEFAULT);
        this.supergroup = conf.get(DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY,
        DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_DEFAULT);
        this.isPermissionEnabled = conf.getBoolean(
        DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY,
        DFSConfigKeys.DFS_PERMISSIONS_ENABLED_DEFAULT);
        this.pipelineSupportECN = conf.getBoolean(
        DFSConfigKeys.DFS_PIPELINE_ECN_ENABLED,
        DFSConfigKeys.DFS_PIPELINE_ECN_ENABLED_DEFAULT);
        confVersion = "core-" +
        conf.get("hadoop.common.configuration.version", "UNSPECIFIED") +
        ",hdfs-" +
        conf.get("hadoop.hdfs.configuration.version", "UNSPECIFIED");
        // Determine whether we should try to pass file descriptors to clients.
        if (conf.getBoolean(DFSConfigKeys.DFS_CLIENT_READ_SHORTCIRCUIT_KEY,
        DFSConfigKeys.DFS_CLIENT_READ_SHORTCIRCUIT_DEFAULT)) {
        String reason = DomainSocket.getLoadingFailureReason();
        if (reason != null) {
        LOG.warn("File descriptor passing is disabled because " + reason);
        this.fileDescriptorPassingDisabledReason = reason;
        } else {
        LOG.info("File descriptor passing is enabled.");
        this.fileDescriptorPassingDisabledReason = null;
        }
        } else {
        this.fileDescriptorPassingDisabledReason =
        "File descriptor passing was not configured.";
        LOG.debug(this.fileDescriptorPassingDisabledReason);
        }
        try {
        hostName = getHostName(conf);
        LOG.info("Configured hostname is " + hostName);
        //启动datanode
        startDataNode(conf, dataDirs, resources);
        } catch (IOException ie) {
        shutdown();
        throw ie;
        }
final int dncCacheMaxSize =
        conf.getInt(DFS_DATANODE_NETWORK_COUNTS_CACHE_MAX_SIZE_KEY,
        DFS_DATANODE_NETWORK_COUNTS_CACHE_MAX_SIZE_DEFAULT) ;
        //二十三种设计模式之构建者设计模式
        datanodeNetworkCounts =
        CacheBuilder.newBuilder()
        .maximumSize(dncCacheMaxSize)
        .build(new CacheLoader<String, Map<String, Long>>() {
@Override
public Map<String, Long> load(String key) throws Exception {
final Map<String, Long> ret = new HashMap<String, Long>();
        ret.put("networkErrors", 0L);
        return ret;
        }
        });
        }
```

启动DataNode
```

/**
 * This method starts the data node with the specified conf.
 * 
 * @param conf - the configuration
 *  if conf's CONFIG_PROPERTY_SIMULATED property is set
 *  then a simulated storage based data node is created.
 * 
 * @param dataDirs - only for a non-simulated storage data node
 * @throws IOException
 */
void startDataNode(Configuration conf, 
                   List<StorageLocation> dataDirs,
                   SecureResources resources
                   ) throws IOException {
  // settings global for all BPs in the Data Node
  this.secureResources = resources;
  synchronized (this) {
    this.dataDirs = dataDirs;
  }
  this.conf = conf;
  this.dnConf = new DNConf(conf);
  checkSecureConfig(dnConf, conf, resources);
  this.spanReceiverHost = SpanReceiverHost.getInstance(conf);
  if (dnConf.maxLockedMemory > 0) {
    if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {
      throw new RuntimeException(String.format(
          "Cannot start datanode because the configured max locked memory" +
          " size (%s) is greater than zero and native code is not available.",
          DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));
    }
    if (Path.WINDOWS) {
      NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);
    } else {
      long ulimit = NativeIO.POSIX.getCacheManipulator().getMemlockLimit();
      if (dnConf.maxLockedMemory > ulimit) {
        throw new RuntimeException(String.format(
          "Cannot start datanode because the configured max locked memory" +
          " size (%s) of %d bytes is more than the datanode's available" +
          " RLIMIT_MEMLOCK ulimit of %d bytes.",
          DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,
          dnConf.maxLockedMemory,
          ulimit));
      }
    }
  }
  LOG.info("Starting DataNode with maxLockedMemory = " +
      dnConf.maxLockedMemory);
  storage = new DataStorage();
  
  // global DN settings
  registerMXBean();
  //点进去看的话会发现里面主要做了如下操作
  //1、初始化TcpPeerServer用于接收tcp请求
  //2、实例化DataXceiverServer，用于接收客户端已经其他DataNode节点之间的数据服务，并设置成守护进程的方式在后台运行
  initDataXceiver(conf);
  
  //与NameNode一样，这里启动HttpServer（httpserver2）服务
  //用于接收http请求，点进去会发现方法里面也用到了构建者设计模式
  //初始化了HttpServer2并且绑定了很多servlet       infoServer.addInternalServlet（）
  //启动了http服务 this.infoServer.start();
  startInfoServer(conf);
  
  pauseMonitor = new JvmPauseMonitor(conf);
  pauseMonitor.start();
  // BlockPoolTokenSecretManager is required to create ipc server.
  this.blockPoolTokenSecretManager = new BlockPoolTokenSecretManager();
  // Login is done by now. Set the DN user name.
  dnUserName = UserGroupInformation.getCurrentUser().getShortUserName();
  LOG.info("dnUserName = " + dnUserName);
  LOG.info("supergroup = " + supergroup);
  
  //初始化RPC的服务，添加了很多Protocol协议给DataNode去implement，处理客户端和DataNode数据请求
  /**
  *又见构建者设计模式
  *ipcServer = new RPC.Builder(conf)
  *  .setProtocol(ClientDatanodeProtocolPB.class)
  *  .setInstance(service)
  *  .setBindAddress(ipcAddr.getHostName())
  *  .setPort(ipcAddr.getPort())
  *  .setNumHandlers(
  *      conf.getInt(DFS_DATANODE_HANDLER_COUNT_KEY,
  *         DFS_DATANODE_HANDLER_COUNT_DEFAULT)).setVerbose(false)
  *  .setSecretManager(blockPoolTokenSecretManager).build();
  */
  initIpcServer(conf);
  
  metrics = DataNodeMetrics.create(conf, getDisplayName());
  metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);
  
  //创建了BlockPoolManager
  //BlockPool，一个集群就有一个BlockPool
  blockPoolManager = new BlockPoolManager(this);
  
  //周期性与NameNode通信，保持心跳，汇报情况
  blockPoolManager.refreshNamenodes(conf);
  // Create the ReadaheadPool from the DataNode context so we can
  // exit without having to explicitly shutdown its thread pool.
  readaheadPool = ReadaheadPool.getInstance();
  saslClient = new SaslDataTransferClient(dnConf.conf, 
      dnConf.saslPropsResolver, dnConf.trustedChannelResolver);
  saslServer = new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);
}
```

注册和心跳机制
```

/**
 * @param addrMap
 * @throws IOException
 */
private void doRefreshNamenodes(
    Map<String, Map<String, InetSocketAddress>> addrMap) throws IOException {
  assert Thread.holdsLock(refreshNamenodesLock);
  Set<String> toRefresh = Sets.newLinkedHashSet();
  Set<String> toAdd = Sets.newLinkedHashSet();
  Set<String> toRemove;
  
  synchronized (this) {
    // Step 1. For each of the new nameservices, figure out whether
    // it's an update of the set of NNs for an existing NS,
    // or an entirely new nameservice.
 
    //循环遍历addrMap，一般情况下只有一个nameservice，如果部署架构是HA模式或者federation模式则会存在多个nameservice   
    for (String nameserviceId : addrMap.keySet()) {
      if (bpByNameserviceId.containsKey(nameserviceId)) {
        toRefresh.add(nameserviceId);
      } else {
        //TODO toAdd里面有多少有的联邦，一个联邦就是一个NameService
        toAdd.add(nameserviceId);
      }
    }
    
    // Step 2. Any nameservices we currently have but are no longer present
    // need to be removed.
    toRemove = Sets.newHashSet(Sets.difference(
        bpByNameserviceId.keySet(), addrMap.keySet()));
    
    assert toRefresh.size() + toAdd.size() ==
      addrMap.size() :
        "toAdd: " + Joiner.on(",").useForNull("<default>").join(toAdd) +
        "  toRemove: " + Joiner.on(",").useForNull("<default>").join(toRemove) +
        "  toRefresh: " + Joiner.on(",").useForNull("<default>").join(toRefresh);
    
    // Step 3. Start new nameservices
    if (!toAdd.isEmpty()) {
      LOG.info("Starting BPOfferServices for nameservices: " +
          Joiner.on(",").useForNull("<default>").join(toAdd));
      //遍历所有的联邦集群，一个联邦里面会有两个NameNode(HA)
      //如果是2个联邦集群，那么这个地方就会有两个值
      //BPOfferService 对应 一个联邦集群
      for (String nsToAdd : toAdd) {
        ArrayList<InetSocketAddress> addrs =
        //如果里面做两个高可用，hdoop1,hadoop2
        Lists.newArrayList(addrMap.get(nsToAdd).values());
        
        //一个联邦对应一个BPOfferService
        //一个联邦里面的一个NameNode就是一个BPServiceActor
        //也就是正常来说一个BPOfferService对应两个BPServiceActor
        //hdfs-site.xml core-site.xml
        BPOfferService bpos = createBPOS(addrs);
        bpByNameserviceId.put(nsToAdd, bpos);
        offerServices.add(bpos);
      }
    }
    //DataNode向NameNode进行注册和心跳
    startAll();
  }
  // Step 4. Shut down old nameservices. This happens outside
  // of the synchronized(this) lock since they need to call
  // back to .remove() from another thread
  if (!toRemove.isEmpty()) {
    LOG.info("Stopping BPOfferServices for nameservices: " +
        Joiner.on(",").useForNull("<default>").join(toRemove));
    
    for (String nsToRemove : toRemove) {
      BPOfferService bpos = bpByNameserviceId.get(nsToRemove);
      bpos.stop();
      bpos.join();
      // they will call remove on their own
    }
  }
  
  // Step 5. Update nameservices whose NN list has changed
  if (!toRefresh.isEmpty()) {
    LOG.info("Refreshing list of NNs for nameservices: " +
        Joiner.on(",").useForNull("<default>").join(toRefresh));
    
    for (String nsToRefresh : toRefresh) {
      BPOfferService bpos = bpByNameserviceId.get(nsToRefresh);
      ArrayList<InetSocketAddress> addrs =
        Lists.newArrayList(addrMap.get(nsToRefresh).values());
      bpos.refreshNNList(addrs);
    }
  }
}
```

```

/**
 * No matter what kind of exception we get, keep retrying to offerService().
 * That's the loop that connects to the NameNode and provides basic DataNode
 * functionality.
 *
 * Only stop when "shouldRun" or "shouldServiceRun" is turned off, which can
 * happen either at shutdown or due to refreshNamenodes.
 */
@Override
public void run() {
  LOG.info(this + " starting to offer service");
  
  //注册+心跳
  
  try {
  //这里的设计很巧妙，直接用来while死循环，为了保证DataNode能注册成功，如果注册过程中发生异常，则捕获异常，沉睡5秒钟并重试，注册完成则break跳出循环继续往下执行
    while (true) {
      // init stuff
      try {
        //注册核心代码
        connectToNNAndHandshake();
        break;
      } catch (IOException ioe) {
        // Initial handshake, storage recovery or registration failed
        runningState = RunningState.INIT_FAILED;
        if (shouldRetryInit()) {
          // Retry until all namenode's of BPOS failed initialization
          LOG.error("Initialization failed for " + this + " "
              + ioe.getLocalizedMessage());
          //如果有问题sleep 5秒
          sleepAndLogInterrupts(5000, "initializing");
        } else {
          runningState = RunningState.FAILED;
          LOG.fatal("Initialization failed for " + this + ". Exiting. ", ioe);
          return;
        }
      }
    }
    //注册结束了
    runningState = RunningState.RUNNING;
    while (shouldRun()) {
      try {
       //发送心跳
        offerService();
      } catch (Exception ex) {
        LOG.error("Exception in BPOfferService for " + this, ex);
        sleepAndLogInterrupts(5000, "offering service");
      }
    }
    runningState = RunningState.EXITED;
  } catch (Throwable ex) {
    LOG.warn("Unexpected exception in block pool " + this, ex);
    runningState = RunningState.FAILED;
  } finally {
    LOG.warn("Ending block pool service for: " + this);
    cleanUp();
  }
}
```

接下来我们把重点放在connectToNNAndHandshake(...) 和offerService(...) 这两个方法上
```

private void connectToNNAndHandshake() throws IOException {
  //获取到namenode的代理
  //RPC的客户端
  //datanode(  获取到了代理 ->(hostname ,port) namenode
  bpNamenode = dn.connectToNN(nnAddr);
  // First phase of the handshake with NN - get the namespace
  // info.
  //开始尝试与NameNode请求通信获取NameNode信息，
  NamespaceInfo nsInfo = retrieveNamespaceInfo();
  
  // Verify that this matches the other NN in this HA pair.
  // This also initializes our block pool in the DN if we are
  // the first NN connection for this BP.
  //校验NamespaceInfo的信息。
  // datanode  -> HA()
  bpos.verifyAndSetNamespaceInfo(nsInfo);
  
  // Second phase of the handshake with the NN.
  //注册
  register(nsInfo);
}
 
 
/**
 * Register one bp with the corresponding NameNode
 * <p>
 * The bpDatanode needs to register with the namenode on startup in order
 * 1) to report which storage it is serving now and 
 * 2) to receive a registrationID
 *  
 * issued by the namenode to recognize registered datanodes.
 * 
 * @param nsInfo current NamespaceInfo
 * @see FSNamesystem#registerDatanode(DatanodeRegistration)
 * @throws IOException
 */
void register(NamespaceInfo nsInfo) throws IOException {
  // The handshake() phase loaded the block pool storage
  // off disk - so update the bpRegistration object from that info
 
//创建注册信息
  bpRegistration = bpos.createRegistration();
  LOG.info(this + " beginning handshake with NN");
  while (shouldRun()) {
    try {
      // Use returned registration from namenode with updated fields
      //调用NameNodeRPC服务端的registerDatanode方法
      
      bpRegistration = bpNamenode.registerDatanode(bpRegistration);
      //如果执行到这儿，说明注册过程已经完成了。
      bpRegistration.setNamespaceInfo(nsInfo);
      break;
    } catch(EOFException e) {  // namenode might have just restarted
      LOG.info("Problem connecting to server: " + nnAddr + " :"
          + e.getLocalizedMessage());
      sleepAndLogInterrupts(1000, "connecting to server");
    } catch(SocketTimeoutException e) {  // namenode is busy
      LOG.info("Problem connecting to server: " + nnAddr);
      sleepAndLogInterrupts(1000, "connecting to server");
    }
  }
  
  LOG.info("Block pool " + this + " successfully registered with NN");
  bpos.registrationSucceeded(this, bpRegistration);
  // random short delay - helps scatter the BR from all DNs
  scheduleBlockReport(dnConf.initialBlockReportDelay);
}
```

```

/**
 * Main loop for each BP thread. Run until shutdown,
 * forever calling remote NameNode functions.
 */
private void offerService() throws Exception {
  LOG.info("For namenode " + nnAddr + " using"
      + " DELETEREPORT_INTERVAL of " + dnConf.deleteReportInterval + " msec "
      + " BLOCKREPORT_INTERVAL of " + dnConf.blockReportInterval + "msec"
      + " CACHEREPORT_INTERVAL of " + dnConf.cacheReportInterval + "msec"
      + " Initial delay: " + dnConf.initialBlockReportDelay + "msec"
      + "; heartBeatInterval=" + dnConf.heartBeatInterval);
  //
  // Now loop for a long time....
  //周期性心跳
  while (shouldRun()) {
    try {
      final long startTime = monotonicNow();
      //
      // Every so often, send heartbeat or block-report
      //心跳是每3秒进行一次
      /**
       *heartBeatInterval = conf.getLong(DFS_HEARTBEAT_INTERVAL_KEY,
       *DFS_HEARTBEAT_INTERVAL_DEFAULT) * 1000L;
       */
      if (startTime - lastHeartbeat >= dnConf.heartBeatInterval) {
        //
        // All heartbeat messages include following info:
        // -- Datanode name
        // -- data transfer port
        // -- Total capacity
        // -- Bytes remaining
        //
        lastHeartbeat = startTime;
        if (!dn.areHeartbeatsDisabledForTests()) {
         //NameNode是不直接跟DataNode进行连接的。
         //DataNode发送心跳给NameNode
         //NameNode接收到心跳以后，会返回来一些指令
         //DataNode接收到这些指令以后，根据这些指令做对应的操作。
          
          //发送心跳，返回来的是NameNode给的响应指令
          HeartbeatResponse resp = sendHeartBeat();
          assert resp != null;
          dn.getMetrics().addHeartbeat(monotonicNow() - startTime);
          // If the state of this NN has changed (eg STANDBY->ACTIVE)
          // then let the BPOfferService update itself.
          //
          // Important that this happens before processCommand below,
          // since the first heartbeat to a new active might have commands
          // that we should actually process.
          bpos.updateActorStatesFromHeartbeat(
              this, resp.getNameNodeHaState());
          state = resp.getNameNodeHaState().getState();
          if (state == HAServiceState.ACTIVE) {
            handleRollingUpgradeStatus(resp);
          }
          long startProcessCommands = monotonicNow();
          //获取到一些namenode发送过来的指令        
          if (!processCommand(resp.getCommands()))
            continue;
          long endProcessCommands = monotonicNow();
          if (endProcessCommands - startProcessCommands > 2000) {
            LOG.info("Took " + (endProcessCommands - startProcessCommands)
                + "ms to process " + resp.getCommands().length
                + " commands from NN");
          }
        }
      }
      if (sendImmediateIBR ||
          (startTime - lastDeletedReport > dnConf.deleteReportInterval)) {
        reportReceivedDeletedBlocks();
        lastDeletedReport = startTime;
      }
      List<DatanodeCommand> cmds = blockReport();
      processCommand(cmds == null ? null : cmds.toArray(new DatanodeCommand[cmds.size()]));
      DatanodeCommand cmd = cacheReport();
      processCommand(new DatanodeCommand[]{ cmd });
      //
      // There is no work to do;  sleep until hearbeat timer elapses, 
      // or work arrives, and then iterate again.
      //
      long waitTime = dnConf.heartBeatInterval - 
      (monotonicNow() - lastHeartbeat);
      synchronized(pendingIncrementalBRperStorage) {
        if (waitTime > 0 && !sendImmediateIBR) {
          try {
            pendingIncrementalBRperStorage.wait(waitTime);
          } catch (InterruptedException ie) {
            LOG.warn("BPOfferService for " + this + " interrupted");
          }
        }
      } // synchronized
    } catch(RemoteException re) {
      String reClass = re.getClassName();
      if (UnregisteredNodeException.class.getName().equals(reClass) ||
          DisallowedDatanodeException.class.getName().equals(reClass) ||
          IncorrectVersionException.class.getName().equals(reClass)) {
        LOG.warn(this + " is shutting down", re);
        shouldServiceRun = false;
        return;
      }
      LOG.warn("RemoteException in offerService", re);
      try {
        long sleepTime = Math.min(1000, dnConf.heartBeatInterval);
        Thread.sleep(sleepTime);
      } catch (InterruptedException ie) {
        Thread.currentThread().interrupt();
      }
    } catch (IOException e) {
      LOG.warn("IOException in offerService", e);
    }
    processQueueMessages();
  } // while (shouldRun())
} // offerService
```

### 元数据管理
在HDFS中我们先将集群数据分为两种：元数据和用户数据，元数据主要是指整个集群的信息，包括数据目录信息、节点信息、数据块位置存储信息、集群通信信息以及心跳信息等等，最重要的用途就是方便集群管理，快速地找到正确有效的信息。

而用户数据顾名思义主要是指外部写入到HDFS集群中的业务数据，与业务场景息息相关，存储用户数据是HDFS集群的核心功能。可以看出，从HDFS本身出发，元数据的管理是HDFS对外提供服务极其重要的一环。

双缓存+分段加锁

下面先用简单的代码模拟一下HDFS写数据的模型。
```
/**
 *
 *  面向对象编程，构建元数据对象
 *
 */
public class FSEditLog {
    private long txid=0L;
    private DoubleBuffer editLogBuffer=new DoubleBuffer();
    //当前是否正在往磁盘里面刷写数据
    private volatile Boolean isSyncRunning = false;
    private volatile Boolean isWaitSync = false;

    private volatile Long syncMaxTxid = 0L;
    /**
     * 一个线程 就会有自己一个ThreadLocal的副本
     */
    private ThreadLocal<Long> localTxid=new ThreadLocal<Long>();

    public static void main(String[] args) {
        final  FSEditLog fsEditLog = new FSEditLog();
        //模拟多线程操作
        for (int i =0; i < 50; i++){
            new Thread(new Runnable() {
                public void run() {

                    for (int j = 0; j < 1000; j++) {
                        fsEditLog.editLog("hdfs metadata");
                    }

                }
            }).start();

        }
    }

    /**
     * 写元数据日志方法
     *
     * @param content
     */
    public void editLog(String content){//mkdir /data
        /**
         * 在这里实现分段加锁，为了保证线程安全，操作的顺序性
         *保证元数据id的唯一性且自动递增
         * 线程1，线程2， 线程3
         */
        synchronized (this){
            //线程1
            //日志的ID号，元数据信息的ID号。
            txid++; 
            /**
             * 每个线程都会有自己的一个副本。
             * 线程1，1
             * 线程2，2
             * 线程3，3
             */
            localTxid.set(txid);
            EditLog log = new EditLog(txid, content);
            //往内存里面写数据
            editLogBuffer.write(log);
        } //释放锁

        /**
         * 内存1：
         * 线程1，1 元数据1
         * 线程2，2 元数据2
         * 线程3，3 元数据3
         */
        logSync();
    }

    private  void logSync(){

        synchronized (this){
            //当前是否正在往磁盘写数据，默认是false
            if(isSyncRunning){
                //当前的元数据信息的编号就是：2

                long txid = localTxid.get();
                // 2 <= 3
                //4 <= 3
                //5 <= 3
                if(txid <= syncMaxTxid){
                    return;
                }

                if(isWaitSync){
                    //直接返回
                    return;
                }
                //重新赋值
                isWaitSync = true;

                while(isSyncRunning){
                    try {
                         //释放锁
                        /**
                         * 1) 时间到了
                         * 2）被唤醒了
                         */
                        wait(2000);
                    }catch (Exception e){
                        e.printStackTrace();
                    }
                }
                //重新赋值
                isWaitSync = false;
            }

            //交换内存
            //真正的源码里面是有各种判断的。
            //频繁的交换内存，也是很影响性能的。
            editLogBuffer.setReadyToSync();

            if(editLogBuffer.currentBuffer.size() > 0) {
                //获取当前 内存2（正在往磁盘上面写数据的那个内存）
                //里面元数据日志日志编号最大的是多少
                syncMaxTxid = editLogBuffer.getSyncMaxTxid();

            }

            //说明接下来就要往磁盘上面写元数据日志信息了。
            isSyncRunning = true;
        } //释放锁

        //往磁盘上面写数据（这个操作是很耗费时间的）
        /**
         * 不加锁
         * 在最耗费时间的这段代码上面是没有加锁
         * 
         */
        editLogBuffer.flush(); //这个地方写完了。

        synchronized (this) {
            //状态恢复
            isSyncRunning = false;
            //唤醒当前wait的线程。
            notify();
        }

    }


    /**
     * 使用了面向对象的思想，把一条日志看成一个对象。
     * 日志信息，或者就是我们说的元数据信息。
     */
    class EditLog{ //
        //日志的编号，递增，唯一。
        long txid;
        //日志内容
        String content;

        //构造函数
        public EditLog(long txid,String content){
            this.txid = txid;
            this.content = content;
        }

        //方便我们打印日志
        @Override
        public String toString() {
            return "EditLog{" +
                    "txid=" + txid +
                    ", content='" + content + '\'' +
                    '}';
        }
    }

    /**
     * 双缓冲算法
     */
    class DoubleBuffer{

        //内存1
        LinkedList<EditLog> currentBuffer = new LinkedList<EditLog>();
        //内存2
        LinkedList<EditLog> syncBuffer= new LinkedList<EditLog>();

        /**
         * 把数据写到当前内存
         * @param log
         */
        public void write(EditLog log){
            currentBuffer.add(log);
        }

        /**
         * 两个内存交换数据
         */
        public void setReadyToSync(){
            LinkedList<EditLog> tmp= currentBuffer;
            currentBuffer = syncBuffer;
            syncBuffer = tmp;
        }

        /**
         * 获取当前正在刷磁盘的内存里的ID最大的值。
         * @return
         */
        public Long getSyncMaxTxid(){
            return syncBuffer.getLast().txid;
        }

        /**
         * 就是把数据刷写到磁盘上面
         * 为了演示效果，所以我们打印出来
         */
        public void flush(){
            for(EditLog log:syncBuffer){
                System.out.println("存入磁盘日志信息："+log);
            }
            //清空内存
            syncBuffer.clear();
        }
    }
}
```
HDFS很巧妙运用了双缓存机制和分段加锁的方式实现了快速写入数据流量，包括现在外面很多公司进行二次开发的Hadoop，有很大部分优化是将重点放在降低锁粒度上，可以极大提高HDFS数据读写性能。

### 元数据管理
在开始剖析源码之前我们有必要先了解FileSystem这个类，FileSystem是分布式文件系统的抽象类，是Hadoop提供给用户的接口类，封装了大量的操作API，比如目录创建、删除、文件读写等等，

也是我们解析元数据管理要聚焦的重点，会围绕它进行展开，以点破面，我们可以针对FileSystem中的某一个元数据管理方法进行剖析，如写数据进行解析，其他方法同理。对于一个文件系统来说，我们写数据的第一件事情就是先创建目录：
```
/**
 * Call {@link #mkdirs(Path, FsPermission)} with default permission.
 * 找到这个方法的实现类，FileSystem类有指明是 DistributedFileSystem
 */
public boolean mkdirs(Path f) throws IOException {
  //重要，执行创建目录
  return mkdirs(f, FsPermission.getDirDefault());
}

/**
 * Create a directory and its parent directories.
 *
 * See {@link FsPermission#applyUMask(FsPermission)} for details of how
 * the permission is applied.
 *
 * @param f           The path to create
 * @param permission  The permission.  See FsPermission#applyUMask for 
 *                    details about how this is used to calculate the
 *                    effective permission.
 */
@Override
public boolean mkdirs(Path f, FsPermission permission) throws IOException {
  return mkdirsInternal(f, permission, true);
}

private boolean mkdirsInternal(Path f, final FsPermission permission,
    final boolean createParent) throws IOException {
  statistics.incrementWriteOps(1);
  Path absF = fixRelativePart(f);
  return new FileSystemLinkResolver<Boolean>() {
    @Override
    public Boolean doCall(final Path p)
        throws IOException, UnresolvedLinkException {
    //关键代码，获取目录路径以及权限
      return dfs.mkdirs(getPathName(p), permission, createParent);
    }
    @Override
    public Boolean next(final FileSystem fs, final Path p)
        throws IOException {
      // FileSystem doesn't have a non-recursive mkdir() method
      // Best we can do is error out
      if (!createParent) {
        throw new IOException("FileSystem does not support non-recursive"
            + "mkdir");
      }
      return fs.mkdirs(p, permission);
    }
  }.resolve(this, absF);
}
```
接下来我们只需要深入了解dfs.mkdirs(...）这个方法的具体实现。
```
/**
 *
 * Create a directory (or hierarchy of directories) with the given name and
 * permission.
 *
 * @param src
 *            The path of the directory being created
 * @param permission
 *            The permission of the directory being created. If permission ==
 *            null, use {@link FsPermission#getDefault()}.
 * @param createParent
 *            create missing parent directory if true
 * 
 * @return True if the operation success.
 * 
 * @see ClientProtocol#mkdirs(String, FsPermission, boolean)
 */
public boolean mkdirs(String src, FsPermission permission, boolean createParent) throws IOException {
   if (permission == null) {
      permission = FsPermission.getDefault();
   }
   //检查目录权限，这个不是我们关注的重点，可以先略过
   FsPermission masked = permission.applyUMask(dfsClientConf.uMask);
   //一眼看下来，眼里只有它
   return primitiveMkdir(src, masked, createParent);
}

/**
 * Same {{@link #mkdirs(String, FsPermission, boolean)} except that the
 * permissions has already been masked against umask.
 */
public boolean primitiveMkdir(String src, FsPermission absPermission) throws IOException {
   return primitiveMkdir(src, absPermission, true);
}

/**
 * Same {{@link #mkdirs(String, FsPermission, boolean)} except that the
 * permissions has already been masked against umask.
 */
public boolean primitiveMkdir(String src, FsPermission absPermission, boolean createParent) throws IOException {
   checkOpen();
   if (absPermission == null) {
      absPermission = FsPermission.getDefault().applyUMask(dfsClientConf.uMask);
   }

   if (LOG.isDebugEnabled()) {
      LOG.debug(src + ": masked=" + absPermission);
   }
   TraceScope scope = Trace.startSpan("mkdir", traceSampler);
   try {

   //Hadoop的RPC设计，调用NameNodeRPC服务端的代码
  //从这里我们也可以和HDFS架构中的NameNode功能对应起来，最终进行元数据管理的是由NameNode实现的
      return namenode.mkdirs(src, absPermission, createParent);
   } catch (RemoteException re) {
      throw re.unwrapRemoteException(AccessControlException.class, InvalidPathException.class,
            FileAlreadyExistsException.class, FileNotFoundException.class, ParentNotDirectoryException.class,
            SafeModeException.class, NSQuotaExceededException.class, DSQuotaExceededException.class,
            UnresolvedPathException.class, SnapshotAccessControlException.class);
   } finally {
      scope.close();
   }
}
```
如果对RPC比较熟悉的小伙伴应该知道接下来我们应该去前面说到的RPC服务端NameNodeRPCServer去找到mkdirs(...)实现方法，顺便验证一下我们的思路是否是正确的，如果能找到mkdirs()方法，则思路没错，反之则说明我们走偏了。
```
@Override 
public boolean mkdirs(String src, FsPermission masked, boolean createParent)
    throws IOException {
    //检查NameNode是否启动
  checkNNStartup();
  if(stateChangeLog.isDebugEnabled()) {
    stateChangeLog.debug("*DIR* NameNode.mkdirs: " + src);
  }
  if (!checkPathLength(src)) {
    throw new IOException("mkdirs: Pathname too long.  Limit " 
                          + MAX_PATH_LENGTH + " characters, " + MAX_PATH_DEPTH + " levels.");
  }
  //调用FSNameSystem创建目录的方法
  return namesystem.mkdirs(src,
      new PermissionStatus(getRemoteUser().getShortUserName(),
          null, masked), createParent);
}

/**
 * Create all the necessary directories
 */
boolean mkdirs(String src, PermissionStatus permissions,
    boolean createParent) throws IOException {
  HdfsFileStatus auditStat = null;
  checkOperation(OperationCategory.WRITE);
  //加写锁
  writeLock();
  try {
    checkOperation(OperationCategory.WRITE);
    //检查NameNode是否处于安全模式，如果NameNode处于安全模式下，我们是无法创建目录的
    checkNameNodeSafeMode("Cannot create directory " + src);
    //创建目录
    auditStat = FSDirMkdirOp.mkdirs(this, src, permissions, createParent);
  } catch (AccessControlException e) {
    logAuditEvent(false, "mkdirs", src);
    throw e;
  } finally {
    //释放锁
    writeUnlock();
  }
  //不管创建目录成不成功都要将数据日志持久化
  getEditLog().logSync();
  logAuditEvent(true, "mkdirs", src, null, auditStat);
  return true;
}
```
说到目录，我们自然而然会想到目录树结构，我们可能创建一级目录，也可能创建多级目录，HDFS的目录树结构和Linux的一样。
```
static HdfsFileStatus mkdirs(FSNamesystem fsn, String src,
    PermissionStatus permissions, boolean createParent) throws IOException {
  //HDFS是如何管理目录树的
  /**
   *  FSDirectory 目录树
   *
   *  hadoop  fs  -ls /
   *
   *  hdfs dfs -ls /
   *
   * 
   *
   */
  FSDirectory fsd = fsn.getFSDirectory();
  if(NameNode.stateChangeLog.isDebugEnabled()) {
    NameNode.stateChangeLog.debug("DIR* NameSystem.mkdirs: " + src);
  }
  if (!DFSUtil.isValidName(src)) {
    throw new InvalidPathException(src);
  }
  FSPermissionChecker pc = fsd.getPermissionChecker();
  byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);
  fsd.writeLock();
  try {
    /**
     * hadoop fs -mkdir /user/hive/warehouse/data/test
     * fsSystem.mkdirs(new Path("/user/hive/warehouse/data/test"))
     *
     */
   //解析要创建目录的路径 /user/hive/warehouse/data/test
    src = fsd.resolvePath(pc, src, pathComponents);
    INodesInPath iip = fsd.getINodesInPath4Write(src);
    if (fsd.isPermissionEnabled()) {
      fsd.checkTraverse(pc, iip);
    }
    //  /user/hive/warehouse
    //  /user/hive/warehouse/data/test
    //找到最后一个node，即最后一级
    /**
     * 比如我们现在已经存在的目录是 /user/hive/warehouse
     * 我们需要创建的目录是：/user/hive/warehouse/data/test
     * 首先找到最后一个INode,其实就是warehouse 这个INode
     */
    final INode lastINode = iip.getLastINode();
    if (lastINode != null && lastINode.isFile()) {
      throw new FileAlreadyExistsException("Path is not a directory: " + src);
    }

    INodesInPath existing = lastINode != null ? iip : iip.getExistingINodes();
    if (lastINode == null) {
      if (fsd.isPermissionEnabled()) {
        fsd.checkAncestorAccess(pc, iip, FsAction.WRITE);
      }

      if (!createParent) {
        fsd.verifyParentDir(iip, src);
      }

      // validate that we have enough inodes. This is, at best, a
      // heuristic because the mkdirs() operation might need to
      // create multiple inodes.
      fsn.checkFsObjectLimit();
      /**
       * 已存在：/user/hive/warehouse
       * 要创建：/user/hive/warehouse/data/mytable
       * 需要创建的目录 /data/mytable
       * 计算要创建的目录结构与已存在的目录结构的差值
       */
      List<String> nonExisting = iip.getPath(existing.length(),
          iip.length() - existing.length());
      int length = nonExisting.size();
      //需要创建多级目录执行这里的逻辑
      if (length > 1) {
        List<String> ancestors = nonExisting.subList(0, length - 1);
        // Ensure that the user can traversal the path by adding implicit
        // u+wx permission to all ancestor directories
        existing = createChildrenDirectories(fsd, existing, ancestors,
            addImplicitUwx(permissions, permissions));
        if (existing == null) {
          throw new IOException("Failed to create directory: " + src);
        }
      }
      //如果只需要创建一个目录就执行下面的判断
      if ((existing = createChildrenDirectories(fsd, existing,
          nonExisting.subList(length - 1, length), permissions)) == null) {
        throw new IOException("Failed to create directory: " + src);
      }
    }
    return fsd.getAuditFileInfo(existing);
  } finally {
    fsd.writeUnlock();
  }
}
```
创建完目录后将元数据进行持久化，元数据的持久化分为两种，一种是存于内存中，一种是存于磁盘，对应HDFS中的类分别为FSDirectory和FSNameSystem，先写内存再刷磁盘，这里是我们要特别关注的地方，双缓存发生的地方就在下面代码中。
```
private static INodesInPath createSingleDirectory(FSDirectory fsd,
    INodesInPath existing, String localName, PermissionStatus perm)
    throws IOException {
  assert fsd.hasWriteLock();
  //更新文件目录树，这棵目录树是存在于内存中的，由FSNameSystem管理的
  //更新内存里面的数据
  existing = unprotectedMkdir(fsd, fsd.allocateNewInodeId(), existing,
      localName.getBytes(Charsets.UTF_8), perm, null, now());
  if (existing == null) {
    return null;
  }

  final INode newNode = existing.getLastINode();
  // Directory creation also count towards FilesCreated
  // to match count of FilesDeleted metric.
  NameNode.getNameNodeMetrics().incrFilesCreated();

  String cur = existing.getPath();
  //把元数据信息记录到磁盘上（但是一开始先写到内存）
  //往磁盘上面记录元数据日志
  fsd.getEditLog().logMkDir(cur, newNode);
  if (NameNode.stateChangeLog.isDebugEnabled()) {
    NameNode.stateChangeLog.debug("mkdirs: created directory " + cur);
  }
  return existing;
}

/** 
 * Add create directory record to edit log
 */
public void logMkDir(String path, INode newNode) {
  PermissionStatus permissions = newNode.getPermissionStatus();

  //创建日志对象
  MkdirOp op = MkdirOp.getInstance(cache.get())
    .setInodeId(newNode.getId())
    .setPath(path)
    .setTimestamp(newNode.getModificationTime())
    .setPermissionStatus(permissions);
  AclFeature f = newNode.getAclFeature();
  if (f != null) {
    op.setAclEntries(AclStorage.readINodeLogicalAcl(newNode));
  }
  XAttrFeature x = newNode.getXAttrFeature();
  if (x != null) {
    op.setXAttrs(x.getXAttrs());
  }
  //记录日志
  logEdit(op);
}
/**
 * Write an operation to the edit log. Do not sync to persistent
 * store yet.
 */
void logEdit(final FSEditLogOp op) {
  //采用了synchronized关键字和事务保证线程安全
  synchronized (this) {
    assert isOpenForWrite() :
      "bad state: " + state;

    // wait if an automatic sync is scheduled
    //一开始不需要等待
    waitIfAutoSyncScheduled();
    //最重要的就是生成了全局唯一的事务ID（日志）

    //开启事务，获取当前的独一无二的事务ID，里面调用了ThreadLocal
    long start = beginTransaction();
    op.setTransactionId(txid);

    try {
     /**
      * 1)  namenode editlog 文件缓冲里面
      * 2） journalnode的内存缓冲
      * 有兴趣的可以关注以下两个类的write方法
      * JournalSetOutputStream
      * QuorumOutputStream
      */
     //把元数据写入到内存缓冲区、write() 方法可以好好研究一下，逻辑比较复杂，里面不仅实现了将元数据写到本地磁盘中，同时还将元数据写到journalnode，并且涉及将fsimage定期checkpoint同步提交到active namenode磁盘，内容比较繁琐，本文不方便展开，感兴趣的同学可以自行阅读。
      editLogStream.write(op);
    } catch (IOException ex) {
      // All journals failed, it is handled in logSync.
    } finally {
      op.reset();
    }

    //结束事务
    endTransaction(start);

    // check if it is time to schedule an automatic sync
    // 看当前的内存大小是否 >= 512kb = true
    //这个条件决定了，两个内存是否交换数据
    //如果当前的内存写满了，512kb >= 512 kb 我们这儿就会返回true
    // !ture = false
    // !false =true
    if (!shouldForceSync()) {
      return;
    }
   //如果到这儿就说明 当前的那个缓冲区存满了
    isAutoSyncScheduled = true;
  } 
  //交换内存，把数据持久化到磁盘
  logSync(); 
}
```
交换内存，将数据刷写到磁盘，清空内存进行交换。
```
public void logSync() {
  long syncStart = 0;

  // Fetch the transactionId of this thread.获取线程的事务id
  long mytxid = myTransactionId.get().txid;

  boolean sync = false;
  try {
    EditLogOutputStream logStream = null;
    synchronized (this) {
      try {
        printStatistics(false);
        //比较当前线程的事务ID和正在同步的事务ID，如果已经在刷磁盘了，当前线程就不用刷写磁盘了
        while (mytxid > synctxid && isSyncRunning) {
          try {
            //释放锁，等待唤醒
            wait(1000);
          } catch (InterruptedException ie) {
          }
        }        
        // If this transaction was already flushed, then nothing to do 
        //如果该事务已经被刷新，则无需操作，直接返回      
        if (mytxid <= synctxid) {
          numTransactionsBatchedInSync++;
          if (metrics != null) {
            // Metrics is non-null only when used inside name node
            metrics.incrTransactionsBatchedInSync();
          }
          return;
        }

        // now, this thread will do the sync
        syncStart = txid;
        isSyncRunning = true;
        sync = true;

        // swap buffers 交换内存
        try {
          if (journalSet.isEmpty()) {
            throw new IOException("No journals available to flush");
          }
          //交换内存缓冲,有兴趣的同学可以点进去看一下setReadyToFlush方法
          /*
          *public void setReadyToFlush() {
              assert isFlushed() : "previous data not flushed yet";
              TxnBuffer tmp = bufReady;
              bufReady = bufCurrent;
              bufCurrent = tmp;
            }
          *
          */
          editLogStream.setReadyToFlush();
        } catch (IOException e) {
          final String msg =
              "Could not sync enough journals to persistent storage " +
              "due to " + e.getMessage() + ". " +
              "Unsynced transactions: " + (txid - synctxid);
          LOG.fatal(msg, new Exception());
          synchronized(journalSetLock) {
            IOUtils.cleanup(LOG, journalSet);
          }
          terminate(1, msg);
        }
      } finally {
        // Prevent RuntimeException from blocking other log edit write
        //恢复标志位 和 唤醒等待的线程
        doneWithAutoSyncScheduling();
      }
      //editLogStream may become null,
      //so store a local variable for flush.
      logStream = editLogStream;
    }

    // do the sync
    long start = monotonicNow();
    try {
      if (logStream != null) {
       //真正把数据写到磁盘
       //默默的在刷写磁盘就可以。      
       /**
        * 內存一： 服務于namenode的内存
        * 內存二： 服務于journalnode的内存
        */

        logStream.flush();
      }
    } catch (IOException ex) {
      synchronized (this) {
        //打印日志
        final String msg =
            "Could not sync enough journals to persistent storage. "
            + "Unsynced transactions: " + (txid - synctxid);
        //如果我们的程序里面发生了fatal 级别日志，这个错误
        //就是灾难型的错误。
        LOG.fatal(msg, new Exception());
        synchronized(journalSetLock) {
          IOUtils.cleanup(LOG, journalSet);
        }
        //执行这段代码
        terminate(1, msg);
      }
    }
    long elapsed = monotonicNow() - start;
    // Metrics non-null only when used inside name node
    if (metrics != null) { 
      metrics.addSync(elapsed);
    }

  } finally {
    // Prevent RuntimeException from blocking other log edit sync 
    synchronized (this) {
      if (sync) {
        synctxid = syncStart;
        //持久化数据到磁盘之后恢复标志位
        isSyncRunning = false;
      }
      //同时唤醒线程，通知该事务已经完成，可以进行下一次刷写
      this.notifyAll();
   }
  }
}
```
本文主要剖析了HDFS元数据管理流程，以上传文件为例，以点破面，引申出HDFS底层架构双缓存+分段加锁两个体现HDFS性能效率的优秀设计思想。这篇文章主要针对双缓存方案进行展开，涉及到了线程安全、高并发、NIO、流对拷相关的知识，这也是我们在学习分布式技术中必不可少的技能包。

HDFS的双缓存和分段加锁同样适用于我们工作中的代码设计，降低锁粒度是提高效率的有效手段。
























