---
layout: post
categories: [Spark]
description: none
keywords: Spark
---
# Spark基础设施
Spark的任何核心功能都离不开这些基础设施的构建，因此阅读本章内容将有助于我们理解Spark最底层的原理与实现。

讲解的内容如下。
- Spark配置。
- RPC框架。
- 事件总线。
- 度量系统。

## Spark配置
Spark作为一款优秀的计算框架，也配备了各种各样的系统配置参数。SparkConf是Spark的配置类，这个类在Spark的历史版本中已经存在很久了，

Spark中的每一个组件都直接或者间接地使用着它所存储的属性，这些属性都存储在如下的数据结构中。
```
private val settings = new ConcurrentHashMap[String, String]()
```
由以上代码的泛型可以看出，Spark的所有配置，无论是key还是value都是String类型。

Spark的配置通过以下3种方式获取。
- 来源于系统参数（即使用System.getProperties获取的属性）中以spark.作为前缀的那部分属性；
- 使用SparkConf的API进行设置；
- 从其他SparkConf中克隆。

### 系统属性中的配置
在SparkConf中有一个Boolean类型的构造器属性loadDefaults，当loadDefaults为true时，将会从系统属性中加载Spark配置，代码如下：
```
if (loadDefaults) {
  loadFromSystemProperties(false)
}

private[spark] def loadFromSystemProperties(silent: Boolean): SparkConf = {
  // 加载以spark.开头的系统属性
  for ((key, value) <- Utils.getSystemProperties if key.startsWith("spark.")) {
    set(key, value, silent)
  }
  this
}

```
以上代码调用了Utils工具类的getSystemProperties方法，其作用为获取系统的键值对属性。loadFromSystemProperties方法在获取了系统属性后，

使用Scala守卫过滤出其中以“spark.”字符串为前缀的key和value并且调用set方法，最终设置到settings中。

SparkConf中set方法的实现
```
private[spark] def set(key: String, value: String, silent: Boolean): SparkConf = {
  if (key == null) {
    throw new NullPointerException("null key")
  }
  if (value == null) {
    throw new NullPointerException("null value for " + key)
  }
  if (!silent) {
    logDeprecationWarning(key)
  }
  settings.put(key, value)
  this
}
```

### 使用SparkConf配置的API
给SparkConf添加配置的一种常见方式是使用SparkConf中提供的API。其中有些API最终实际调用了set的重载方法
```
def set(key: String, value: String): SparkConf = {
  set(key, value, false)
}

```
SparkConf中的setMaster、setAppName、setJars、setExecutorEnv、setSparkHome、setAll等方法最终都是通过set方法完成Spark配置的

设置Spark的部署模式的配置方法setMaster
```
def setMaster(master: String): SparkConf = {
  set("spark.master", master)
}
```

### 克隆SparkConf配置
有些情况下，同一个SparkConf实例中的配置信息需要被Spark中的多个组件共用，例如，组件A中存在一个SparkConf实例a，组件B中也需要实例a中的配置信息，这时该如何处理？我们往往首先想到的方法是将SparkConf实例定义为全局变量或者通过参数传递给其他组件，但是这会引入并发问题。虽然settings是线程安全的ConcurrentHashMap类，而且ConcurrentHashMap也被证明是高并发下性能表现不错的数据结构。

但是只要存在并发，就一定会有性能的损失问题。我们可以新建一个SparkConf实例b，并将a中的配置信息全部拷贝到b中，这种方式显然不是最优雅的，复制代码会散落在程序的各个角落。现在是时候阅读下SparkConf的构造器了
```
class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging with Serializable {
//省略无关代码
def this() = this(true)

```
SparkConf继承了Cloneable特质并实现了clone方法，clone方法的实现跟我们所讨论的方式是一样的，并且通过Cloneable特质提高了代码的可复用性。

```
override def clone: SparkConf = {
  val cloned = new SparkConf(false)
  settings.entrySet().asScala.foreach { e =>
    cloned.set(e.getKey(), e.getValue(), true)
  }
  cloned
}
```
这样我们就可以在任何想要使用SparkConf的地方使用克隆方式来优雅地编程了。

## Spark内置RPC框架
在Spark中很多地方都涉及网络通信，比如Spark各个组件间的消息互通、用户文件与Jar包的上传、节点间的Shuffle过程、Block数据的复制与备份等。

在Spark 0.x.x与Spark 1.x.x版本中，组件间的消息通信主要借助于Akka，使用Akka可以轻松地构建强有力的高并发与分布式应用。但是Akka在Spark 2.0.0版本中被移除了，

Spark官网文档对此的描述为：“Akka的依赖被移除了，因此用户可以使用任何版本的Akka来编程了。

在Spark 1.x.x版本中，用户文件与Jar包的上传采用了由Jetty实现的HttpFileServer，但在Spark 2.0.0版本中它也被废弃了，现在使用的是基于Spark内置RPC框架的NettyStreamManager。

节点间的Shuffle过程和Block数据的复制与备份这两个部分在Spark 2.0.0版本中依然沿用了Netty，通过对接口和程序进行重新设计，将各个组件间的消息互通、用户文件与Jar包的上传等内容统一纳入Spark的RPC框架体系中。

### RPC框架的基本架构
- TransportContext内部包含传输上下文的配置信息TransportConf和对客户端请求消息进行处理的RpcHandler。
- TransportConf在创建TransportClientFactory和TransportServer时都是必需的，
- RpcHandler只用于创建TransportServer。
- TransportClientFactory是RPC客户端的工厂类。
- TransportServer是RPC服务端的实现。

通过调用TransportContext的createClientFactory方法创建传输客户端工厂TransportClientFactory的实例。在构造TransportClientFactory的实例时，还会传递客户端引导程序TransportClientBootstrap的列表。此外，TransportClientFactory内部还存在针对每个Socket地址的连接池ClientPool，这个连接池缓存的定义如下：
```
private final ConcurrentHashMap<SocketAddress, ClientPool> connectionPool;
```

ClientPool的类型定义如下：
```
private static class ClientPool {
  TransportClient[] clients;
  Object[] locks;

  ClientPool(int size) {
    clients = new TransportClient[size];
    locks = new Object[size];
    for (int i = 0; i < size; i++) {
      locks[i] = new Object();
    }
  }
}
```
由此可见，ClientPool实际是由TransportClient的数组构成的，而locks数组中的Object与clients数组中的TransportClient按照数组索引一一对应，通过对每个TransportClient分别采用不同的锁，降低并发情况下线程间对锁的争用，进而减少阻塞，提高并发度。


















