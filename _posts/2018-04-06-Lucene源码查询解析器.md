---
layout: post
categories: [Lucene]
description: none
keywords: Lucene
---
# Lucene源码查询解析器
结合当下正在学习与研究的Lucene Query，带着好奇心深入学习了JavaCC和Lucene QueryParse.jj语法规则的实现。

## Lucene的Token正规表达式定义
```

/* ***************** */
/* Token Definitions */
/* ***************** */

<*> TOKEN : {
  <#_NUM_CHAR:   ["0"-"9"] >
// every character that follows a backslash is considered as an escaped character
| <#_ESCAPED_CHAR: "\\" ~[] >
| <#_TERM_START_CHAR: ( ~[ " ", "\t", "\n", "\r", "+", "-", "!", "(", ")", ":", "^",
                           "[", "]", "\"", "{", "}", "~", "*", "?", "\\" ]
                       | <_ESCAPED_CHAR> ) >
| <#_TERM_CHAR: ( <_TERM_START_CHAR> | <_ESCAPED_CHAR> | "-" | "+" ) >
| <#_WHITESPACE: ( " " | "\t" | "\n" | "\r") >
}

<DEFAULT, RangeIn, RangeEx> SKIP : {
  < <_WHITESPACE>>
}

<DEFAULT> TOKEN : {
  <AND:       ("AND" | "&&") >
| <OR:        ("OR" | "||") >
| <NOT:       ("NOT" | "!") >
| <PLUS:      "+" >
| <MINUS:     "-" >
| <LPAREN:    "(" >
| <RPAREN:    ")" >
| <COLON:     ":" >
| <STAR:      "*" >
| <CARAT:     "^" > : Boost
| <QUOTED:     "\"" (~["\""] | "\\\"")* "\"">
| <TERM:      <_TERM_START_CHAR> (<_TERM_CHAR>)*  >
| <FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
| <PREFIXTERM:  ("*") | ( <_TERM_START_CHAR> (<_TERM_CHAR>)* "*" ) >
| <WILDTERM:  (<_TERM_START_CHAR> | [ "*", "?" ]) (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
| <RANGEIN_START: "[" > : RangeIn
| <RANGEEX_START: "{" > : RangeEx
}

<Boost> TOKEN : {
<NUMBER:    (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? > : DEFAULT
}

<RangeIn> TOKEN : {
<RANGEIN_TO: "TO">
| <RANGEIN_END: "]"> : DEFAULT
| <RANGEIN_QUOTED: "\"" (~["\""] | "\\\"")+ "\"">
| <RANGEIN_GOOP: (~[ " ", "]" ])+ >
}

<RangeEx> TOKEN : {
<RANGEEX_TO: "TO">
| <RANGEEX_END: "}"> : DEFAULT
| <RANGEEX_QUOTED: "\"" (~["\""] | "\\\"")+ "\"">
| <RANGEEX_GOOP: (~[ " ", "}" ])+ >
}
```
分别来解释主要token的定义
- Token: 数字
```
<#_NUM_CHAR:   ["0"-"9"] >
```

- Token: 特殊字符
```
_ESCAPED_CHAR::= "\" [ "\", "+", "-", "!", "(", ")", ":", "^", "[", "]", """, "{", "}", "~", "*", "?" ] >
```

- Token: term的起始字符
Term的起始字符，除了下面列出的其它字符都可以。
```
<#_TERM_START_CHAR: ( ~[ " ", "\t", "\n", "\r", "+", "-", "!", "(", ")", ":", "^", "[", "]", "\"", "{", "}", "~", "*", "?", "\\" ]
| <_ESCAPED_CHAR> ) >
```

- Token: term
```
 <#_TERM_CHAR: ( <_TERM_START_CHAR> | <_ESCAPED_CHAR> | "-" | "+" ) >
```

- Token:term分词
term由二部分TERM_START_CHAR + (TERM_CHAR)*来组成
```
 <TERM:      <_TERM_START_CHAR> (<_TERM_CHAR>)*  >
```

- Token: 空格和回车
```
<#_WHITESPACE: ( " " | "\t" | "\n" | "\r") >
```

- Token: 词权重：
字符串必须以字符~开始，而后是数字。
```
<FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
```

- Token: 引号
表示用"包起来的字符串，字符"开始，中间由不是"的符号或者连着的这两个符号"组成，字符"结束，
```
<QUOTED:     "\"" (~["\""] | "\\\"")* "\"">
```

- Token: 模糊查找
模糊字符串的正规表达式必须以字符~开始。
```
<FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
```

- Token: 前缀查询
前缀字符串的正规表达式必须以*结尾。
```
 <PREFIXTERM:  ("*") | ( <_TERM_START_CHAR> (<_TERM_CHAR>)* "*" ) >
```

- Token: wildterm
通配字符串的正规表达式必须包含*或者？字符。
```
<WILDTERM:  (<_TERM_START_CHAR> | [ "*", "?" ]) (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
```

- Token：rangeQuery
rangeQuery有二种形式：

包含边界的rnage查询：[beging to end]
不包含边界的range查询：{begin to end}

```
| <RANGEIN_START: "[" > : RangeIn
| <RANGEEX_START: "{" > : RangeEx

<RangeIn> TOKEN : {
<RANGEIN_TO: "TO">
| <RANGEIN_END: "]"> : DEFAULT
| <RANGEIN_QUOTED: "\"" (~["\""] | "\\\"")+ "\"">
| <RANGEIN_GOOP: (~[ " ", "]" ])+ >
}

<RangeEx> TOKEN : {
<RANGEEX_TO: "TO">
| <RANGEEX_END: "}"> : DEFAULT
| <RANGEEX_QUOTED: "\"" (~["\""] | "\\\"")+ "\"">
| <RANGEEX_GOOP: (~[ " ", "}" ])+ >
}
```

## Lucene语法产生式： 分析与生成Query

- 连接语义
它的输入是一串token，匹配连接产生式后，它的输入ret会被初始化成CONJ_AND或者CONJ_OR。
```
Conjunction::=[ <AND> { ret = CONJ_AND; } | <OR> { ret = CONJ_OR; } ] 

int Conjunction() : {
  int ret = CONJ_NONE;
}
{
  [
    <AND> { ret = CONJ_AND; }
    | <OR>  { ret = CONJ_OR; }
  ]
  { return ret; }
}
```

- “+ - !”语义
它的输入是一串token，匹配连接产生式后，它的输入ret会被初始化成MOD_REQ或者MOD_NOT, 分别代表必选与可选项。
(a b) 会转换成 a OR b
+a +b =>会转换成a AND b
```
Modifiers::=[ <PLUS> { ret = MOD_REQ; } | <MINUS> { ret = MOD_NOT; } | <NOT> { ret = MOD_NOT; } ] 


int Modifiers() : {
  int ret = MOD_NONE;
}
{
  [
     <PLUS> { ret = MOD_REQ; }
     | <MINUS> { ret = MOD_NOT; }
     | <NOT> { ret = MOD_NOT; }
  ]
  { return ret; }
}
```

## Query语义
整个Lucene的搜索入口点在TopLevelQuery函数，它最后返回Query类型对象。
```
public static void main(String[] args) throws Exception {
    QueryParser qp = new QueryParser("field",
                           new org.apache.lucene.analysis.SimpleAnalyzer());
    Query q = qp.parse(args[0]);
    System.out.println(q.toString("field"));
  }

public Query parse(String query) throws ParseException {
  ReInit(new FastCharStream(new StringReader(query)));
  try {
 // TopLevelQuery is a Query followed by the end-of-input (EOF)
    Query res = TopLevelQuery(field);
    return res!=null ? res : new BooleanQuery();
  }
}
```

一个Query查询语句，是由多个clause组成，每个cluas有对应的modifier (+/-)连接符。
```
Query::=Modifiers Clause (Conjunction Modifiers Clause)*

// This makes sure that there is no garbage after the query string
Query TopLevelQuery(String field) : 
{
  Query q;
}
{
  q=Query(field) <EOF>
  {
    return q;
  }
}

Query Query(String field) :
{
  Vector clauses = new Vector();
  Query q, firstQuery=null;
  int conj, mods;
}
{
  mods=Modifiers() q=Clause(field)
  {
    addClause(clauses, CONJ_NONE, mods, q);
    if (mods == MOD_NONE)
        firstQuery=q;
  }
  (
    conj=Conjunction() mods=Modifiers() q=Clause(field)
    { addClause(clauses, conj, mods, q); }
  )*
    {
      if (clauses.size() == 1 && firstQuery != null)
        return firstQuery;
      else {
  return getBooleanQuery(clauses);
      }
    }
}
```
Query产生式分析如下：

首先匹配modifier连接符，它可以为空。modifier连接符后面是另一个clause的产生式。可以生成子查询语句。比如，分析“field:abc"

如果后续出现了and/or连接符，需要考虑"field:abc" and "field2:xyz"的查询输入。


### Clause语义
clause产生式使用的LL(2)提前查询了二个token,避免了函数的回朔。多补充一句，JavaCC是自顶而下的非递归算法

驱动表 (first + follow集)
stack

```
Clause::=[(<TERM> <COLON>|<STAR> <COLON>)]


Query Clause(String field) : {
  Query q;
  Token fieldToken=null, boost=null;
}
{
  [
    LOOKAHEAD(2)
    (
    fieldToken=<TERM> <COLON> {field=discardEscapeChar(fieldToken.image);}
    | <STAR> <COLON> {field="*";}
    )
  ]

  (
   q=Term(field)
   | <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?

  )
    {
      if (boost != null) {
        float f = (float)1.0;
  try {
    f = Float.valueOf(boost.image).floatValue();
          q.setBoost(f);
  } catch (Exception ignored) { }
      }
      return q;
    }
}
```
Clause产生式分析如下：

- 当遇到“field : abc"时，确定是field而不是term查询，<term><colon>是输入的特征。通过执行动作：field=discardEscapeChar(fieldToken.image))来获得field的值。
- 当遇到"abc and (xyz or efg)"时，确定是grouping分组情况，支持使用小括号对每个子句进行分组，从而支持更加复杂的嵌套查询逻辑。

分组递归定义:
```
| <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?
```

Term语义
```
(Term|<LPAREN> Query <RPAREN> (<CARAT> <NUMBER>)?) 

Term::=(
  (<TERM>|<STAR>|<PREFIXTERM>|<WILDTERM>|<NUMBER>) [<FUZZY_SLOP>] [<CARAT><NUMBER>[<FUZZY_SLOP>]]

| ( <RANGEIN_START> (<RANGEIN_GOOP>|<RANGEIN_QUOTED>) [ <RANGEIN_TO> ] (<RANGEIN_GOOP>|<RANGEIN_QUOTED> <RANGEIN_END> ) [ <CARAT> boost=<NUMBER> ] 

| ( <RANGEEX_START> <RANGEEX_GOOP>|<RANGEEX_QUOTED> [ <RANGEEX_TO> ] <RANGEEX_GOOP>|<RANGEEX_QUOTED> <RANGEEX_END> )[ <CARAT> boost=<NUMBER> ] 

| <QUOTED> [ <FUZZY_SLOP> ] [ <CARAT> boost=<NUMBER> ] 
)
```

Query产生式分析如下

第一阶段
普通normal-term
通配wildcard-term
对应的解析动作：wilcard变量进行了初始化。
前缀prefix-term
对应的解析动作：prefix变量进行了初始化。
模糊词fuzzy-term
对应的解析动作：fuzzy变量进行了初始化。
它必须以~开头，后面接数字。
数字term

类型的token的term变量正确初始化,并且根据token来初始化wildcard/prefix/fuzzy这样的bool值。
```
Query Term(String field) : {
  Token term, boost=null, fuzzySlop=null, goop1, goop2;
  boolean prefix = false;
  boolean wildcard = false;
  boolean fuzzy = false;
  boolean rangein = false;
  Query q;
}
{
  (
     (
       term=<TERM>
       | term=<STAR> { wildcard=true; }
       | term=<PREFIXTERM> { prefix=true; }
       | term=<WILDTERM> { wildcard=true; }
       | term=<NUMBER>
     )
     [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ]
     [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
     {
       String termImage=discardEscapeChar(term.image);
       if (wildcard) {
       q = getWildcardQuery(field, termImage);
       } else if (prefix) {
         q = getPrefixQuery(field,
           discardEscapeChar(term.image.substring
          (0, term.image.length()-1)));
       } else if (fuzzy) {
             float fms = fuzzyMinSim;
             try {
            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
             } catch (Exception ignored) { }
            if(fms < 0.0f || fms > 1.0f){
              throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");
            }
            q = getFuzzyQuery(field, termImage,fms);
       } else {
         q = getFieldQuery(field, termImage);
       }
     }
```

第二阶段
根据第一阶段识别出term的类型，调用不同API生成 不同子类的term对象

或者只包含普通字符的term ( abc)
调用getFieldQuery
或者包含wildcard通配符的term (abc*xyz 或者abc ?xyz)
调用getWildcardQuery
或者包含prefix的term (abc*). [其中必须出现在未尾]
调用getPrefixQuery
或者包含fuzzy模糊匹配的term (abc~3)
调用getFuzzyQuery
或者包含数字的term (333)
调用getFieldQuery

```
 {
       String termImage=discardEscapeChar(term.image);
       if (wildcard) {
       q = getWildcardQuery(field, termImage);
       } else if (prefix) {
         q = getPrefixQuery(field,
           discardEscapeChar(term.image.substring
          (0, term.image.length()-1)));
       } else if (fuzzy) {
          float fms = fuzzyMinSim;
          try {
            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
          } catch (Exception ignored) { }
         if(fms < 0.0f || fms > 1.0f){
           throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");
         }
         q = getFuzzyQuery(field, termImage,fms);
       } else {
         q = getFieldQuery(field, termImage);
       }
     }
```

第三阶段
支持有界与无界范围查询。

[date1 to date2]
[name1 to name2]
比如，range匹配: title: {leon to sammi}

<pangein_start>匹配 {
通过goop = <pangein_goop>取"león"
<pangein_to>匹配 to
通过goop = <pangein_goop>取"sammi"
<pangein_end>匹配 }
其对应的执行动作是调用getRangeQuery函数，创建RangeQuery对象。
```
| ( <RANGEIN_START> ( goop1=<RANGEIN_GOOP>|goop1=<RANGEIN_QUOTED> )
         [ <RANGEIN_TO> ] ( goop2=<RANGEIN_GOOP>|goop2=<RANGEIN_QUOTED> )
         <RANGEIN_END> )
       [ <CARAT> boost=<NUMBER> ]
        {
          if (goop1.kind == RANGEIN_QUOTED) {
            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
          }
          if (goop2.kind == RANGEIN_QUOTED) {
            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
          }
          q = getRangeQuery(field, discardEscapeChar(goop1.image), discardEscapeChar(goop2.image), true);
        }
     | ( <RANGEEX_START> ( goop1=<RANGEEX_GOOP>|goop1=<RANGEEX_QUOTED> )
         [ <RANGEEX_TO> ] ( goop2=<RANGEEX_GOOP>|goop2=<RANGEEX_QUOTED> )
         <RANGEEX_END> )
       [ <CARAT> boost=<NUMBER> ]
        {
          if (goop1.kind == RANGEEX_QUOTED) {
            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
          }
          if (goop2.kind == RANGEEX_QUOTED) {
            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
          }

          q = getRangeQuery(field, discardEscapeChar(goop1.image), discardEscapeChar(goop2.image), false);
        }
     | 
```

第四阶段
Lucene支持给不同的查询词设置不同的权重，设置权重使用"^"符号，将"^"符号放置于查询词term的尾部，同时紧跟权重值。权重值越大，这个词term就越重要。在搜索过程中具备更高的相关性。

term查询的加权匹配: field: xyz^4

<cart>匹配 ^
通过boost =<number> 来初始化boost=4
```
{
    if (boost != null) {
      float f = (float) 1.0;
      try {
        f = Float.valueOf(boost.image).floatValue();
      }
      catch (Exception ignored) {
    /* Should this be handled somehow? (defaults to "no boost", if
     * boost number is invalid)
     */
      }

      // avoid boosting null queries, such as those caused by stop words
      if (q != null) {
        q.setBoost(f);
      }
    }
    return q;
  }
```

getFieldQuery公共函数
```
  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
 
    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
    Vector v = new Vector();
    org.apache.lucene.analysis.Token t;
    int positionCount = 0;
    boolean severalTokensAtSamePosition = false;
​
    while (true) {
      try {
        t = source.next();
      }
      catch (IOException e) {
        t = null;
      }
      if (t == null)
        break;
      v.addElement(t);
      if (t.getPositionIncrement() != 0)
        positionCount += t.getPositionIncrement();
      else
        severalTokensAtSamePosition = true;
    }
    try {
      source.close();
    } 
​
    if (v.size() == 0)
      return null;
    else if (v.size() == 1) {
      t = (org.apache.lucene.analysis.Token) v.elementAt(0);
      return new TermQuery(new Term(field, t.termText()));
    } else {
      if (severalTokensAtSamePosition) {
        if (positionCount == 1) {
          // no phrase query:
          BooleanQuery q = new BooleanQuery(true);
          for (int i = 0; i < v.size(); i++) {
            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
            TermQuery currentQuery = new TermQuery(
                new Term(field, t.termText()));
            q.add(currentQuery, BooleanClause.Occur.SHOULD);
          }
          return q;
        }
        else {
          // phrase query:
          MultiPhraseQuery mpq = new MultiPhraseQuery();
          mpq.setSlop(phraseSlop);          
          List multiTerms = new ArrayList();
          int position = -1;
          for (int i = 0; i < v.size(); i++) {
            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
            if (t.getPositionIncrement() > 0 && multiTerms.size() > 0) {
              if (enablePositionIncrements) {
                mpq.add((Term[])multiTerms.toArray(new Term[0]),position);
              } else {
                mpq.add((Term[])multiTerms.toArray(new Term[0]));
              }
              multiTerms.clear();
            }
            position += t.getPositionIncrement();
            multiTerms.add(new Term(field, t.termText()));
          }
          if (enablePositionIncrements) {
            mpq.add((Term[])multiTerms.toArray(new Term[0]),position);
          } else {
            mpq.add((Term[])multiTerms.toArray(new Term[0]));
          }
          return mpq;
        }
      }
      else {
        PhraseQuery pq = new PhraseQuery();
        pq.setSlop(phraseSlop);
        int position = -1;
        for (int i = 0; i < v.size(); i++) {
          t = (org.apache.lucene.analysis.Token) v.elementAt(i);
          if (enablePositionIncrements) {
            position += t.getPositionIncrement();
            pq.add(new Term(field, t.termText()),position);
          } else {
            pq.add(new Term(field, t.termText()));
          }
        }
        return pq;
      }
    }
  }
```

TermQuery
```
 if (v.size() == 0)
      return null;
    else if (v.size() == 1) {
      t = (org.apache.lucene.analysis.Token) v.elementAt(0);
      return new TermQuery(new Term(field, t.termText()));
```
比如，输入"field: Beijing"，搜索内容"Beijing"只会被解析成一个分词，对应一个term：term1("Beijing")。如果分词后只有一个Token，则生成new TermQuery(new Term(field, t.termText()))对象。

BoolQuery
```
BooleanQuery q = new BooleanQuery(true);
          for (int i = 0; i < v.size(); i++) {
            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
            TermQuery currentQuery = new TermQuery(
                new Term(field, t.termText()));
            q.add(currentQuery, BooleanClause.Occur.SHOULD);
          }
          return q;
```
比如，输入field:"Beijing China"时，搜索内容"Beijing China"会被解析成二个分词，对应二个term：term1("Beijing")和term("China")。最后会实例化PhraseQuery pq = new PhraseQuery()对象。它背后语义是：所有的分词在搜索中必须同时匹配与满足。

Demo: "+(foo bar) +(baz boo)" 表达成 (foo OR bar) AND (baz OR boo)
```
assertQueryEquals("(foo OR bar) AND (baz OR boo)", null,
            "+(foo bar) +(baz boo)");
```


