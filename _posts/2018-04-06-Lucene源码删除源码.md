---
layout: post
categories: [Lucene]
description: none
keywords: Lucene
---
# Lucene源码删除源码
我们首先可以把Lucene中的删除操作进行分类，下面会对这里的分类进行详细的介绍，通过阅读源码可以知道，Lucene中从方式上来说删除可分为Term删除和ByQuery删除，其实Term删除和Query删除也可以认为是删除具体动作发生时机的分类结果。从删除发生“地点”来说由可分为全局删除和局部删除。

下面我们介绍下分类的依据：

首先我们可以看下IndexWriter中的两个deleteDocuments函数的声明，分别为public long deleteDocuments(Query... queries)和public long deleteDocuments(Term... terms)，从这两个重载的deleteDocuments可以知道Term删除和Query删除的划分依据。

除了deleteDocuments(Term... terms)可以实现ByTerm删除，熟悉Lucene的同学应该知道，Lucene中的Update也是通过先删除后新增实现的，这里对原文档的删除也是使用Term的方式实现的。了解Lucene中“线程池”以及DocumentsWriterPerThread（后文使用DWPT代替）原理的可以知道deleteDocuments属于全局删除，而IndexWriter.updateDocument系列重载函数因为需要分配DWPT属于局部删除。

看Lucene源码也可以知道,addDocument也是通过updateDocument实现的。

在上文也提到了也可以通过区分删除动作具体发生时机将删除分为Term和Query，这里首先你得了解Lucene中“线程池”以及DWPT等的原理（这里后续有机会会有文章进行介绍），在flush或commt（因为comit也会触发flush，所以下文只会提到flush）时，IndexWriter会调用DocumentsWriter.flushAllThreads()，触发每个DWPT的刷新动作，每个DWPT会单独刷新成自己的segment，即会将自己维护的内存中数据刷新到自己的si、.cfs、.cfe(假设UseCompoundFile = true）文件中，Term删除需要删除Term查询得到的所有文档ID删除，所以在每个DWPT flush时即可遍历所有Term删除指定的文档，即该动作发生在触发全局flush时每个DWPT的flush中。

但是Query删除却需要等待所有的文档刷新之后，即所有DWPT刷新完成之后才能进行查询，然后才能进行Query删除动作，所以Query删除的发生时机发生在所有DWPT刷新之后。

上面还说到删除从删除发生“地点”来说由可分为全局删除和局部删除，这里涉及到对DocumentsWriterDeleteQueue的理解，DocumentsWriter自己会维护一个全局的DocumentsWriterDeleteQueue（下文使用globalQueue代替）以及一个DeleteSlice全局变量globalSlice，每个DWPT自己也会使用DeleteSlice（下文使用deleteSlice指代DWPT维护的DeleteSlice）维护globalQueue上的一个分片，IndexWriter.deleteDocuments就直接作将删除操作放到globalQueue中，放入之后会更新globalSlice，而IndexWriter.updateDocument因为发生在DWPT中，其删除操作在放入globalQueue后更新自己的deleteSlice。

这里虽然区分了globalQueue（或者globalSlice）和deleteSlice，但是Lucene在DocumentsWriter只会维护唯一的一个globalQueue，DocumentsWriterDeleteQueue采用单链表实现，globalSlice维护全局的删除操作，只是采用两个指针在globalQueue上截取一个分片，记录自己还没有处理的删除操作。同理，每个deleteSlice也是在globalQueue上采用两个“指针”记录了一段需要自己关注的删除操作，在apply delete queue中的删除操作时，DWPT会将自己关注的那些删除操作记录到自己的BufferedUpdates中。这里可以大概注意下，文章一开始提到的参考文章介绍了删除操作是一个全局的操作，全局删除和局部删除会进行双向的同步下文会具体介绍下其实现逻辑。

## 删除相关基础类介绍

### BufferedUpdates
从BufferedUpdates源码注释中可以知道其功能：
Holds buffered deletes and updates, by docID, term or query for a single segment. This is used to hold buffered pending deletes and updates against the to-be-flushed segment. Once the deletes and updates are pushed (on flush in DocumentsWriter), they are converted to a {@link FrozenBufferedUpdates} instance and pushed to the {@link BufferedUpdatesStream}.

由DWPT持有的记录删除或更新操作的工具类，其实除了每个DWPT会持有BufferedUpdates对象实例：pendingUpdates，DocumentsWriterDeleteQueue也会持有一个该对象的全局实例：globalBufferedUpdates，上文说到了全局删除和局部删除的双向同步，其实现关键就在DWPT持有的pendingUpdates和全局globalBufferedUpdates中。等介绍完其他相关基础类会介绍双向同步原理。

BufferedUpdates中维护三种类型的删除，可从其成员得出：
```
//维护Term删除操作
final Map<Term,Integer> deleteTerms = new HashMap<>();
//维护query删除操作
final Map<Query,Integer> deleteQueries = new HashMap<>();
//存放需要删除的docID，这里主要是记录新增或更新文档发生异常时已经缓存了
//的docID，在写入时会过滤这些doc，避免写入发生异常的doc
final List<Integer> deleteDocIDs = new ArrayList<>();
```

### DocumentsWriterDeleteQueue
DocumentsWriterDeleteQueue负责保存全局删除操作，由DocumentsWriter持有唯一的一个实例。其内部采用单链表数据结构，DocumentsWriterDeleteQueue则持有该链表的tail节点，即尾节点。相信读者会有个疑问，为什么只持有链表的尾节点呢？因为尾节点之前的删除操作都已经加入到BufferedUpdates中了，也就是已经被处理过了，所以不再有用。

上面说的处理并不是执行了实际的删除动作，还是已经将节点代表的删除操作放置到全局globalBufferedUpdates或DWPT持有的pendingUpdates中。

首先看下DocumentsWriterDeleteQueue中节点类型，其基类为DocumentsWriterDeleteQueue中的静态内部类Node

为了简单，我们只看TermNode:
```
//DocumentsWriterDeleteQueue.TermNode

private static final class TermNode extends Node<Term> {
    TermNode(Term term) {
      super(term);
    }
    //这里要重点关注下apply函数，从下面函数实现可以知道其主要工作就是将
    //删除操作放到BufferedUpdates中
    @Override
    void apply(BufferedUpdates bufferedDeletes, int docIDUpto) {
      bufferedDeletes.addTerm(item, docIDUpto);
    }

    @Override
    public String toString() {
      return "del=" + item;
    }

}
```
Lucene会在合适的时机触发Node.apply操作，比如在进行全局删除时调用DocumentsWriterDeleteQueue.tryApplyGlobalSlice进行全局操作，也会在update时，调用DeleteSlice.apply进行局部操作。

### DeleteSlice
DeleteSlice同样是DocumentsWriterDeleteQueue中的静态内部类，其主要包含两个域（field、成员变量_）:sliceHead、sliceTail，用来从globalQueue上截取自上次flush之后自己需要记录的删除操作。

DeleteSlice实现如下：
```
static class DeleteSlice {
    // No need to be volatile, slices are thread captive (only accessed by one thread)!
    Node<?> sliceHead; // we don't apply this one
    Node<?> sliceTail;

    DeleteSlice(Node<?> currentTail) {
      assert currentTail != null;
      /*
       * Initially this is a 0 length slice pointing to the 'current' tail of
       * the queue. Once we update the slice we only need to assign the tail and
       * have a new slice
       */
      sliceHead = sliceTail = currentTail;
    }
    //2.2提到的调用Node.apply的一个地方
    void apply(BufferedUpdates del, int docIDUpto) {
      if (sliceHead == sliceTail) {
        // 0 length slice
        return;
      }
      /*
       * When we apply a slice we take the head and get its next as our first
       * item to apply and continue until we applied the tail. If the head and
       * tail in this slice are not equal then there will be at least one more
       * non-null node in the slice!
       */
      Node<?> current = sliceHead;
      do {
        current = current.next;
        assert current != null : "slice property violated between the head on the tail must not be a null node";
        current.apply(del, docIDUpto);
      } while (current != sliceTail);
      reset();
    }

    void reset() {
      // Reset to a 0 length slice
      sliceHead = sliceTail;
    }

    /**
     * Returns <code>true</code> iff the given node is identical to the the slices tail,
     * otherwise <code>false</code>.
     */
    boolean isTail(Node<?> node) {
      return sliceTail == node;
    }

    /**
     * Returns <code>true</code> iff the given item is identical to the item
     * hold by the slices tail, otherwise <code>false</code>.
     */
    boolean isTailItem(Object object) {
      return sliceTail.item == object;
    }

    boolean isEmpty() {
      return sliceHead == sliceTail;
    }
}
```

### DocumentsWriterFlushQueue
DocumentsWriterFlushQueue（后文使用ticketQueue代替）主要用于在flush时记录每个DWPT的刷新操作，这里记录的刷新操作主要是刷新之后形成的FlushedSegment以及全局删除操作，FlushedSegment是此次刷新之后的最终段内存视图。其中各种更新操作已经进行过具体处理，Term删除也进行了处理，但是Query删除还依然保持未处理，所以FlushedSegment中还保存了Query删除操作，但是没有
Term删除操作。

ticketQueue保存的这些操作会在所有DWPT的flush操作完成之后，会被purge到全局eventQueue中，然后再调用processEvents进行操作处理，使得内存中保存的SegmentInfo保持最新。这里也是NRT(near real-time)的实现关键。

### FrozenBufferedUpdates
FrozenBufferedUpdates是每个DWPT在进行flush时，对自己持有pendingUpdates做的一个快照。

### IndexWriter.eventQueue
IndexWriter.eventQueue在IndexWriter中定义如下：
```
private final Queue<Event> eventQueue = new ConcurrentLinkedQueue<>();
```
就是采用JDK ConcurrentLinkedQueue定义的一个事件queue，刷盘过程中的的一些必要操作会被封装为Event并放入eventQueue中国，这些事件会在刷盘完成之后进行处理。

## 相关删除入口函数的实现
本节主要介绍相关删除的入口函数，这里可能会涉及到局部删除、全局删除双向同步相关函数的调用，遇到的话会稍微提及一下，具体的双向同步逻辑会在下一节介绍。

### 全局删除IndexWriter.deleteDocuments
IndexWriter.deleteDocuments系列重载函数主要做全局删除动作。

### deleteDocuments(Term... terms)
直接看起实现函数：
```
//IndexWriter
 public long deleteDocuments(Term... terms) throws IOException {
    ensureOpen();
    try {
    //调用DocumentsWriter.deleteTerms进行删除
      long seqNo = docWriter.deleteTerms(terms);
      if (seqNo < 0) {
        seqNo = -seqNo;
        processEvents(true);
      }
      return seqNo;
    } catch (VirtualMachineError tragedy) {
      tragicEvent(tragedy, "deleteDocuments(Term..)");
      throw tragedy;
    }
}

//DocumentsWriter
long deleteTerms(final Term... terms) throws IOException {
    return applyDeleteOrUpdate(q -> q.addDelete(terms));
}

//DocumentsWriter
private synchronized long applyDeleteOrUpdate(ToLongFunction<DocumentsWriterDeleteQueue> function) throws IOException {
    // TODO why is this synchronized?
    final DocumentsWriterDeleteQueue deleteQueue = this.deleteQueue;
    //这里将此Term删除放入globalQueue中
    long seqNo = function.applyAsLong(deleteQueue);
    flushControl.doOnDelete();
    lastSeqNo = Math.max(lastSeqNo, seqNo);
    if (applyAllDeletes(deleteQueue)) {
      seqNo = -seqNo;
    }
    return seqNo;
}

//DocumentsWriterDeleteQueue
long addDelete(Term... terms) {
    long seqNo = add(new TermArrayNode(terms));
    //将globalSlice中截取的删除分片放入globalBufferedUpdates中
    tryApplyGlobalSlice();
    return seqNo;
}
```

### deleteDocuments(Query... queries)
deleteDocuments(Query... queries)其代码路径和deleteDocuments(Term... terms)一样，最后也是将删除操作放入globalQueue中。

但是和deleteDocuments(Term... terms)不同的是，其放入globalQueue中的Node子类是QueryArrayNode，而deleteDocuments(Term... terms)放入的Node子类是TermArrayNode。

还有个区别是，如果是MatchAllDocsQuery删除，则会删除所有的文档：
```
//IndexWriter
public long deleteDocuments(Query... queries) throws IOException {
    ensureOpen();

    // LUCENE-6379: Specialize MatchAllDocsQuery
    for(Query query : queries) {
      if (query.getClass() == MatchAllDocsQuery.class) {
        //删除所有文档
        return deleteAll();
      }
    }

    try {
      long seqNo = docWriter.deleteQueries(queries);
      if (seqNo < 0) {
        seqNo = -seqNo;
        processEvents(true);
      }

      return seqNo;
    } catch (VirtualMachineError tragedy) {
      tragicEvent(tragedy, "deleteDocuments(Query..)");
      throw tragedy;
    }
}
```
这里不再展开deleteAll的实现，可自行去看源码。

### 局部删除IndexWriter.updateDocument
IndexWriter.addDocument也是通过调用IndexWriter.updateDocument实现的新增文档，在调用时，如下：
```
//IndexWriter
public long addDocument(Iterable<? extends IndexableField> doc) throws IOException {
    //第一个参数传null，表示新增
    return updateDocument((DocumentsWriterDeleteQueue.Node<?>) null, doc);
}
```
同样直接看源码
```
//IndexWriter
 public long updateDocument(Term term, Iterable<? extends IndexableField> doc) throws IOException {
    //判断是否要删除原先的文档
    return updateDocument(term == null ? null : DocumentsWriterDeleteQueue.newNode(term), doc);
}

//DocumentsWriterDeleteQueue
static Node<Term> newNode(Term term) {
    return new TermNode(term);
}

//IndexWriter
private long updateDocument(final DocumentsWriterDeleteQueue.Node<?> delNode,
                              Iterable<? extends IndexableField> doc) throws IOException {
    ensureOpen();
    boolean success = false;
    try {
        //调用DocumentsWriter.updateDocument
      long seqNo = docWriter.updateDocument(doc, analyzer, delNode);
      if (seqNo < 0) {
        seqNo = -seqNo;
        processEvents(true);
      }
      success = true;
      return seqNo;
    } catch (VirtualMachineError tragedy) {
      tragicEvent(tragedy, "updateDocument");
      throw tragedy;
    } finally {
      if (success == false) {
        if (infoStream.isEnabled("IW")) {
          infoStream.message("IW", "hit exception updating document");
        }
      }
      maybeCloseOnTragicEvent();
    }
}
```
下面看发生在DocumentsWriter.updateDocument中的具体操作：
```
//DocumentsWriter
long updateDocument(final Iterable<? extends IndexableField> doc, final Analyzer analyzer,
                      final DocumentsWriterDeleteQueue.Node<?> delNode) throws IOException {

    boolean hasEvents = preUpdate();
    //获取一个DWPT
    final ThreadState perThread = flushControl.obtainAndLock();

    final DocumentsWriterPerThread flushingDWPT;
    long seqNo;
    try {
      // This must happen after we've pulled the ThreadState because IW.close
      // waits for all ThreadStates to be released:
      ensureOpen();
      //初始化DWPT
      ensureInitialized(perThread);
      assert perThread.isInitialized();
      final DocumentsWriterPerThread dwpt = perThread.dwpt;
      final int dwptNumDocs = dwpt.getNumDocsInRAM();
      try {
        //这里是具体文档更新逻辑所在，调用DWPT.updateDocument实现
        seqNo = dwpt.updateDocument(doc, analyzer, delNode, flushNotifications);
      } finally {
        if (dwpt.isAborted()) {
          flushControl.doOnAbort(perThread);
        }
        // We don't know whether the document actually
        // counted as being indexed, so we must subtract here to
        // accumulate our separate counter:
        numDocsInRAM.addAndGet(dwpt.getNumDocsInRAM() - dwptNumDocs);
      }
      final boolean isUpdate = delNode != null && delNode.isDelete();
      flushingDWPT = flushControl.doAfterDocument(perThread, isUpdate);

      assert seqNo > perThread.lastSeqNo: "seqNo=" + seqNo + " lastSeqNo=" + perThread.lastSeqNo;
      perThread.lastSeqNo = seqNo;

    } finally {
      perThreadPool.release(perThread);
    }

    if (postUpdate(flushingDWPT, hasEvents)) {
      seqNo = -seqNo;
    }
    
    return seqNo;
}
```
ensureInitialized负责对此次分配到的DWPT进行初始化，如果DWPT为null，则new一个新的实例。
```
//
private void ensureInitialized(ThreadState state) throws IOException {
    if (state.dwpt == null) {
      final FieldInfos.Builder infos = new FieldInfos.Builder(globalFieldNumberMap);
      //DWPT的构造函数会传入globalQueue对象
      state.dwpt = new DocumentsWriterPerThread(indexCreatedVersionMajor, segmentNameSupplier.get(), directoryOrig,
                                                directory, config, infoStream, deleteQueue, infos,
                                                pendingNumDocs, enableTestPoints);
    }
  }
```
DWPT构造函数会构造此DWPT维护的pendingUpdates和deleteSlice：
```
 public DocumentsWriterPerThread(int indexVersionCreated, String segmentName, Directory directoryOrig, Directory directory, LiveIndexWriterConfig indexWriterConfig, InfoStream infoStream, DocumentsWriterDeleteQueue deleteQueue,
                                  FieldInfos.Builder fieldInfos, AtomicLong pendingNumDocs, boolean enableTestPoints) throws IOException {
    ...
    //DWPT维护的pendingUpdates初始化
    pendingUpdates = new BufferedUpdates(segmentName);
    intBlockAllocator = new IntBlockAllocator(bytesUsed);
    this.deleteQueue = deleteQueue;
    assert numDocsInRAM == 0 : "num docs " + numDocsInRAM;
    //新建deleteSlice
    deleteSlice = deleteQueue.newSlice();
   //此DWPT维护的Segment分段信息
    segmentInfo = new SegmentInfo(directoryOrig, Version.LATEST, Version.LATEST, segmentName, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), indexWriterConfig.getIndexSort());
    ...
    // this should be the last call in the ctor 
    // it really sucks that we need to pull this within the ctor and pass this ref to the chain!
    //consumer用来进行实际的索引相关操作
    consumer = indexWriterConfig.getIndexingChain().getChain(this);
    ...
}

//DocumentsWriterDeleteQueue
DeleteSlice newSlice() {
    //返回一个新的DeleteSlice对象实例
    return new DeleteSlice(tail);
}
```
DeleteSlice类的代码上面已经列出过，这里看下其构造函数：
```
DeleteSlice(Node<?> currentTail) {
      assert currentTail != null;
      /*
       * Initially this is a 0 length slice pointing to the 'current' tail of
       * the queue. Once we update the slice we only need to assign the tail and
       * have a new slice
       */
       //此DeleteSlice的首尾指针均指向globalQueue的当前尾节点
      sliceHead = sliceTail = currentTail;
}
```
好了，现在DWPT已经初始化好了，现在看updateDocument的具体实现，上面代码已经指出updateDocument最终调用DWPT.updateDocument完成：
```
//DocumentsWriterPerThread
public long updateDocument(Iterable<? extends IndexableField> doc, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {
    try {
      ...更新之前相关处理
      docState.doc = doc;
      docState.analyzer = analyzer;
      docState.docID = numDocsInRAM;//采用计数器分配当前文档ID
      ...
      boolean success = false;
      try {
        try {
        //consumer实际上是DefaultIndexingChain，负责具体操作，这里不再展开
          consumer.processDocument();
        } finally {
          docState.clear();
        }
        success = true;
      } finally {
        if (!success) {
        //如果更新失败，则标识删除此文档
          // mark document as deleted
          deleteDocID(docState.docID);
          numDocsInRAM++;
        }
      }
    //这里会处理具体的deleteSlice操作
      return finishDocument(deleteNode);
    } finally {
      maybeAbort("updateDocument", flushNotifications);
    }
  }

void deleteDocID(int docIDUpto) {
    //如果更新失败，则调用次函数删除更新的文档，这里直接放入DWPT自己的
    //pendingUpdates里，直接放入pendingUpdates里的删除操作不会进行全局同步
    //其实这也很合理，自己线程内发生的更新异常，只要自己记录，待刷新时删除
    //此文档保证不记录到文件中即可，其他的线程根本不需要知道这些。
    pendingUpdates.addDocID(docIDUpto);
    // NOTE: we do not trigger flush here.  This is
    // potentially a RAM leak, if you have an app that tries
    // to add docs but every single doc always hits a
    // non-aborting exception.  Allowing a flush here gets
    // very messy because we are only invoked when handling
    // exceptions so to do this properly, while handling an
    // exception we'd have to go off and flush new deletes
    // which is risky (likely would hit some other
    // confounding exception).
  }
```
DWPT.finishDocument会进行DWPT内部deleteSlice相关处理：
```
//DocumentsWriterPerThread
private long finishDocument(DocumentsWriterDeleteQueue.Node<?> deleteNode) {
    /*
     * here we actually finish the document in two steps 1. push the delete into
     * the queue and update our slice. 2. increment the DWPT private document
     * id.
     * 
     * the updated slice we get from 1. holds all the deletes that have occurred
     * since we updated the slice the last time.
     */
    boolean applySlice = numDocsInRAM != 0;
    long seqNo;
    //deleteNode != null表示是更新操作
    if (deleteNode != null) {
      seqNo = deleteQueue.add(deleteNode, deleteSlice);
      assert deleteSlice.isTail(deleteNode) : "expected the delete node as the tail";
    } else  {
        //表示deleteNode = null，表示是addDocument触发的调用，简单更新deleteSlice即可
      seqNo = deleteQueue.updateSlice(deleteSlice);
      
      if (seqNo < 0) {
        seqNo = -seqNo;
      } else {
        applySlice = false;
      }
    }
    
    if (applySlice) {
      deleteSlice.apply(pendingUpdates, numDocsInRAM);
    } else { // if we don't need to apply we must reset!
      deleteSlice.reset();
    }
    ++numDocsInRAM;

    return seqNo;
}
```

## 全局删除和局部删除之间的同步
删除入口函数的实现，其中也看到了deleteSlice、globalSlice、globalQueue、pendingUpdates的相关调用和操作，但是没有具体展开介绍，这里将详细介绍。

在阅读之前需要先理解第2节中对deleteSlice、pendingUpdates、globalQueue、globalSlice、globalBufferedUpdates的介绍。

首先有个前提，Term删除属于一种具体的删除，在segment写入disk时，只需要测试每个doc相应的Field是否匹配该Term，如果匹配则取消对该doc的写入即可。此时是不会产生liv文件的，因为应该写入的都写入了，被删除的doc也进行了过滤而没有写入文件，所以文件中所有的文件都是没有被删除的，也即都是存活的，因此不需要liv文件进行标记。

但是上述描述还有一种意外情况，如果DWPT在update文档时发生了异常，导致pendingUpdates.deleteDocIDs记录的由异常文档id，那么此时需要生成liv文件，当然这属于一种异常情况。

除了异常情况，使用liv文件进行标记的就是在发生Query删除时，上文也说过Query删除需要在所有DWPT刷新完之后，打开Reader进行查询，然后更新livDoc，再生成liv文件标识那些文件被删除。Query删除只会发生在全局删除，调用deleteDocuments实现。

首先看下globalSlice的初始化，在DocumentsWriterDeleteQueue的构造函数中：
```
//DocumentsWriterDeleteQueue
DocumentsWriterDeleteQueue(InfoStream infoStream, BufferedUpdates globalBufferedUpdates, long generation, long startSeqNo) {
    ...
    //尾部使用哨兵节点初始化
    tail = new Node<>(null); // sentinel
    //globalSlice默认指向globalQueue的尾部
    globalSlice = new DeleteSlice(tail);
}
```
如果此时进行了全局删除操作，比如调用deleteDocuments(Term... terms)
上面已经列出过源码，这里再次简要列出，删除操作还有一个优化，即如果缓存的删除太多会先解析一部分写入磁盘，以此释放一部分的内存占用，这里不做过多的介绍。
```
//IndexWriter
public long deleteDocuments(Term... terms) throws IOException {
    ensureOpen();
    try {
      long seqNo = docWriter.deleteTerms(terms);
      ...
}

//DocumentsWriter
 long deleteTerms(final Term... terms) throws IOException {
    return applyDeleteOrUpdate(q -> q.addDelete(terms));
}

private synchronized long applyDeleteOrUpdate(ToLongFunction<DocumentsWriterDeleteQueue> function) throws IOException {
    // TODO why is this synchronized?
    final DocumentsWriterDeleteQueue deleteQueue = this.deleteQueue;
    //这里调用q.addDelete(terms)
    long seqNo = function.applyAsLong(deleteQueue);
    flushControl.doOnDelete();
    lastSeqNo = Math.max(lastSeqNo, seqNo);
    //如果缓存的删除太多会先解析一部分写入磁盘，以此释放一部分的内存占用
    //这里不做过多的介绍
    if (applyAllDeletes(deleteQueue)) {
      seqNo = -seqNo;
    }
    return seqNo;
}

//DocumentsWriterDeleteQueue
long addDelete(Term... terms) {
    long seqNo = add(new TermArrayNode(terms));
    //更新globalSlice,将globalSlice中截取的删除操作放入
    //globalBufferedUpdates中
    tryApplyGlobalSlice();
    return seqNo;
  }
synchronized long add(Node<?> newNode) {
    tail.next = newNode;
    this.tail = newNode;
    return getNextSequenceNumber();
  }

//将globalSlice截取的Node节点放入globalBufferedUpdates中
void tryApplyGlobalSlice() {
    if (globalBufferLock.tryLock()) {
      /*
       * The global buffer must be locked but we don't need to update them if
       * there is an update going on right now. It is sufficient to apply the
       * deletes that have been added after the current in-flight global slices
       * tail the next time we can get the lock!
       */
      try {
          //更新globalSlice尾指针sliceTail使其指向globalQueue新的尾节点
        if (updateSliceNoSeqNo(globalSlice)) {
        //将globalSlice中截取的删除操作放入globalBufferedUpdates
          globalSlice.apply(globalBufferedUpdates, BufferedUpdates.MAX_INT);
        }
      } finally {
        globalBufferLock.unlock();
      }
    }
  }

   /** Just like updateSlice, but does not assign a sequence number */
  boolean updateSliceNoSeqNo(DeleteSlice slice) {
    if (slice.sliceTail != tail) {
      // new deletes arrived since we last checked
      //更新sliceTail使其指向新的尾节点
      slice.sliceTail = tail;
      return true;
    }
    return false;
  }
//DocumentsWriterDeleteQueue.DeleteSlice
void apply(BufferedUpdates del, int docIDUpto) {
    if (sliceHead == sliceTail) {
    // 0 length slice
    return;
    }
    /*
    * When we apply a slice we take the head and get its next as our first
    * item to apply and continue until we applied the tail. If the head and
    * tail in this slice are not equal then there will be at least one more
    * non-null node in the slice!
    */
    //对于sliceHead和sliceTail之间的所有Node，全部放入globalBufferedUpdates中
    Node<?> current = sliceHead;
    do {
        current = current.next;
        assert current != null : "slice property violated between the head on the tail must not be a null node";
        //具体Node的apply函数可自行查看代码，其实就是将Node中的Term或Query放入BufferedUpdates中
        current.apply(del, docIDUpto);
    } while (current != sliceTail);
    //重置globalSlice头尾节点指向位置
    reset();
}

void reset() {
    // Reset to a 0 length slice
    sliceHead = sliceTail;
}
```
DocumentsWriterDeleteQueue.add函数调用之后状态，此时globalSlice还没有更新：

DocumentsWriterDeleteQueue.add在globalQueue中添加一个节点之后，会调用tryApplyGlobalSlice(),tryApplyGlobalSlice()中会调用updateSliceNoSeqNo(globalSlice)将globalSlice的尾指针指向globalQueue最新尾节点:

tryApplyGlobalSlice()调用updateSliceNoSeqNo(globalSlice)之后会调用globalSlice.apply(globalBufferedUpdates, BufferedUpdates.MAX_INT)将globalSlice首尾节点之间的所有节点（不包含首节点包含尾节点）都加入globalBufferedUpdates里，最后调用'reset'将globalSlice的头节点指向其尾节点，其实也就是将globalSlice首尾节点都指向globalQueue最新的尾节点：

如果此时再次调用IndexWriter.deleteDocuments(Query... queries)并经历上述过程之后的状态为：

此时如果有更新操作，比如IndexWriter.updateDocument(Term term, Iterable<? extends IndexableField> doc)更新一个文档，则会启动一个DWPT,这里记为DWPT1，DWPT1会维护自己的deleteSlice和pendingUpdates：
```
//IndexWriter
public long updateDocument(Term term, Iterable<? extends IndexableField> doc) throws IOException {
    return updateDocument(term == null ? null : DocumentsWriterDeleteQueue.newNode(term), doc);
}

private long updateDocument(final DocumentsWriterDeleteQueue.Node<?> delNode,
                            Iterable<? extends IndexableField> doc) throws IOException {
    ensureOpen();
    boolean success = false;
    try {
    //调用DocumentsWriter.updateDocument
        long seqNo = docWriter.updateDocument(doc, analyzer, delNode);
        ...
}

//DocumentsWriter
long updateDocument(final Iterable<? extends IndexableField> doc, final Analyzer analyzer,
                      final DocumentsWriterDeleteQueue.Node<?> delNode) throws IOException {

    boolean hasEvents = preUpdate();

    final ThreadState perThread = flushControl.obtainAndLock();

    final DocumentsWriterPerThread flushingDWPT;
    long seqNo;
    try {
      // This must happen after we've pulled the ThreadState because IW.close
      // waits for all ThreadStates to be released:
      ensureOpen();
      //初始化DWPT
      ensureInitialized(perThread);
      assert perThread.isInitialized();
      final DocumentsWriterPerThread dwpt = perThread.dwpt;
      final int dwptNumDocs = dwpt.getNumDocsInRAM();
      try {
        //调用DWPT.updateDocument
        seqNo = dwpt.updateDocument(doc, analyzer, delNode, flushNotifications);
      } finally {
        if (dwpt.isAborted()) {
          flushControl.doOnAbort(perThread);
        }
        // We don't know whether the document actually
        // counted as being indexed, so we must subtract here to
        // accumulate our separate counter:
        numDocsInRAM.addAndGet(dwpt.getNumDocsInRAM() - dwptNumDocs);
      }
      final boolean isUpdate = delNode != null && delNode.isDelete();
      flushingDWPT = flushControl.doAfterDocument(perThread, isUpdate);

      assert seqNo > perThread.lastSeqNo: "seqNo=" + seqNo + " lastSeqNo=" + perThread.lastSeqNo;
      perThread.lastSeqNo = seqNo;

    } finally {
      perThreadPool.release(perThread);
    }

    if (postUpdate(flushingDWPT, hasEvents)) {
      seqNo = -seqNo;
    }
    
    return seqNo;
}

//DocumentsWriterPerThread
public long updateDocument(Iterable<? extends IndexableField> doc, Analyzer analyzer, DocumentsWriterDeleteQueue.Node<?> deleteNode, DocumentsWriter.FlushNotifications flushNotifications) throws IOException {
    try {
      ...
      reserveOneDoc();
      //保存该文档相关信息
      docState.doc = doc;
      docState.analyzer = analyzer;
      docState.docID = numDocsInRAM;
      if (INFO_VERBOSE && infoStream.isEnabled("DWPT")) {
        infoStream.message("DWPT", Thread.currentThread().getName() + " update delTerm=" + deleteNode + " docID=" + docState.docID + " seg=" + segmentInfo.name);
      }
      ...
      boolean success = false;
      try {
        try {
          consumer.processDocument();
        } finally {
          docState.clear();
        }
        success = true;
      } finally {
        if (!success) {
          // mark document as deleted
          //如果失败，则将该文档放入此DWPT pendingUpdates的deleteDocIDs中
          deleteDocID(docState.docID);
          numDocsInRAM++;
        }
      }
        //结束文档更新，开始此DWPT deleteSlice相关逻辑
      return finishDocument(deleteNode);
    } finally {
      maybeAbort("updateDocument", flushNotifications);
    }
}

private long finishDocument(DocumentsWriterDeleteQueue.Node<?> deleteNode) {
    /*
     * here we actually finish the document in two steps 1. push the delete into
     * the queue and update our slice. 2. increment the DWPT private document
     * id.
     * 
     * the updated slice we get from 1. holds all the deletes that have occurred
     * since we updated the slice the last time.
     */
    //因为上面已经插入一个文档，所以numDocsInRAM !=0，因此applySlice= true
    boolean applySlice = numDocsInRAM != 0;
    long seqNo;
    //如果不是通过addDocument触发的新增更新，则deleteNode不为空
    //此时将此deleteNode添加到deleteSlice中
    if (deleteNode != null) {
      seqNo = deleteQueue.add(deleteNode, deleteSlice);
      assert deleteSlice.isTail(deleteNode) : "expected the delete node as the tail";
    } else  {
      seqNo = deleteQueue.updateSlice(deleteSlice);
      
      if (seqNo < 0) {
        seqNo = -seqNo;
      } else {
        applySlice = false;
      }
    }
    //如果有删除操作，则将此时deleteSlice中维护的Node放入pendingUpdates中
    //否则重置deleteSlice的收尾指针.
    if (applySlice) {
      deleteSlice.apply(pendingUpdates, numDocsInRAM);
    } else { // if we don't need to apply we must reset!
      deleteSlice.reset();
    }
    ++numDocsInRAM;

    return seqNo;
  }

long add(Node<?> deleteNode, DeleteSlice slice) {
    long seqNo = add(deleteNode);
    ...
    slice.sliceTail = deleteNode;
    assert slice.sliceHead != slice.sliceTail : "slice head and tail must differ after add";
    //全局删除同步此局部删除
    tryApplyGlobalSlice(); // TODO doing this each time is not necessary maybe
    // we can do it just every n times or so?

    return seqNo;
  }
```
初始化一个新的DWPT，新的DWPT维护的pendingUpdates默认为空，没有任何元素，其deleteSlice首尾指针此时都会指向当前globalQueue的尾节点：

DWPT1在finishDocument中调用deleteQueue.add(deleteNode, deleteSlice)将删除节点添加到globalQueue中，并更新自己deleteSlice尾节点使其指向globalQueue最新尾节点：

上面DocumentsWriterDeleteQueue.add(Node<?> deleteNode, DeleteSlice slice)更新完DWPT1自己的deleteSlice之后，会调用tryApplyGlobalSlice，这个函数上面已经介绍过，会先调用updateSliceNoSeqNo(globalSlice)使globalSlice的尾指针指向最新的globalQueue尾节点：

之后会调用globalSlice.apply将DWPT1的删除操作放入globalBufferedUpdates中：

可见上面的状态途中已经发生了一次全局删除同步局部删除的操作，局部删除term3已经到了globalBufferedUpdates中。

发生完上述操作之后，DWPT1在finishDocument会调用deleteSlice.apply(pendingUpdates, numDocsInRAM)将自己deleteSlice首尾节点之间的Node添加到自己的pendingUpdates里：

通过上面的过程相信大家已经理解了全局删除和局部删除之间的同步原理。

有的读者可能会想DWPT1初始化时直接指向了globalQueue的尾节点，没有包含一开始的全局删除term1，那DWPT1在写入时怎么反映term1删除？

因为term1删除发生在DWPT1初始化之前，如果DWPT1没有操作过满足term1删除的文档，则DWPT1后续写入disk时根本不用理会term1删除，因为其缓存中根本没有相关的满足term1删除条件的文档。如果后续DWPT1又新增或更新了满足term1删除条件的doc，那么DWPT1刷盘时直接新增即可，也不用理会term1删除，这里从发生时间的先后可以理解为删除之后又新增了文档。

对于DocumentsWriterDeleteQueue的垃圾回收，在上述整个过程中globalQueue自身只有一个tail指向最新的尾节点，其他的就是globalSlice和deleteSlice对其还有引用，可以看上面最后一幅图，第一个null节点到TermNode3都属于不可达对象，因此都有机会在GC中被回收。

## 刷盘时对删除的处理
至此，已经介绍了删除的一些基础内容，下面就看下flush时，Lucene时如何处理删除操作的。

刷盘入口函数分析
因为IndexWriter.commit最终也会调用IndexWriter.flush，所以这里只介绍IndexWriter.flush。
```
//IndexWriter
/** Moves all in-memory segments to the {@link Directory}, but does not commit
*  (fsync) them (call {@link #commit} for that). */
public final void flush() throws IOException {
  flush(true, true);
}

/**
   * Flush all in-memory buffered updates (adds and deletes)
   * to the Directory.
   * @param triggerMerge if true, we may merge segments (if
   *  deletes or docs were flushed) if necessary
   * @param applyAllDeletes whether pending deletes should also
   */
final void flush(boolean triggerMerge, boolean applyAllDeletes) throws IOException {

  // NOTE: this method cannot be sync'd because
  // maybeMerge() in turn calls mergeScheduler.merge which
  // in turn can take a long time to run and we don't want
  // to hold the lock for that.  In the case of
  // ConcurrentMergeScheduler this can lead to deadlock
  // when it stalls due to too many running merges.

  // We can be called during close, when closing==true, so we must pass false to ensureOpen:
  //保证该索引是打开状态
  ensureOpen(false);
  //最终调用doFlush(applyAllDeletes)，之后会调用maybeMerge，也就是可能会触发一次段合并
  //不是本文重点，不会展开介绍
  if (doFlush(applyAllDeletes) && triggerMerge) {
    maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);
  }
}

/** Returns true a segment was flushed or deletes were applied. */
private boolean doFlush(boolean applyAllDeletes) throws IOException {
  ...
  doBeforeFlush();
  testPoint("startDoFlush");
  boolean success = false;
  try {

    ...
    boolean anyChanges = false;
    
    synchronized (fullFlushLock) {
      boolean flushSuccess = false;
      try {
        //调用DocumentsWriter.flushAllThreads刷新所有的DWPT
        long seqNo = docWriter.flushAllThreads();
        if (seqNo < 0) {
          seqNo = -seqNo;
          anyChanges = true;
        } else {
          anyChanges = false;
        }
        if (!anyChanges) {
          // flushCount is incremented in flushAllThreads
          flushCount.incrementAndGet();
        }
        //刷新之后的所有DWPT都会返回一个FlushedSegments放入ticketQueue中
        //这里就执行ticketQueue的purge操作
        publishFlushedSegments(true);
        flushSuccess = true;
      } finally {
        assert holdsFullFlushLock();
        docWriter.finishFullFlush(flushSuccess);
        processEvents(false);
      }
    }

    if (applyAllDeletes) {
      applyAllDeletesAndUpdates();
    }

    anyChanges |= maybeMerge.getAndSet(false);
    
    synchronized(this) {
      writeReaderPool(applyAllDeletes);
      doAfterFlush();
      success = true;
      return anyChanges;
    }
  } catch (VirtualMachineError tragedy) {
    tragicEvent(tragedy, "doFlush");
    throw tragedy;
  } finally {
    if (!success) {
      if (infoStream.isEnabled("IW")) {
        infoStream.message("IW", "hit exception during flush");
      }
      maybeCloseOnTragicEvent();
    }
  }
}
```
从上面可以看出，刷新的第一步是调用DocumentsWriter.flushAllThreads刷新所有的DWPT，前面说了Term删除属于局部、确定的删除，如果需要刷盘的doc能够匹配该term删除条件，就会被过滤掉。下面看下DocumentsWriter.flushAllThreads的具体逻辑

## DWPT刷盘：Term删除处理
DWPT刷盘时会拿自己的pendingUpdates中deleteTerms记录的term删除匹配所有的文档，如果匹配上则不会写入硬盘，所以此阶段不需要生成liv文件，因为刷到硬盘里的都是存活的，其实这里说的也不全对，“刷到硬盘里的都是存活的”是对于pendingUpdates里的deleteTerms删除来说的，因为有可能这些存活的文档能匹配query删除条件，也需要被删除。对于query删除的处理是在所有DWPT刷盘完成之后进行的，那时会生成liv文件，标识满足query删除的文档是dead的。

这里还存在第4节说的意外情况，如果pendingUpdates.deleteDocIDs中有因更新文档异常记录的需要删除的docID，则也会生成liv文件。

这里先建立这样的一个全局刷新观念，下面看一下DocumentsWriter.flushAllThreads函数：
```
//DocumentsWriter
/*
  * FlushAllThreads is synced by IW fullFlushLock. Flushing all threads is a
  * two stage operation; the caller must ensure (in try/finally) that finishFlush
  * is called after this method, to release the flush lock in DWFlushControl
  */
long flushAllThreads()
  throws IOException {
  final DocumentsWriterDeleteQueue flushingDeleteQueue;
  ...

  long seqNo;

  synchronized (this) {
    pendingChangesInCurrentFullFlush = anyChanges();
    flushingDeleteQueue = deleteQueue;
    /* Cutover to a new delete queue.  This must be synced on the flush control
      * otherwise a new DWPT could sneak into the loop with an already flushing
      * delete queue */
    seqNo = flushControl.markForFullFlush(); // swaps this.deleteQueue synced on FlushControl
    assert setFlushingDeleteQueue(flushingDeleteQueue);
  }
  assert currentFullFlushDelQueue != null;
  assert currentFullFlushDelQueue != deleteQueue;
  
  boolean anythingFlushed = false;
  try {
    DocumentsWriterPerThread flushingDWPT;
    // Help out with flushing:
    //这里依次刷新所有的DWPT
    while ((flushingDWPT = flushControl.nextPendingFlush()) != null) {
      anythingFlushed |= doFlush(flushingDWPT);
    }
    // If a concurrent flush is still in flight wait for it
    //因为有可能刷新时并发的，这里等待所有DWPT刷新完成
    flushControl.waitForFlush();  
    if (anythingFlushed == false && flushingDeleteQueue.anyChanges()) { // apply deletes if we did not flush any document
      if (infoStream.isEnabled("DW")) {
        infoStream.message("DW", Thread.currentThread().getName() + ": flush naked frozen global deletes");
      }
      assertTicketQueueModification(flushingDeleteQueue);
      //将globalQueue中的变更仿佛ticketQueue中，等待被purge
      ticketQueue.addDeletes(flushingDeleteQueue);
    }
    // we can't assert that we don't have any tickets in teh queue since we might add a DocumentsWriterDeleteQueue
    // concurrently if we have very small ram buffers this happens quite frequently
    assert !flushingDeleteQueue.anyChanges();
  } finally {
    assert flushingDeleteQueue == currentFullFlushDelQueue;
  }
  if (anythingFlushed) {
    return -seqNo;
  } else {
    return seqNo;
  }
}
```
下面看DWPT刷新操作：
```
private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {
  boolean hasEvents = false;
  while (flushingDWPT != null) {
    hasEvents = true;
    boolean success = false;
    DocumentsWriterFlushQueue.FlushTicket ticket = null;
    try {
      assert currentFullFlushDelQueue == null
          || flushingDWPT.deleteQueue == currentFullFlushDelQueue : "expected: "
          + currentFullFlushDelQueue + "but was: " + flushingDWPT.deleteQueue
          + " " + flushControl.isFullFlush();
      /*
        * Since with DWPT the flush process is concurrent and several DWPT
        * could flush at the same time we must maintain the order of the
        * flushes before we can apply the flushed segment and the frozen global
        * deletes it is buffering. The reason for this is that the global
        * deletes mark a certain point in time where we took a DWPT out of
        * rotation and freeze the global deletes.
        * 
        * Example: A flush 'A' starts and freezes the global deletes, then
        * flush 'B' starts and freezes all deletes occurred since 'A' has
        * started. if 'B' finishes before 'A' we need to wait until 'A' is done
        * otherwise the deletes frozen by 'B' are not applied to 'A' and we
        * might miss to deletes documents in 'A'.
        */
      try {
        assert assertTicketQueueModification(flushingDWPT.deleteQueue);
        // Each flush is assigned a ticket in the order they acquire the ticketQueue lock
        //每个DWPT刷新都会从tickerQueue获取一个ticker，用于保证顺序
        ticket = ticketQueue.addFlushTicket(flushingDWPT);
        final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();
        boolean dwptSuccess = false;
        try {
          // flush concurrently without locking
          //调用DWPT的刷新flush操作
          final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);
          //将DWPT.flush返回的FlushedSegment放到刚拿到的ticket位置上
          ticketQueue.addSegment(ticket, newSegment);
          dwptSuccess = true;
        } finally {
          subtractFlushedNumDocs(flushingDocsInRam);
          if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {
            //可以看下下面flushNotifications的定义，这里将需要删除的文件封装为Event放入eventQueue中
            Set<String> files = flushingDWPT.pendingFilesToDelete();
            flushNotifications.deleteUnusedFiles(files);
            hasEvents = true;
          }
          if (dwptSuccess == false) {
            flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());
            hasEvents = true;
          }
        }
        // flush was successful once we reached this point - new seg. has been assigned to the ticket!
        success = true;
      } finally {
        if (!success && ticket != null) {
          // In the case of a failure make sure we are making progress and
          // apply all the deletes since the segment flush failed since the flush
          // ticket could hold global deletes see FlushTicket#canPublish()
          ticketQueue.markTicketFailed(ticket);
        }
      }
      /*
        * Now we are done and try to flush the ticket queue if the head of the
        * queue has already finished the flush.
        */
      if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {
        // This means there is a backlog: the one
        // thread in innerPurge can't keep up with all
        // other threads flushing segments.  In this case
        // we forcefully stall the producers.
        flushNotifications.onTicketBacklog();
        break;
      }
    } finally {
      flushControl.doAfterFlush(flushingDWPT);
    }
    //当前DWPT处理完毕之后，会获取下一个DWPT进行刷新，这里感觉像工作密取模式
    //因为这里有一个while循环，如果本DWPT完成后处理下一个DWPT，在flushAllThreads
    //其实也是一个while循环从flushControl获取DWPT依次进行flush
    flushingDWPT = flushControl.nextPendingFlush();
  }

  if (hasEvents) {
    flushNotifications.afterSegmentsFlushed();
  }

  // If deletes alone are consuming > 1/2 our RAM
  // buffer, force them all to apply now. This is to
  // prevent too-frequent flushing of a long tail of
  // tiny segments:
  final double ramBufferSizeMB = config.getRAMBufferSizeMB();
  if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&
      flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {
    hasEvents = true;
    if (applyAllDeletes(deleteQueue) == false) {
      if (infoStream.isEnabled("DW")) {
        infoStream.message("DW", String.format(Locale.ROOT, "force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB",
                                                flushControl.getDeleteBytesUsed()/(1024.*1024.),
                                                ramBufferSizeMB));
      }
      flushNotifications.onDeletesApplied();
    }
  }

  return hasEvents;
}
```
上面代码中出现了flushNotifications，因为DWPT的flush操作有可能是并发的，熟悉异步操作的读者可以知道，异步操作一般都会传入一个callback，在异步操作完成、发生异常时等会调用callback，flushNotifications就相当于callback。

flushNotifications是DocumentsWriter的一个成员变量，其实是在IndexWriter中实例化的，通过DocumentsWriter构造函数传入的。

flushNotifications在IndexWriter定义如下：
```
//从上面可以看到，主要就是将一些刷新过程中的操作定义成Event放入IndexWriter维护的eventQueue中。
private final DocumentsWriter.FlushNotifications flushNotifications = new DocumentsWriter.FlushNotifications() {
  @Override
  public void deleteUnusedFiles(Collection<String> files) {
    eventQueue.add(w -> w.deleteNewFiles(files));
  }

  @Override
  public void flushFailed(SegmentInfo info) {
    eventQueue.add(w -> w.flushFailed(info));
  }

  @Override
  public void afterSegmentsFlushed() throws IOException {
    try {
      publishFlushedSegments(false);
    } finally {
      if (false) {
        maybeMerge(config.getMergePolicy(), MergeTrigger.SEGMENT_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);
      }
    }
  }

  @Override
  public void onTragicEvent(Throwable event, String message) {
    IndexWriter.this.onTragicEvent(event, message);
  }

  @Override
  public void onDeletesApplied() {
    eventQueue.add(w -> {
        try {
          w.publishFlushedSegments(true);
        } finally {
          flushCount.incrementAndGet();
        }
      }
    );
  }

  @Override
  public void onTicketBacklog() {
    eventQueue.add(w -> w.publishFlushedSegments(true));
  }
};
```
DWPT刷盘之前，会调用ticket = ticketQueue.addFlushTicket(flushingDWPT)获取一个ticket, ticketQueue在第2节已经介绍过，可继续看下面的过程加深对ticketQueue的理解：
```
//DocumentsWriterFlushQueue
synchronized FlushTicket addFlushTicket(DocumentsWriterPerThread dwpt) throws IOException {
  // Each flush is assigned a ticket in the order they acquire the ticketQueue
  // lock
  incTickets();
  boolean success = false;
  try {
    // prepare flush freezes the global deletes - do in synced block!
    //dwpt.prepareFlush会把当前globalSlice中截取的globalQueue放入
    //globalBufferedQueue中，并对其进行冻结，返回冻结之后的FrozenBufferedUpdates实例
    //FlushTicket.frozenUpdates保存的就是刚冻结的全局globalBufferedUpdates.
    //这里有个需要注意的点，如果flush过程中没有Term删除和Query删除
    //的发生，那么后续的DWPT刷新时就不会产生冻结的globalBufferedUpdates,
    //因为此时globalSlice首尾指针都已经指向了globalQueue的尾节点
    final FlushTicket ticket = new FlushTicket(dwpt.prepareFlush(), true);
    queue.add(ticket);
    success = true;
    return ticket;
  } finally {
    if (!success) {
      decTickets();
    }
  }
}

FrozenBufferedUpdates prepareFlush() throws IOException {
    assert numDocsInRAM > 0;
    //冻结globalBufferedUpdates过程中也会将当前DWPT的deleteSlice尾指针指向当前globalQueue尾节点
    //以便后续deleteSlice.appy
    final FrozenBufferedUpdates globalUpdates = deleteQueue.freezeGlobalBuffer(deleteSlice);
    /* deleteSlice can possibly be null if we have hit non-aborting exceptions during indexing and never succeeded 
    adding a document. */
    if (deleteSlice != null) {
      // apply all deletes before we flush and release the delete slice
      //将当前DWPT deleteSlice截取的globalQueue中操作放入其pendingUpdates中
      deleteSlice.apply(pendingUpdates, numDocsInRAM);
      assert deleteSlice.isEmpty();
      deleteSlice.reset();
    }
    return globalUpdates;
  }
//DocumentsWriterFlushQueue
  FrozenBufferedUpdates freezeGlobalBuffer(DeleteSlice callerSlice) throws IOException {
    globalBufferLock.lock();
    /*
     * Here we freeze the global buffer so we need to lock it, apply all
     * deletes in the queue and reset the global slice to let the GC prune the
     * queue.
     */
    final Node<?> currentTail = tail; // take the current tail make this local any
    // Changes after this call are applied later
    // and not relevant here
    if (callerSlice != null) {
      // Update the callers slices so we are on the same page
      //这里也会将当前刷新的DWPT的deleteSlice尾指针更新为globalQueue
      //最新尾节点，等待deleteSlice后续apply
      callerSlice.sliceTail = currentTail;
    }
    try {
      if (globalSlice.sliceTail != currentTail) {
        globalSlice.sliceTail = currentTail;
        //将globalSlice首尾节点截取的globalQueue更新放入globalBufferedUpdates中
        globalSlice.apply(globalBufferedUpdates, BufferedUpdates.MAX_INT);
      }

      if (globalBufferedUpdates.any()) {
        //冻结globalBufferedUpdates
        final FrozenBufferedUpdates packet = new FrozenBufferedUpdates(infoStream, globalBufferedUpdates, null);
        //清空globalBufferedUpdates
        globalBufferedUpdates.clear();
        return packet;
      } else {
        return null;
      }
    } finally {
      globalBufferLock.unlock();
    }
  }

//DocumentsWriterFlushQueue.FlushTicket
//FlushTicket是DocumentsWriterFlushQueue的内部类，封装了DWPT刷新之后的段视图FlushedSegment以及该DWPT pendingUpdates冻结版本frozenUpdates
static final class FlushTicket {
    private final FrozenBufferedUpdates frozenUpdates;
    private final boolean hasSegment;
    private FlushedSegment segment;
    private boolean failed = false;
    private boolean published = false;

    FlushTicket(FrozenBufferedUpdates frozenUpdates, boolean hasSegment) {
      this.frozenUpdates = frozenUpdates;
      this.hasSegment = hasSegment;
    }

    boolean canPublish() {
      return hasSegment == false || segment != null || failed;
    }

    synchronized void markPublished() {
      assert published == false: "ticket was already published - can not publish twice";
      published = true;
    }

    private void setSegment(FlushedSegment segment) {
      assert !failed;
      this.segment = segment;
    }

    private void setFailed() {
      assert segment == null;
      failed = true;
    }

    /**
     * Returns the flushed segment or <code>null</code> if this flush ticket doesn't have a segment. This can be the
     * case if this ticket represents a flushed global frozen updates package.
     */
    FlushedSegment getFlushedSegment() {
      return segment;
    }

    /**
     * Returns a frozen global deletes package.
     */
    FrozenBufferedUpdates getFrozenUpdates() {
      return frozenUpdates;
    }
  }
}
```
下面看DWPT.flush函数实现：
```
//DocumentsWriterPerThread
/** Flush all pending docs to a new segment */
FlushedSegment flush(DocumentsWriter.FlushNotifications flushNotifications) throws IOException {
  assert numDocsInRAM > 0;
  assert deleteSlice.isEmpty() : "all deletes must be applied in prepareFlush";
  segmentInfo.setMaxDoc(numDocsInRAM);
  //首先将本地维护的一些信息封装为一个SegmentWriteState对象进行处理
  //本地维护的信息包括：sementInfo、pendingUpdates等
  final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segmentInfo, fieldInfos.finish(),
      pendingUpdates, new IOContext(new FlushInfo(numDocsInRAM, bytesUsed())));
  final double startMBUsed = bytesUsed() / 1024. / 1024.;

  // Apply delete-by-docID now (delete-byDocID only
  // happens when an exception is hit processing that
  // doc, eg if analyzer has some problem w/ the text):
  //处理pendingUpdates维护的deleteDocIDs，本文一开始介绍了pendingUpdates中维护的三大类更新操作，
  //deleteDocIDs主要维护更新异常之后需要删除的doc，这里可以直接在flushState.liveDocs标识其出来
  //如果deleteDocIDs不为空，则此DWPT刷盘会生成liv文件
  if (pendingUpdates.deleteDocIDs.size() > 0) {
    flushState.liveDocs = new FixedBitSet(numDocsInRAM);
    flushState.liveDocs.set(0, numDocsInRAM);
    for(int delDocID : pendingUpdates.deleteDocIDs) {
      flushState.liveDocs.clear(delDocID);
    }
    flushState.delCountOnFlush = pendingUpdates.deleteDocIDs.size();
    pendingUpdates.bytesUsed.addAndGet(-pendingUpdates.deleteDocIDs.size() * BufferedUpdates.BYTES_PER_DEL_DOCID);
    //pendingUpdates.deleteDocIDs已经处理了，即转化为liveDocs中标志为了，所有此处将其清空
    pendingUpdates.deleteDocIDs.clear();
  }

  if (aborted) {
    if (infoStream.isEnabled("DWPT")) {
      infoStream.message("DWPT", "flush: skip because aborting is set");
    }
    return null;
  }

  long t0 = System.nanoTime();

  if (infoStream.isEnabled("DWPT")) {
    infoStream.message("DWPT", "flush postings as segment " + flushState.segmentInfo.name + " numDocs=" + numDocsInRAM);
  }
  final Sorter.DocMap sortMap;
  try {
    DocIdSetIterator softDeletedDocs;
    if (indexWriterConfig.getSoftDeletesField() != null) {
      softDeletedDocs = consumer.getHasDocValues(indexWriterConfig.getSoftDeletesField());
    } else {
      softDeletedDocs = null;
    }
    //进行实际的刷盘操作
    sortMap = consumer.flush(flushState);
    if (softDeletedDocs == null) {
      flushState.softDelCountOnFlush = 0;
    } else {
      flushState.softDelCountOnFlush = PendingSoftDeletes.countSoftDeletes(softDeletedDocs, flushState.liveDocs);
      assert flushState.segmentInfo.maxDoc() >= flushState.softDelCountOnFlush + flushState.delCountOnFlush;
    }
    // We clear this here because we already resolved them (private to this segment) when writing postings:
    //因为在刷盘时已经处理了term删除，所以这里可以将其清除
    pendingUpdates.clearDeleteTerms();
    //将此DWPT维护的segmentInfo文件更新为此次刷新新创建的文件
    segmentInfo.setFiles(new HashSet<>(directory.getCreatedFiles()));

    final SegmentCommitInfo segmentInfoPerCommit = new SegmentCommitInfo(segmentInfo, 0, flushState.softDelCountOnFlush, -1L, -1L, -1L);
    ...

    final BufferedUpdates segmentDeletes;
    if (pendingUpdates.deleteQueries.isEmpty() && pendingUpdates.numericUpdates.isEmpty() && pendingUpdates.binaryUpdates.isEmpty()) {
      pendingUpdates.clear();
      segmentDeletes = null;
    } else {
      segmentDeletes = pendingUpdates;
    }

    ...

    assert segmentInfo != null;
    //这里类似于为此DWPT刷新之后状态建立一个视图，返回给DocumentsWriter
    FlushedSegment fs = new FlushedSegment(infoStream, segmentInfoPerCommit, flushState.fieldInfos,
        segmentDeletes, flushState.liveDocs, flushState.delCountOnFlush, sortMap);
    sealFlushedSegment(fs, sortMap, flushNotifications);
    if (infoStream.isEnabled("DWPT")) {
      infoStream.message("DWPT", "flush time " + ((System.nanoTime() - t0) / 1000000.0) + " msec");
    }
    return fs;
  } catch (Throwable t) {
    onAbortingException(t);
    throw t;
  } finally {
    maybeAbort("flush", flushNotifications);
  }
}

```
上面的consumer.flush(flushState)负责进行实际的刷盘动作，默认的consumer为DefaultIndexingChain，下面列出其flush函数代码，其实这里已经超出了本文标题应该涵盖的内容，因为DWPT.flush已经处理了term删除，
在consumer.flush(flushState)会进行各个索引要素的刷盘动作，如nvd,dvd,dim,dii，最后需要注意的是liv文件会通过位图标识出被删除的doc，但是其他文件中该文档的相关信息并没有被删除。
```
@Override
public Sorter.DocMap flush(SegmentWriteState state) throws IOException {

  // NOTE: caller (DocumentsWriterPerThread) handles
  // aborting on any exception from this method
  Sorter.DocMap sortMap = maybeSortSegment(state);
  int maxDoc = state.segmentInfo.maxDoc();
  long t0 = System.nanoTime();
  writeNorms(state, sortMap);
  if (docState.infoStream.isEnabled("IW")) {
    docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write norms");
  }
  
  t0 = System.nanoTime();
  writeDocValues(state, sortMap);
  if (docState.infoStream.isEnabled("IW")) {
    docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write docValues");
  }

  t0 = System.nanoTime();
  writePoints(state, sortMap);
  if (docState.infoStream.isEnabled("IW")) {
    docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write points");
  }
  
  // it's possible all docs hit non-aborting exceptions...
  t0 = System.nanoTime();
  storedFieldsConsumer.finish(maxDoc);
  storedFieldsConsumer.flush(state, sortMap);
  if (docState.infoStream.isEnabled("IW")) {
    docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to finish stored fields");
  }

  t0 = System.nanoTime();
  Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();
  for (int i=0;i<fieldHash.length;i++) {
    PerField perField = fieldHash[i];
    while (perField != null) {
      if (perField.invertState != null) {
        fieldsToFlush.put(perField.fieldInfo.name, perField.termsHashPerField);
      }
      perField = perField.next;
    }
  }

  termsHash.flush(fieldsToFlush, state, sortMap);
  if (docState.infoStream.isEnabled("IW")) {
    docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write postings and finish vectors");
  }

  // Important to save after asking consumer to flush so
  // consumer can alter the FieldInfo* if necessary.  EG,
  // FreqProxTermsWriter does this with
  // FieldInfo.storePayload.
  t0 = System.nanoTime();
  docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, "", state.fieldInfos, IOContext.DEFAULT);
  if (docState.infoStream.isEnabled("IW")) {
    docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write fieldInfos");
  }

  return sortMap;
}
```
到这里，DWPT刷盘已经完成，并向DocumentsWriter返回一个FlushedSegment实例。从上面的doFlush(DocumentsWriterPerThread flushingDWPT)代码可以看出，此时使用DWPT刷盘前的ticket，设置其FlushedSegment，即ticketQueue.addSegment(ticket, newSegment)

## 所有DWPT刷盘之后处理Query删除
如果所有的DWPT都完成了刷盘操作，DocumentsWriter.doFlush(DocumentsWriterPerThread flushingDWPT)会调用flushNotifications.afterSegmentsFlushed()，从flushNotifications定义可以知道afterSegmentsFlushed操作如下：
```
//IndexWriter.new DocumentsWriter.FlushNotifications
@Override
public void afterSegmentsFlushed() throws IOException {
  try {
    publishFlushedSegments(false);
  } finally {
    if (false) {
      maybeMerge(config.getMergePolicy(), MergeTrigger.SEGMENT_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);
    }
  }
}
//forced为false，表示尝试purge ticketQueue中的记录
void publishFlushedSegments(boolean forced) throws IOException {
    docWriter.purgeFlushTickets(forced, ticket -> {
      DocumentsWriterPerThread.FlushedSegment newSegment = ticket.getFlushedSegment();
      //这里的获取的冻结的updates其实就是第一个刷新的DWPT冻结的全局
      //globalBufferedUpdates，见上面的ticketQueue.addFlushTicket(flushingDWPT);函数注释
      FrozenBufferedUpdates bufferedUpdates = ticket.getFrozenUpdates();
      ticket.markPublished();
      //如果该ticket没有FlushedSegment，则表示是DocumentsWriter.flushAllThreads在flush完所有DWPT后
      //使用ticketQueue.addDeletes(flushingDeleteQueue)放入的globalQueue
      if (newSegment == null) { // this is a flushed global deletes package - not a segments
        if (bufferedUpdates != null && bufferedUpdates.any()) { // TODO why can this be null?
          publishFrozenUpdates(bufferedUpdates);
          if (infoStream.isEnabled("IW")) {
            infoStream.message("IW", "flush: push buffered updates: " + bufferedUpdates);
          }
        }
      } else {
        //每个DWPT刷盘之后放入的FlushedSegment
        assert newSegment.segmentInfo != null;
        if (infoStream.isEnabled("IW")) {
          infoStream.message("IW", "publishFlushedSegment seg-private updates=" + newSegment.segmentUpdates);
        }
        if (newSegment.segmentUpdates != null && infoStream.isEnabled("DW")) {
          infoStream.message("IW", "flush: push buffered seg private updates: " + newSegment.segmentUpdates);
        }
        // now publish!
        //bufferedUpdates是冻结的全局globalBufferedUpdates
        //newSegment.segmentUpdates才是DWPT刷新时自己的pendingUpdates
        publishFlushedSegment(newSegment.segmentInfo, newSegment.fieldInfos, newSegment.segmentUpdates,
            bufferedUpdates, newSegment.sortMap);
      }
    });
  }

//DocumentsWriter
void purgeFlushTickets(boolean forced, IOUtils.IOConsumer<DocumentsWriterFlushQueue.FlushTicket> consumer)
      throws IOException {
    if (forced) {
      ticketQueue.forcePurge(consumer);
    } else {
      ticketQueue.tryPurge(consumer);
    }
  }

//DocumentsWriterFlushQueue
 void tryPurge(IOUtils.IOConsumer<FlushTicket> consumer) throws IOException {
    assert !Thread.holdsLock(this);
    if (purgeLock.tryLock()) {
      try {
        innerPurge(consumer);
      } finally {
        purgeLock.unlock();
      }
    }
  }

 private void innerPurge(IOUtils.IOConsumer<FlushTicket> consumer) throws IOException {
    assert purgeLock.isHeldByCurrentThread();
    while (true) {
      final FlushTicket head;
      final boolean canPublish;
      synchronized (this) {
        head = queue.peek();
        canPublish = head != null && head.canPublish(); // do this synced 
      }
      if (canPublish) {
        try {
          /*
           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block
           * concurrent segment flushes just because they want to append to the queue.
           * the downside is that we need to force a purge on fullFlush since there could
           * be a ticket still in the queue. 
           */
           //consumer为IndexWriter.publishFlushedSegments传入的参数
          consumer.accept(head);

        } finally {
          synchronized (this) {
            // finally remove the published ticket from the queue
            final FlushTicket poll = queue.poll();
            decTickets();
            // we hold the purgeLock so no other thread should have polled:
            assert poll == head;
          }
        }
      } else {
        break;
      }
    }
  }
```
这里首先看IndexWriter.publishFlushedSegment函数
```
//IndexWriter
/**
   * Atomically adds the segment private delete packet and publishes the flushed
   * segments SegmentInfo to the index writer.
   */
  private synchronized void publishFlushedSegment(SegmentCommitInfo newSegment, FieldInfos fieldInfos,
                                                  FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,
                                                  Sorter.DocMap sortMap) throws IOException {
    boolean published = false;
    try {
      // Lock order IW -> BDS
      ensureOpen(false);

      if (infoStream.isEnabled("IW")) {
        infoStream.message("IW", "publishFlushedSegment " + newSegment);
      }

      if (globalPacket != null && globalPacket.any()) {
        //这里处理DWPT刷新时冻结的globalBufferedUpdates
        publishFrozenUpdates(globalPacket);
      }

      // Publishing the segment must be sync'd on IW -> BDS to make the sure
      // that no merge prunes away the seg. private delete packet
      final long nextGen;
      if (packet != null && packet.any()) {
        //这里处理DWPT自身的pendingUpdates
        nextGen = publishFrozenUpdates(packet);
      } else {
        // Since we don't have a delete packet to apply we can get a new
        // generation right away
        nextGen = bufferedUpdatesStream.getNextGen();
        // No deletes/updates here, so marked finished immediately:
        bufferedUpdatesStream.finishedSegment(nextGen);
      }
      //这里其实有个问题，根据上面的代码发现，即处理了冻结的全局globalBufferedUpdates
      //又处理了DWPT吱声的pendingUpdates，而二者之间是有同步关系的，
      //会不会造成处理两次？
      //其实通过源码实际调试发现，因为全局删除和局部删除之间的同步
      //DWPT初始化之后，globalBufferedUpdates中新增的删除操作
      //确实也会在pendingUpdates中出现。
      //避免对相同的操作执行两次删除的关键就在于这里的deleteGen。
      //首先IndexWriter在其成员bufferedUpdatesStream中维护了一个全局的delGen，称为nextGen
      //每次调用publishFrozenUpdates都会调用bufferedUpdatesStream.push递增该nextGen
      //并返回之前的nextGen（类似于return nextGen++）
      //除此之外，每个BufferedUpdates也有个delGen，该delGen也在
      //bufferedUpdatesStream.push设置，设置为push中递增之前的nextGen
      //假设当前bufferedUpdatesStream.nextGen = 1(也是源码中的默认值)
      //publishFrozenUpdates(globalPacket);（即publish全局变更）之后，globalPakcet.delGen=1,bufferedUpdatesStream.nextGen = 2;
      //nextGen = publishFrozenUpdates(packet)（即publish DWPT变更）之后，packet.delGen = 2,bufferedUpdatesStream.nextGen = 3; 但是这里publishFrozenUpdates(packet)返回值等于递增之前的值，即2，nextGen=2;
      //下面语句会设置当前待刷新段的delGen为2
      //在实际(FrozenBufferedUpdates)pakcet.apply(IndexWriter writer)中调用openSegmentStates时，会调用openSegmentStates
      //里面有个判断info.getBufferedDeletesGen() <= delGen
      //info.getBufferedDeletesGen() 返回的就是下面语句设置newSegment的delGen，小于等于右边的delGen就是packet的delGen
      //所以根据上面举例，globalPakcet的delGen肯定要小于info.getBufferedDeletesGen，所以在既有globalUpdates，又有pendingUpdates时，只有pendingUpdates才能打开相应的segment并执行。
      //这样就避免了因为同步造成的两次执行。

      //如果该DWPT没有pendingUpdates，则上面else里执行nextGen = bufferedUpdatesStream.getNextGen();
      //这样也会令globalPakcet的delGen肯定要小于info.getBufferedDeletesGen，避免了globalUpdates的执行

      //根据上面的注释也发现了globalBufferedUpdates优先级要小于pendingUpdates
      newSegment.setBufferedDeletesGen(nextGen);
      segmentInfos.add(newSegment);
      published = true;
      checkpoint();
      if (packet != null && packet.any() && sortMap != null) {
        // TODO: not great we do this heavyish op while holding IW's monitor lock,
        // but it only applies if you are using sorted indices and updating doc values:
        ReadersAndUpdates rld = getPooledInstance(newSegment, true);
        rld.sortMap = sortMap;
        // DON't release this ReadersAndUpdates we need to stick with that sortMap
      }
      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present
      // this is a corner case where documents delete them-self with soft deletes. This is used to
      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.
      // if we have seen updates the update code checks if the segment is fully deleted.
      boolean hasInitialSoftDeleted = (fieldInfo != null
          && fieldInfo.getDocValuesGen() == -1
          && fieldInfo.getDocValuesType() != DocValuesType.NONE);
      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();
      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need
      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers
      // for the soft delete right after we flushed to disk.
      if (hasInitialSoftDeleted || isFullyHardDeleted){
        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed
        // if we deleted all docs in this newly flushed segment.
        ReadersAndUpdates rld = getPooledInstance(newSegment, true);
        try {
          if (isFullyDeleted(rld)) {
            dropDeletedSegment(newSegment);
            checkpoint();
          }
        } finally {
          release(rld);
        }
      }

    } finally {
      if (published == false) {
        adjustPendingNumDocs(-newSegment.info.maxDoc());
      }
      flushCount.incrementAndGet();
      doAfterFlush();
    }

}

synchronized long publishFrozenUpdates(FrozenBufferedUpdates packet) {
    assert packet != null && packet.any();
    long nextGen = bufferedUpdatesStream.push(packet);
    // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:
    //向eventQueue中放入一个Event，该event调用了FrozenBufferedUpdates.apply(IndexWriter)方法。
    eventQueue.add(w -> {
      try {
        packet.apply(w);
      } catch (Throwable t) {
        try {
          w.onTragicEvent(t, "applyUpdatesPacket");
        } catch (Throwable t1) {
          t.addSuppressed(t1);
        }
        throw t;
      }
      w.flushDeletesCount.incrementAndGet();
    });
    return nextGen;
  }

```
下面看IndexWriter.eventQueue的相关处理,eventQueue里放入的Event会在IndexWriter.doFlush结束docWriter.flushAllThreads()之后调用processEvents(false)进行处理：
```
//IndexWriter
private void processEvents(boolean triggerMerge) throws IOException {
    if (tragedy.get() == null) {
      Event event;
      while ((event = eventQueue.poll()) != null)  {
        event.process(this);
      }
    }
    if (triggerMerge) {
      maybeMerge(getConfig().getMergePolicy(), MergeTrigger.SEGMENT_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);
    }
  }
```
我们直接看刚放入调用FrozenBufferedUpdates.apply(IndexWriter)的事件，即FrozenBufferedUpdates.apply(IndexWriter)函数:
```
//ForzenBufferedUpdates
 /** Translates a frozen packet of delete term/query, or doc values
  *  updates, into their actual docIDs in the index, and applies the change.  This is a heavy
  *  operation and is done concurrently by incoming indexing threads. */
@SuppressWarnings("try")
public synchronized void apply(IndexWriter writer) throws IOException {
  if (applied.getCount() == 0) {
    // already done
    return;
  }

  long startNS = System.nanoTime();

  assert any();

  Set<SegmentCommitInfo> seenSegments = new HashSet<>();

  int iter = 0;
  int totalSegmentCount = 0;
  long totalDelCount = 0;

  boolean finished = false;

  // Optimistic concurrency: assume we are free to resolve the deletes against all current segments in the index, despite that
  // concurrent merges are running.  Once we are done, we check to see if a merge completed while we were running.  If so, we must retry
  // resolving against the newly merged segment(s).  Eventually no merge finishes while we were running and we are done.
  while (true) {
    String messagePrefix;
    if (iter == 0) {
      messagePrefix = "";
    } else {
      messagePrefix = "iter " + iter;
    }

    long iterStartNS = System.nanoTime();

    long mergeGenStart = writer.mergeFinishedGen.get();

    Set<String> delFiles = new HashSet<>();
    BufferedUpdatesStream.SegmentState[] segStates;

    synchronized (writer) {
      //取得IndexWriter里维护的SegmentInfos
      List<SegmentCommitInfo> infos = getInfosToApply(writer);
      if (infos == null) {
        break;
      }

      for (SegmentCommitInfo info : infos) {
        delFiles.addAll(info.files());
      }

      // Must open while holding IW lock so that e.g. segments are not merged
      // away, dropped from 100% deletions, etc., before we can open the readers
      //每个SegmentInfo都生成一个SegmentState，这里涉及到ReadersAndUpdates
      //ReadersAndUpdates负责对内存中SegmentInfo应用删除、更新操作
      segStates = openSegmentStates(writer, infos, seenSegments, delGen());

      if (segStates.length == 0) {

        if (infoStream.isEnabled("BD")) {
          infoStream.message("BD", "packet matches no segments");
        }
        break;
      }

      ...

      totalSegmentCount += segStates.length;

      // Important, else IFD may try to delete our files while we are still using them,
      // if e.g. a merge finishes on some of the segments we are resolving on:
      writer.deleter.incRef(delFiles);
    }

    AtomicBoolean success = new AtomicBoolean();
    long delCount;
    try (Closeable finalizer = () -> finishApply(writer, segStates, success.get(), delFiles)) {
      // don't hold IW monitor lock here so threads are free concurrently resolve deletes/updates:
      //调用ForzenBufferedUpdates.apply(BufferedUpdatesStream.SegmentState[] segStates)函数
      delCount = apply(segStates);
      success.set(true);
    }

    // Since we just resolved some more deletes/updates, now is a good time to write them:
    //因为上面对内存中segment进行了更新，所以这里将更新的内容写入磁盘
    writer.writeSomeDocValuesUpdates();

    // It's OK to add this here, even if the while loop retries, because delCount only includes newly
    // deleted documents, on the segments we didn't already do in previous iterations:
    totalDelCount += delCount;

    ...
    if (privateSegment != null) {
      // No need to retry for a segment-private packet: the merge that folds in our private segment already waits for all deletes to
      // be applied before it kicks off, so this private segment must already not be in the set of merging segments

      break;
    }

    // Must sync on writer here so that IW.mergeCommit is not running concurrently, so that if we exit, we know mergeCommit will succeed
    // in pulling all our delGens into a merge:
    synchronized (writer) {
      long mergeGenCur = writer.mergeFinishedGen.get();

      if (mergeGenCur == mergeGenStart) {

        // Must do this while still holding IW lock else a merge could finish and skip carrying over our updates:
        
        // Record that this packet is finished:
        writer.finished(this);

        finished = true;

        // No merge finished while we were applying, so we are done!
        break;
      }
    }

    if (infoStream.isEnabled("BD")) {
      infoStream.message("BD", messagePrefix + "concurrent merges finished; move to next iter");
    }
      
    // A merge completed while we were running.  In this case, that merge may have picked up some of the updates we did, but not
    // necessarily all of them, so we cycle again, re-applying all our updates to the newly merged segment.

    iter++;
  }

  if (finished == false) {
    // Record that this packet is finished:
    writer.finished(this);
  }
      
  ...
    message += "; " + writer.getPendingUpdatesCount() + " packets remain";
    infoStream.message("BD", message);
  }
}

/** Applies pending delete-by-term, delete-by-query and doc values updates to all segments in the index, returning
  *  the number of new deleted or updated documents. */
private synchronized long apply(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {

  if (delGen == -1) {
    // we were not yet pushed
    throw new IllegalArgumentException("gen is not yet set; call BufferedUpdatesStream.push first");
  }

  assert applied.getCount() != 0;

  if (privateSegment != null) {
    assert segStates.length == 1;
    assert privateSegment == segStates[0].reader.getOriginalSegmentInfo();
  }

  //如果是FlushedSegment中的updates，则不会包含term删除，因为已经在DWPT刷盘时处理过了
  totalDelCount += aplyTermDeletes(segStates);
  //处理Query删除
  totalDelCount += applyQueryDeletes(segStates);
  //处理DocValues删除，不在本文介绍范围之内
  totalDelCount += applyDocValuesUpdates(segStates);

  return totalDelCount;
}

// Delete by query
private long applyQueryDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {

  if (deleteQueries.length == 0) {
    return 0;
  }

  long startNS = System.nanoTime();

  long delCount = 0;
  //双重for循环，对于每个segStage，运行每个query删除
  for (BufferedUpdatesStream.SegmentState segState : segStates) {

    if (delGen < segState.delGen) {
      // segment is newer than this deletes packet
      continue;
    }
    
    if (segState.rld.refCount() == 1) {
      // This means we are the only remaining reference to this segment, meaning
      // it was merged away while we were running, so we can safely skip running
      // because we will run on the newly merged segment next:
      continue;
    }

    final LeafReaderContext readerContext = segState.reader.getContext();
    for (int i = 0; i < deleteQueries.length; i++) {
      Query query = deleteQueries[i];
      int limit;
      if (delGen == segState.delGen) {
        assert privateSegment != null;
        limit = deleteQueryLimits[i];
      } else {
        limit = Integer.MAX_VALUE;
      }
      //进行查询
      final IndexSearcher searcher = new IndexSearcher(readerContext.reader());
      searcher.setQueryCache(null);
      query = searcher.rewrite(query);
      final Weight weight = searcher.createWeight(query, false, 1);
      final Scorer scorer = weight.scorer(readerContext);
      if (scorer != null) {
        final DocIdSetIterator it = scorer.iterator();
        if (segState.rld.sortMap != null && limit != Integer.MAX_VALUE) {
          assert privateSegment != null;
          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:
          int docID;
          while ((docID = it.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
            // The limit is in the pre-sorted doc space:
            if (segState.rld.sortMap.newToOld(docID) < limit) {
              //删除满足Query条件的文档
              if (segState.rld.delete(docID)) {
                delCount++;
              }
            }
          }
        } else {
          int docID;
          while ((docID = it.nextDoc()) < limit) {
            if (segState.rld.delete(docID)) {
              delCount++;
            }
          }
        }
      }
    }
  }

 ...
  return delCount;
}

//ReadersAndUpdates
//ReadersAndUpdates.pengdingDeletes记录删除的docID
public synchronized boolean delete(int docID) throws IOException {
  if (reader == null && pendingDeletes.mustInitOnDelete()) {
    getReader(IOContext.READ).decRef(); // pass a reader to initialize the pending deletes
  }
  return pendingDeletes.delete(docID);
}
```
注意上述FrozenBufferedUpdates.apply(IndexWriter writer)中调用apply(BufferedUpdatesStream.SegmentState[] segStates)是放在包含资源的try语句中，如下：
```
try (Closeable finalizer = () -> finishApply(writer, segStates, success.get(), delFiles)) {
  // don't hold IW monitor lock here so threads are free concurrently resolve deletes/updates:
  delCount = apply(segStates);
  success.set(true);
}
```
所以apply(BufferedUpdatesStream.SegmentState[] segStates)执行完之后会调用finishApply：
```
private void finishApply(IndexWriter writer, BufferedUpdatesStream.SegmentState[] segStates,
                           boolean success, Set<String> delFiles) throws IOException {
  synchronized (writer) {

    BufferedUpdatesStream.ApplyDeletesResult result;
    try {
      //closeSegmentStates会返回完全被删除的segment，完全被删除表示该segment里所有文档都满足query删除条件，因此已经不包含任何文件了
      result = closeSegmentStates(writer, segStates, success);
    } finally {
      // Matches the incRef we did above, but we must do the decRef after closing segment states else
      // IFD can't delete still-open files
      writer.deleter.decRef(delFiles);
    }

    if (result.anyDeletes) {
        writer.maybeMerge.set(true);
        writer.checkpoint();
    }

    if (result.allDeleted != null) {
      if (infoStream.isEnabled("IW")) {
        infoStream.message("IW", "drop 100% deleted segments: " + writer.segString(result.allDeleted));
      }
      //删除完全被删除的segment
      for (SegmentCommitInfo info : result.allDeleted) {
        writer.dropDeletedSegment(info);
      }
      //checkpoint里会调用deleter.checkpoint(segmentInfos, false)不需要的文件（完全被删除的segment关联的文件会在此时被删除
      writer.checkpoint();
    }
  }
}

/** Close segment states previously opened with openSegmentStates. */
public static BufferedUpdatesStream.ApplyDeletesResult closeSegmentStates(IndexWriter writer, BufferedUpdatesStream.SegmentState[] segStates, boolean success) throws IOException {
  List<SegmentCommitInfo> allDeleted = null;
  long totDelCount = 0;
  final List<BufferedUpdatesStream.SegmentState> segmentStates = Arrays.asList(segStates);
  //找出完全被删除的segment
  for (BufferedUpdatesStream.SegmentState segState : segmentStates) {
    if (success) {
      totDelCount += segState.rld.getDelCount() - segState.startDelCount;
      int fullDelCount = segState.rld.getDelCount();
      assert fullDelCount <= segState.rld.info.info.maxDoc() : fullDelCount + " > " + segState.rld.info.info.maxDoc();
      if (segState.rld.isFullyDeleted() && writer.getConfig().getMergePolicy().keepFullyDeletedSegment(() -> segState.reader) == false) {
        if (allDeleted == null) {
          allDeleted = new ArrayList<>();
        }
        allDeleted.add(segState.reader.getOriginalSegmentInfo());
      }
    }
  }
  IOUtils.close(segmentStates);
  if (writer.infoStream.isEnabled("BD")) {
    writer.infoStream.message("BD", "closeSegmentStates: " + totDelCount + " new deleted documents; pool " + writer.getPendingUpdatesCount()+ " packets; bytesUsed=" + writer.getReaderPoolRamBytesUsed());
  }

  return new BufferedUpdatesStream.ApplyDeletesResult(totDelCount > 0, allDeleted);
}
```
到这里，Query删除标记的删除文档已经记录在每个ReadersAndUpdates里，rld由IndexWriter使用readerPool维护。

现在回到一开始的IndexWriter.doFlush(boolean applyAllDeletes)函数，在调用processEvents(false)之后，会调用writeReaderPool(applyAllDeletes)函数：
```
//IndexWriter
private final void writeReaderPool(boolean writeDeletes) throws IOException {
  assert Thread.holdsLock(this);
  if (writeDeletes) {
    //我们重点看这里，这里会将query删除生成的liv文件写入硬盘
    if (readerPool.commit(segmentInfos)) {
      checkpointNoSIS();
    }
  } else { // only write the docValues
    if (readerPool.writeAllDocValuesUpdates()) {
      checkpoint();
    }
  }
  // now do some best effort to check if a segment is fully deleted
  List<SegmentCommitInfo> toDrop = new ArrayList<>(); // don't modify segmentInfos in-place
  for (SegmentCommitInfo info : segmentInfos) {
    ReadersAndUpdates readersAndUpdates = readerPool.get(info, false);
    if (readersAndUpdates != null) {
      if (isFullyDeleted(readersAndUpdates)) {
        toDrop.add(info);
      }
    }
  }
  for (SegmentCommitInfo info : toDrop) {
    dropDeletedSegment(info);
  }
  if (toDrop.isEmpty() == false) {
    checkpoint();
  }
}
```

```
//ReaderPool
/**
  * Commit live docs changes for the segment readers for
  * the provided infos.
  *
  * @throws IOException If there is a low-level I/O error
  */
synchronized boolean commit(SegmentInfos infos) throws IOException {
  boolean atLeastOneChange = false;
  for (SegmentCommitInfo info : infos) {
    final ReadersAndUpdates rld = readerMap.get(info);
    if (rld != null) {
      assert rld.info == info;
      //调用rld.writeLiveDocs对liv文件进行刷盘
      boolean changed = rld.writeLiveDocs(directory);
      changed |= rld.writeFieldUpdates(directory, fieldNumbers, completedDelGenSupplier.getAsLong(), infoStream);

      if (changed) {
        // Make sure we only write del docs for a live segment:
        assert assertInfoIsLive(info);

        // Must checkpoint because we just
        // created new _X_N.del and field updates files;
        // don't call IW.checkpoint because that also
        // increments SIS.version, which we do not want to
        // do here: it was done previously (after we
        // invoked BDS.applyDeletes), whereas here all we
        // did was move the state to disk:
        atLeastOneChange = true;
      }
    }
  }
  return atLeastOneChange;
}

//ReadersAndUpdates
// Commit live docs (writes new _X_N.del files) and field updates (writes new
// _X_N updates files) to the directory; returns true if it wrote any file
// and false if there were no new deletes or updates to write:
public synchronized boolean writeLiveDocs(Directory dir) throws IOException {
  return pendingDeletes.writeLiveDocs(dir);
}

//PendingDeletes
/**
  * Writes the live docs to disk and returns <code>true</code> if any new docs were written.
  */
boolean writeLiveDocs(Directory dir) throws IOException {
  if (pendingDeleteCount == 0) {
    return false;
  }

  Bits liveDocs = this.liveDocs;
  assert liveDocs != null;
  // We have new deletes
  assert liveDocs.length() == info.info.maxDoc();

  // Do this so we can delete any created files on
  // exception; this saves all codecs from having to do
  // it:
  TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);

  // We can write directly to the actual name (vs to a
  // .tmp & renaming it) because the file is not live
  // until segments file is written:
  boolean success = false;
  try {
    //这里终于看到最终编码刷盘的动作
    Codec codec = info.info.getCodec();
    codec.liveDocsFormat().writeLiveDocs(liveDocs, trackingDir, info, pendingDeleteCount, IOContext.DEFAULT);
    success = true;
  } finally {
    if (!success) {
      // Advance only the nextWriteDelGen so that a 2nd
      // attempt to write will write to a new file
      info.advanceNextWriteDelGen();

      // Delete any partially created file(s):
      for (String fileName : trackingDir.getCreatedFiles()) {
        IOUtils.deleteFilesIgnoringExceptions(dir, fileName);
      }
    }
  }

  // If we hit an exc in the line above (eg disk full)
  // then info's delGen remains pointing to the previous
  // (successfully written) del docs:
  info.advanceDelGen();
  info.setDelCount(info.getDelCount() + pendingDeleteCount);
  dropChanges();
  return true;
}
```