---
layout: post
categories: [Lucene]
description: none
keywords: Lucene
---
# Lucene源码查询分析


## 
考虑一个简单的连接查询条件："leon" and "andy"。

首先，在term-dictionary词字典中，寻找"leon"这个词对应的倒排列表Posting。假定如下：
```
leon -> [1, 2, 4, 11, 31, 174]
```
其次，在term-dictionary词字典中，寻找“andy"这个词对应的倒排列表Posting。假定如下：
```
andy => [2, 31, 101]
```
将二个Posting倒排列表进行求交集。leon-posting & andy-posting的结果：
```
intersection => [2, 31]
```
对二个倒排列表集合取交集的高效算法是合并算法。维护二个Posting倒排列表的指针，并同时遍历二个Posting列表。对于每个步骤，比较二个指针指向的docID，如果它们相同，将这个docID放入结果列表中，并将二个指针同时向前移一个位置；否则， 向前移动较小的docID的指针。

布尔查询可以处理更加复杂的查询，比如：
```
(a or b) and not c
(a and b) and c
```
Lucene search查询器模块与Lucene score评分体系密切相关。背后隐藏了很多复杂的细节。如果用户需要对搜索结果自定义一套评分机制，有必要深入理解search和scoring背后的评分机制。

## Query, Weight和Scorer对象树体系

### Lucener查询遵循如下的结构
首先，JavaCC根据定义的规则生成词法分析器和语法分析器。通过语法分析器将用户的布尔查询表达式（ (a or b) and not c) )转换成一个Query查询树。Query查询树主要职责是创建一个Weight对象树。每当我谈到Weight实例对象时，你第一时间应该想到IDF权重。

```
protected Weight createWeight(Query query) throws IOException {
    return query.weight(this);
}

public Weight weight(Searcher searcher)
{
    Query query = searcher.rewrite(this);
    Weight weight = query.createWeight(searcher);
    float sum = weight.sumOfSquaredWeights();
    float norm = getSimilarity(searcher).queryNorm(sum);
    weight.normalize(norm);
    return weight;
}
```

其次，weight对象树主要职责有二块：计算查询的权重要和建立查询对应的评分器scorer对象树。

设计weight对象树是确保search搜索不会修改query对象树，IndexSearcher查询器相关的查询状态全部驻留在weight对象树中，以便于重用query查询对象实例（此处指bq = new BooleanQuery()）。
```
//construct booleanQuery instance
BooleanQuery bq = new BooleanQuery();
            bq.add(query, BooleanClause.Occur.MUST);
            bq.add(new TermQuery(
                      new Term("mandant",
                               Integer.toString(switcher))),
                   BooleanClause.Occur.MUST);

//query1
IndexSearcher searcher1 = new IndexSearcher(directory1);
Hits hits = searcher1.search(bq);         

//query2
IndexSearcher searcher2 = new IndexSearcher(directory2);
Hits hits = searcher2.search(bq, filter);
```
最后，weight对象树会创建对应的scorer对象树，searcher查询器会创建HitCollector子类并传入scorer对象树，
```
IndexSearcher searcher3 = new IndexSearcher(directory3);
IndexReader reader = searcher.getIndexReader();
Query q = new TermQuery(new Term(TEXT_FIELD,"text")); 
Hits hits = searcher.search(q);
reader.deleteDocument(reader.maxDoc()-1);
```
前文提及IndexSearcher查询器相关的查询状态全部驻留在weight对象树中；而indexReader依赖状态全部驻留在scorer对象树中。

比如，TermScorer对应布尔查询（"leon and Andy"）,Lucene分别从索引中取出词项leon和andy的倒排索引posting，然后对这二个词项的posting求交集，最终可以获得查询返回的文档集。indexReader依赖的状态最终是通过诸如TermScorer管理posting的状态。TermScorer对象中分别定义的docs与freqs数组，用来存储某个词term对应的<docID,docFreq>倒排索引信息。
```
final class TermScorer extends Scorer {
  private Weight weight;
  private TermDocs termDocs;
  
  private final int[] docs = new int[32];         
  // buffered doc numbers
  private final int[] freqs = new int[32];       
  // buffered term freqs
  
  private int pointer;
  private int pointerMax;
```

## 三棵树的转换
如果用户需要对搜索结果自定义一套评分机制，并且获得更强的控制力，可以自定义一套query，weight和scorer对象树。

query对象树是对用户查询信息（比如，布尔逻辑表达式）的抽象。
weight对象树是对query内部统计信息（比如，TF-IDF值）的抽象。
scorer对象树提供了评分与解说评分的接口能力。

## Search without filter 流程
搜索主要流程可以概括如下几个步骤：

解析查询条件生成一棵基于Query的查询语法树
提取基于Term的原子查询
通过分词字典信息定位Term的Posting倒排索引结构
读取Posting信息进入Score用于对文档进行评分
通过collector收集目标文档集合

### workflow
Query#rewrite

在query查询树中，叶子节点大致分成二类：

第一类：termQuery表示一个精确的词。
第二类：prefixQuery或者fuzzyQuery表示一堆模糊的词。
在搜索过程中，对query查询树进行重写，这个重写过程是一个递归过程，会一直处理到叶子节点，如果有叶子节点需要重写会返回新的query对象树，否则返回老的query对象树。

为什么要重写查询？最直观简单的理解是：通过重写查询来改善查询语句的性能。当下几乎所有的数据库产品都会引入类似查询优化器提升查询效率。

对booleanQuery查询树进行递归遍历重写查询语句过程中，真正需要重写的是第二类（prefixQuery或者fuzzyQuery)，即一个query代表多个term参与查询。

### 重写过程
首先，对于一个布尔表达式“Leon and Andy”，它的查询对象BooleanQuery是一个组合对象。遍历的叶子节点属于第二类 （prefixQuery），从索引文件中将多个term取出来。比如，“lu*“，需要从倒排索引文件中取出“lucene”，“luke”，“luggage”这些term分词，所有这三个term参与到查询过程中。显然，不能使用 “lu*”参与查询过程，索引文件中并没有“lu*”分词。

其次，将取出的多个Term重新组织成新的Query对象进行查询。基本有二种实现方法。

方法1： 将多个term看成一个term，即将包含这些term的docIds全部取出来放在一起（docid-set）。
方法2：将多个term组成一个新的booleanQuery查询树，并且这些term之间是OR的连接关系。比如，prefixQuery前缀查询工作机制：它会被重写为一个BooleanQuery，包含索引中以前缀查询term词开头的所有分词。比如： “lu*”会被转换成以or条件连接的布尔表达式"lucene OR luke OR luggage"。
最后，调用BooleanQuery#rewrite本质是遍历组合中所有query子类的rewrite方法，但并不是所有的query子类都需要重写。比如termQuery就不需要重写，而对于prefixQuery前缀查询，则必须重写。

```
public Query rewrite(IndexReader reader){

  BooleanQuery clone = null;                    
  // recursively rewrite
  for (int i = 0 ; i < clauses.size(); i++) {
    BooleanClause c = (BooleanClause)clauses.get(i);
    Query query = c.getQuery().rewrite(reader);
    if (query != c.getQuery()) {   
      if (clone == null)
        clone = (BooleanQuery)this.clone();
      clone.clauses.set(i, new BooleanClause(query, c.getOccur()));
    }
  }
  if (clone != null) {
    return clone;      
  } else
    return this;     
}
```
重写后的prefixQuery前缀查询会生成多个termQuery对象，并再次组合成BooleanQuery对象。本质上，将查询转换成等价，效率更高的形式。

```
public class PrefixQuery extends Query {
  private Term prefix;
 
  public Query rewrite(IndexReader reader) {
    BooleanQuery query = new BooleanQuery(true);
    TermEnum enumerator = reader.terms(prefix);
    try {
      String prefixText = prefix.text();
      String prefixField = prefix.field();
      do {
        Term term = enumerator.term();
        if (term != null &&
            term.text().startsWith(prefixText) &&
            term.field() == prefixField)
        {
          TermQuery tq = new TermQuery(term);  
          tq.setBoost(getBoost());               
          query.add(tq, BooleanClause.Occur.SHOULD);    
        } else {
          break;
        }
      } while (enumerator.next());
    }
    return query;
  }
```

创建weight对象
查询语句BooleanQuery展开重写后，开始创建weight对象树。BooleanQuery通过递归遍历组合中每一个query对象来生成weight树。遍历的叶子节点类型是termQuery，它生成的是TermWeight对象。
```
protected Weight createWeight(Searcher searcher) {
  return new BooleanWeight(searcher);
}

public BooleanWeight(Searcher searcher){
  this.similarity = getSimilarity(searcher);
  for (int i = 0 ; i < clauses.size(); i++) {
    BooleanClause c = (BooleanClause)clauses.get(i);
    weights.add(c.getQuery().createWeight(searcher));
  }
}
```
在termWeight构造函数中，最重要的是计算idf值。termWeight作为booleanQuery的叶子节点，它代表着布尔查询表达式中的某一项。比如 name="Leon"，而idf的值用来表达“Leon” 分词在整个索引文档中的统计信息，idf值越大，代表“Leon“这一分词Term对结果文档评分影响越大。
```
public TermWeight(Searcher searcher) {
  this.similarity = getSimilarity(searcher);
  idf = similarity.idf(term, searcher); // compute idf
}

public float idf(Term term, Searcher searcher) {
  return idf(searcher.docFreq(term), searcher.maxDoc());
}

public float idf(int docFreq, int numDocs) {
  return (float)(Math.log(numDocs/(double)(docFreq+1)) + 1.0);
}
```
weight构造函数
```
public Weight weight(Searcher searcher)
  throws IOException {
  Query query = searcher.rewrite(this);
  Weight weight = query.createWeight(searcher);
  float sum = weight.sumOfSquaredWeights();
  float norm = getSimilarity(searcher).queryNorm(sum);
  weight.normalize(norm);
  return weight;
}
```
sumOfSquaredWeights
```
∑ (tf(t in d)· idf(t)²  · t.getBoost()
```
getBoost表示查询中给Term设置的boost值。比如：“Leon^2”。

TermWeight#sumOfSquaredWeights用来计算组合查询条件中每一查询子项的权重。

booleanWeight#sumOfSquaredWeights用来计算整个查询的权重。对booleanWeight树递归遍历每一个叶子节点（查询子项）并累计求和权重。

```
public float sumOfSquaredWeights() throws IOException {
      float sum = 0.0f;
      for (int i = 0 ; i < weights.size(); i++) {
        BooleanClause c = (BooleanClause)clauses.get(i);
        Weight w = (Weight)weights.elementAt(i);
        float s = w.sumOfSquaredWeights();        
        // sum sub weights
        if (!c.isProhibited())
          sum += s;
      }

      sum *= getBoost() * getBoost();             
      return sum ;
}

public float sumOfSquaredWeights() {
  queryWeight = idf * getBoost();             
  // compute query weight
  return queryWeight * queryWeight;          
}
```

normalize
DefaultSimilarity#queryNorm是一个标准化因子，用于使查询之间的分数具有可比性。这个因素不影响文件排名，而只是尝试使不同查询的分数具有可比性。
```
float norm = getSimilarity(searcher).queryNorm(sum);
weight.normalize(norm);

public class DefaultSimilarity extends Similarity {
  public float queryNorm(float sumOfSquaredWeights) {
    return (float)(1.0 / Math.sqrt(sumOfSquaredWeights));
  }
}

public void normalize(float queryNorm) {
  this.queryNorm = queryNorm;
  queryWeight *= queryNorm;                   
  value = queryWeight * idf;                  
}
```

Lucene: TF-IDF模型
```
score(q,d)  =  queryNorm(q)  · coord(q,d) · ∑ (tf(t in d)· idf(t)²  · t.getBoost() · norm(t,d)) 

```

Hits
整个search流程，大致经历如下关键步骤：

首先，执行 new Hits(this, query, filter)返回包含检索结果的集合hits. Hits类内部维护了一个LRU cache，Lucene将从多个倒排索引取出的文档集，执行逻辑and或者or运算后放入hitDocs LRU Cache中。

其次，执行searcher.search(weight, filter, n)方法从Hits类回到了IndexSearcher类中，调用collector = new TopDocCollector(nDocs)方法初始化一个优先队列。优先队列的元素是ScoreDoc类型。比如“Leon”查询条件，它找到Leon分词在索引中的倒排数据列表， scoreDoc-leon1和scoreDoc-leon2二个文档 ，如果scoreDoc-leon1.socre > scoreDoc-leon2.score，则选择让scoreDoc-leon插入队列。

基于上述构建的collector优先队列，继续调用IndexSearcher类中另一个search重载方法search(Weight,Filter,HitCollector)将检索结果放入collector队列中。最后根据collector队列的结果从ScoreDoc转换成 TopDocs对象返回。
```
public Hits search(Query query, Filter filter) {
    return new Hits(this, query, filter);
}

private final void getMoreDocs(int min) {
   searcher.search(weight, filter, n)
}

public TopDocs search(Weight weight, Filter filter,final int nDocs){
    TopDocCollector collector = new TopDocCollector(nDocs);
    search(weight, filter, collector);
    return collector.topDocs();
}
```
再次，weight.scorer(reader)生成一个 scorer对象树。比如它可以是基于“Leon and Andy”查询语句解析出ConjunctionScorerc对象树， 也可以是基于“Leon or Andy"查询语句解析出DisjunctionSumScorer对象树。经过DisjunctionSumScorer.score方法，从Lucene中获取满足条件的doc文档集，并放入优先队列collector中。

补充：关于ConjunctionScorer和DisjunctionSumScorer组合对象树从索引中检索出文档集的逻辑，后面有详述。
```
public void search(Weight weight, Filter filter,
                   final HitCollector results) {
  Scorer scorer = weight.scorer(reader);
  scorer.score(collector);
}

public void score(HitCollector hc){
  while (next()) {
    hc.collect(doc(), score());
  }
}
```

最后，我们以TermScorer原子对象为例，来说明如何从索引中检索出文档集。

比如，TermScorer对应布尔查询（"leon"）,Lucene从索引中取出词项leon的倒排索引posting，作为文档集返回。 TermScorer#next方法内部使用termDocs.read方法分别从 tis文档 与frq索引文件中，读出当前term("leon")分词出现的所有文档docid列表，并缓存在TermScorer.doc[]数组中，后续会基于文档得分写入collector优先队列中。
```
final class TermScorer extends Scorer {
  private final int[] docs = new int[32];         
  private final int[] freqs = new int[32];  
  
  public boolean next() throws IOException {
    this.pointerMax = this.termDocs.read(this.docs, this.freqs);
  }
```

## hitCollector类体系
```
public final class Hits {
  private Weight weight;
  private Searcher searcher;
  private Filter filter = null;
  private Sort sort = null;

  private int length;             
  // the total number of hits
  private Vector hitDocs = new Vector();     
  // cache of hits retrieved
  private HitDoc first;         // head of LRU cache
  private HitDoc last;          // tail of LRU cache
   
  Hits(Searcher s, Query q, Filter f) throws IOException {
    weight = q.weight(s);
    searcher = s;
    filter = f;
    nDeletions = countDeletions(s);
    getMoreDocs(50); // retrieve 100 initially
    lengthAtStart = length;
  }
```

### HitCollector基类
它有二个文档收集器，分别是TopDocCollector和TopFieldDocCollector。其中TopDocCollector根据doc-score得分进行排序，写入TopDocCollector优先队列，其中TopFieldDocCollector根据字段进行排序。

HitCollector允许用户自由扩展，通过对collect方法重写，来决定过滤收集哪些文档并返回给用户。

### TopDocCollector
```
public class TopDocCollector extends HitCollector {
  private ScoreDoc reusableSD;
}

public class ScoreDoc implements java.io.Serializable {
  public float score;
  public int doc;
}
```

### TopFieldDocCollector
```
public class TopFieldDocCollector extends TopDocCollector {
  private FieldDoc reusableFD;
}

public class FieldDoc extends ScoreDoc {
  public Comparable[] fields;
}
public FieldSortedHitQueue (IndexReader reader, SortField[] fields, int size)
throws IOException {
  final int n = fields.length;
  comparators = new ScoreDocComparator[n];
  this.fields = new SortField[n];
  for (int i=0; i<n; ++i) {
    String fieldname = fields[i].getField();
    comparators[i] = getCachedComparator (reader, fieldname, fields[i].getType(), fields[i].getLocale(), fields[i].getFactory());
    
    if (comparators[i].sortType() == SortField.STRING) {
     this.fields[i] = new SortField (fieldname, fields[i].getLocale(), fields[i].getReverse());
    } else {
     this.fields[i] = new SortField (fieldname, comparators[i].sortType(), fields[i].getReverse());
    }
  }
  initialize (size);
}
```

termWeight vs. termScorer
二者有一个显著的差别，读取Lucene索引文件的职责交由termScorer对象树。termWeight与termQuery对象树只负责管理term在index中的统计信息。

首先，我们知道term dictionary与 reverted-index倒排索引这二块的信息是独立分开存储在不同索引文件中（比如tis, frq和 pros文件中）。

其次，termScorer依赖SegmentTermDocs对象，它负责从tis文件维护的<term，posting>元组中找到查询条件中某term的posting倒排索引结构。

再次，获得了某个term的倒排索引posting结构后，就可以方便从frq与pros文件中取得docid-list与 docFreq-list列表信息。这些信息加载进termScore

```
 // buffered doc numbers
private final int[] docs = new int[32];         

// buffered term freqs
private final int[] freqs = new int[32];        
private byte[] norms;
```
最后，scorer是Lucene搜索流程中用于计算query与文档相似度的组件，它实际上并不负责文档得分的计算逻辑，这部分工作委托给了similarity组件，而scorer只负责对文档打分的补充工作。比如为文档打分提供额外的参数。

termscorer类定义的docs[]与freqs[]字段分别表示某个term分词出现在哪些文档列表中，以及在各自文档中出现的频率。

## 标准化因子文件(nrm)
为什么会有标准化因子？根据Lucene搜索流程，搜索出的文档要按照与查询语句的相关性进行排序，相关性的得分高的文档，会排列在结果集前面。

根据TF-IDF的模型，在计算term weight时，主要有二个主要影响因素：一个是此term在文档中出现的次数（TF），另一个是此term在整个Lucene索引的流行程度。显然此term在此文档中出现次数越多，此term在此文档中越重要。

TF-IDF模型非常普及，但也存在以下问题：

不同的文档重要性不同。
文档的不同域Field重要性不同。
根据term在文档中出现的绝对次数来决定term对文档的重要性，存在不合理的设计。比如在长的文档中，某个term出现绝对次数相对多，而短的文档中出现某个term的绝对次数相对少。

于上原因，Lucene在计算term weight的时候，都会考虑乘上一个标准化因子，来抵销上面问题的影响。比如，一个term出现在不同的文档的不同域field中，标准化因子设定不同。

事实上，Lucene设计了nrm文件用来包含标准化因子，表示索引创建期间收集的boost信息。每个文档的不同域field在这个nrm文件中都有对应的字节来存储boost元信息。

```
AllNorms (.nrm) --> NormsHeader,<Norms> NumFieldsWithNorms
NormsHeader --> 'N','R','M',Version

Norms --> <Byte> SegSize
Version --> Byte
```

## TermScorer#next
以termScorer为例，termScorer#next方法遍历某个term出现的所有文档id列表，取出文档ID以及文档得分并创建ScoreDoc对象表示单个搜索结果，写入hitCollector优先队列中。

```
public abstract class Scorer {
  public void score(HitCollector hc) throws IOException {
    while (next()) {
      hc.collect(doc(), score());
    }
  }
}

final class TermScorer extends Scorer {
  public float score() {
    int f = freqs[pointer];
    float raw =                                   
      // compute tf(f)*weight
      f < SCORE_CACHE_SIZE                       
      // check cache
      ? scoreCache[f]                             
      // cache hit
      : getSimilarity().tf(f)*weightValue;       
    // cache miss
    return raw * Similarity.decodeNorm(norms[doc]); 
    // normalize for field
  }
}

public void collect(int doc, float score) {
    if (score > 0.0f) {
      totalHits++;
      reusableSD = new ScoreDoc(doc, score);
      reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);
    }
  }
```

## 布尔查询
BooleanQuery对多个查询条件进行连接。连接方式有如下选择：

must (and)： 条件必须成立。
should (or)：条件可以成立。
must_not (not)： 条件必须不成立。
filter: 条件必须成立。性能比search要高，它不需要计算得分，并且引入了缓存。
在真实的场景下，多个查询条件组合的复合查询比较普遍，Lucene使用BooleanQuery来描述一个复合查询：

所有的叶子节点都是原子查询，都代表着某个term的原子查询，每个原子term查询都需要读取Lucene索引文件获取term对应的posting倒排索引表信息。所有的非叶子节点都是通过对叶子节点的posting倒排表进行各种谓词（and or not）逻辑运算来获得。BooleanQuery还可以继续嵌套BooleanQuery。

查询语句分类
查询语句的组合大致归为如下几类：

多个must的组合，比如 “+leon +Andy”，经过query->weight->scorer对象树转换，会生成ConjunctionScorer对象树，即对所有叶子节点的倒排表求交集。
must与must_not的组合，比如“+leon -Andy”，经过query->weight->scorer对象树转换，会生成ReqExclScorer(required，exclusive)对象树，即返回must的倒排表，同时删除must_not倒排表中的文档。
must与should的组合，比如“+Leon Andy”，经过query->weight->scorer对象树转换，会生成ReqOptSumScorer(required，optional)对象树，即返回must的倒排表，如果文档也出现在should的倒排表中，增加对应文档的得分。
多个should的组合，比如“Leon Andy”，经过query->weight->scorer对象树转换，会生成DisjunctionSumScorer对象树，即对所有叶子节点的倒排表求并集。
should与must_not的组合，比如“Leon -Andy”，经过query->weight->scorer对象树转换，会生成ReqExclScorer对象树，同上。

二个倒排表记录列表合并
ConjunctionScorer是一棵组合查询树，对应布尔逻辑与运算，要求将所有的term找出的倒排索引数据集postings进行取交集。

我们以二个倒排索引数据集取交集进行说明。一个显而易见的方法是二层for循环，外层循环遍历其中一个posting list1，然后内层循环对另外一个posting list2元素进行遍历。这种取交集的需要比较的次数是O( num(posting list1) * num(posting list2))。

假设，一个索引包含100W篇文档，每篇文档有1000个词，根据经验100W文档集中大概有50W数量不同的term分词，那么每个term分词对应的posting倒排列表长度为2000 （100W * 1000 / 50W )。

使用上述二层循环时间复杂度是O(2000 * 2000) = 400W。大致比较400W次才能返回检索的结果。事实上，在Web互联网中，海量数据比当前举例规模要大得多，查询比较的次数就更加多了。那么，有没有更加高效的算法呢？

上面算法忽略了一个事实，Lucene中任意term对应的docID列表在frq文件存储是以doc-id有序排列的，它的排序方法是根据全局统一的docID来对posting list排序存储。
```
//合并倒排记录表，p1, p2是根据docID排序的有序list，对p1和p2求交集（合并） 
Intersect(p1, p2)
    answer = <>  // 初始化取交集的结果
    while p1 ≠ NIL and p2 ≠ NIL  // list不为空
        // 两个posting list遇到一样的docID
        if docID(p1) == docID(p2)  
            Add(answer, docID(p1))
            p1 = p1.next
            p2 = p2.next
        else if docID(p1) < docID(p2) 
            p1 = p1.next
        else  
            p2 = p2.next
    return answer
```

BooleanScorer2
indexSearcher#serach方法中，会由weight生成 BooleanScorer2的对象树。
```
public void search(Weight weight, Filter filter,
                   final HitCollector results) {
  java
  Scorer scorer = weight.scorer(reader);
  scorer.score(collector);
}

 public Scorer scorer(IndexReader reader) {
      BooleanScorer2 result = 
        new BooleanScorer2(similarity,
                           minNrShouldMatch,
                           allowDocsOutOfOrder);

      for (int i = 0 ; i < weights.size(); i++) {
        BooleanClause c = (BooleanClause)clauses.get(i);
        Weight w = (Weight)weights.elementAt(i);
        Scorer subScorer = w.scorer(reader);
        if (subScorer != null)
          result.add(subScorer, c.isRequired(), c.isProhibited());
      return result;
    }
```
BooleanScorer2#initCountingSumScorer函数根据输入的查询查询，生成ConjunctionScorer或者DisjunctionSumScorer对象树。
```
public void score(HitCollector hc) throws IOException {
   if (countingSumScorer == null) {
      initCountingSumScorer();
    }
    while (countingSumScorer.next()) {
      hc.collect(countingSumScorer.doc(), score());
    }
}
```

上面代码基本逻辑：不断从索引中取出文档号，并且加入检索结果集中。取下一篇文档的过程，就是合并倒排列表的过程。countingSumScorer本身是一个树，合并倒排列表也是按照树的结构来尝试递归来进行，首先合并叶子节点，然后是子树与子树的合并，最后到根节点。倒排列表合并主要使用countingSumScorer有如下几种：

交集：ConjunctionScorer
并集： DisjunctionSumScorer
差集： ReqExclScorer
下面逐一分析。ConjunctionScorer，DisjunctionSumScorer和ReqExclScorer类的next方法，如上所述，取下一篇文档的过程也就是全并倒排列表的过程。


ConjunctionScorer
ConjunctionScorer类中有成员变量scorers，它是一个数组。代表查询语句树中每个叶子节点，即代表每一个原子term对应的termScorer对象。

比如，仍然是”Leon and Andy and Shawn“这个查询语句，那么ConjunctionScorer.scorers数组有三个元素。每个元素是一个termScorer对象，分别代表分词Term("Leon")，Term("Andy")和Term("Shawn")的scorer对象。前面分析过，TermScorer具备获取posting倒排表的接口。TermScorer对象中分别定义的docs与freqs数组，用来存储某个词term对应的<docID,docFreq>倒排索引信息。

ConjunctionScorer就是对这些倒排表（scorers[]）取交集，然后将交集中文档号在nextDoc函数中返回。
```
class ConjunctionScorer extends Scorer {
  private final Scorer[] scorers;

  public boolean next() throws IOException {
    return doNext();
  }

  private boolean doNext() throws IOException {
    int first=0;
    Scorer lastScorer = scorers[scorers.length-1];
    Scorer firstScorer;
    while (more && (firstScorer=scorers[first]).doc() < (lastDoc=lastScorer.doc())) {
      more = firstScorer.skipTo(lastDoc);
      lastScorer = firstScorer;
      first = (first == (scorers.length-1)) ? 0 : first+1;
    }
    return more;
  }
```
first = (first == (scorers.length-1)) ? 0 : first+1是关键代码~~

DisjunctionSumScorer
比如，仍然是”Leon and Andy and Shawn“这个查询语句，DisjunctionSumScorer.subScorers成员变量是一个链表，每个元素同样代表一个term对应的posting倒排列表。DisjunctionSumScorer对这些倒排表进行并集运算，然后将并集中的文档号在nextDoc函数中返回。
```
class DisjunctionSumScorer extends Scorer {
  /** The subscorers. */
  protected final List subScorers;
  
  public boolean next() throws IOException {
    if (scorerDocQueue == null) {
      initScorerDocQueue();
    }
    return (scorerDocQueue.size() >= minimumNrMatchers)
      && advanceAfterCurrent();
  }
  
  protected boolean advanceAfterCurrent() throws IOException {
    do { // repeat until minimum nr of matchers
      currentDoc = scorerDocQueue.topDoc();
      currentScore = scorerDocQueue.topScore();
      nrMatchers = 1;
      do { // Until all subscorers are after currentDoc
        if (! scorerDocQueue.topNextAndAdjustElsePop()) {
          if (--queueSize == 0) {
            break; // nothing more to advance, check for last match.
          }
        }
        if (scorerDocQueue.topDoc() != currentDoc) {
          break; // All remaining subscorers are after currentDoc.
        }
        currentScore += scorerDocQueue.topScore();
        nrMatchers++;
      } while (true);
      
      if (nrMatchers >= minimumNrMatchers) {
        return true;
      } else if (queueSize < minimumNrMatchers) {
        return false;
      }
    } while (true);
  }
  
}  
```

DisjunctionSumScorer的成员变量minimumNrMatchers，表示最少需要满足子条件的个数，也即在subScorer中，必须有至少minimumNrMatchers数量scorer都包含此文档号，此文档号才允许作为并集结果返回。

DisjunctionSumScorer遍历倒排表过程上，会将结果放入一个优先队列scorerDocQueue中，它是一个最小堆。它的初始化数据完全来自于subScorers[]数组。换句话说，scorerDocQueue堆顶对应的元素肯定是最小的。对应上图的理解。
```
private void initScorerDocQueue() throws IOException {
  Iterator si = subScorers.iterator();
  scorerDocQueue = new ScorerDocQueue(nrScorers);
  queueSize = 0;
  while (si.hasNext()) {
    Scorer se = (Scorer) si.next();
    if (se.next()) { // doc() method will be used in scorerDocQueue.
      if (scorerDocQueue.insert(se)) {
        queueSize++;
      }
    }
  }
}
```
当调用DisjunctionSumScorer#next方法时，间接调用advanceAfterCurrent函数。此函数主要逻辑如下：
```
public final boolean topSkipToAndAdjustElsePop(int target) {
  return checkAdjustElsePop( topHSD.scorer.skipTo(target));
}

private boolean checkAdjustElsePop(boolean cond) {
  if (cond) { // see also adjustTop
    topHSD.doc = topHSD.scorer.doc();
  } else { // see also popNoResult
    heap[1] = heap[size]; // move last to first
    heap[size] = null;
    size--;
  }
  downHeap();
  return cond;
}
```
scorerDocQueue.topDoc以及topScore取出最小堆堆顶的文档ID及文档打分值。
scorerDocQueue.topNextAndAdjustElsePop方法表示，堆顶scorer取下一篇文件（ topHSD.scorer.skipTo(target))，如果存在，则此时最小堆可能不再是最小堆了，需要通过downHeap进行调整。如果堆顶scorer的倒排表已经不存在doc文档元素了，说明堆顶已经为空了，需要弹出队列，并减少堆的大小。
当堆顶scorer的倒排表取到下一篇文档后，此时如果堆顶docID值与currentDoc不相等，说明currentDoc已经统计完毕，退出内层循环。
if (scorerDocQueue.topDoc() != currentDoc) {
break; // All remaining subscorers are after currentDoc.
}
如果统计出某个currentDoc出现的次数nrMatchers大于定义的最小需满足条件的个数，则此currentDoc就是满足条件的文档，返回给用户。
if (nrMatchers >= minimumNrMatchers) {
return true;

当currentDoc=7的时候，它累计的nrMatchers等于4，大于等于minimumNrMatchers（4），所以文档docID(7)作为结果返回给用户。

















