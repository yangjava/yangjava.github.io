---
layout: post
categories: [Python,scikitlearn]
description: none
keywords: Python
---
# 线性回归算法
逻辑回归算法的名字里虽然带有“回归”二字，但实际上逻辑回归算法是用来解决分类问题的算法。本章首先从二元分类入手，介绍了逻辑回归算法的预测函数、成本函数和梯度下降算法公式；然后再介绍了怎样由二元分类延伸到多元分类的问题；接着介绍了正则化，即通过数学的手段来解决模型过拟合问题；针对正则化，还介绍了L1范数和L2范数的含义及区别；最后用一个乳腺癌检测的实例及其模型性能优化来结束本章的内容。本章涵盖的内容如下：

- 逻辑回归算法的原理；
- 用梯度下降算法求解逻辑回归算法的模型参数；
- 正则化及正则化的作用；
- L1范数和L2范数的含义及其作为模型正则项的区别；

## 算法原理
假设有一场足球赛，我们有两支球队的所有出场球员信息、历史交锋成绩、比赛时间、主客场、裁判和天气等信息，根据这些信息预测球队的输赢。假设比赛结果记为y，赢球标记为1，输球标记为0，这个就是典型的二元分类问题，可以用逻辑回归算法来解决。

从这个例子里可以看出，逻辑回归算法的输出是个离散值，这是与线性回归算法的最大区别。

### 预测函数
需要找出一个预测函数模型，使其值输出在[0，1]之间。然后选择一个基准值，如0.5，如果算出来的预测值大于0.5，就认为其预测值为1，反之则其预测值为0。

选择


来作为预测函数，其中e是自然对数的底数。函数g（z）称为Sigmoid函数，也称为Logistic函数。以z为横坐标，以g（z）为纵坐标，画出的图形如图6-1所示。




从图中可以看出，当z=0时，g（z）=0.5。当z＞0时，g（z）＞0.5，当z越来越大时，g（z）无限接近于1。当z＜0时，g（z）＜0.5，当z越来越小时，g（z）无限接近于0。这正是我们想要的针对二元分类算法的预测函数。

问题来了，怎样把输入特征和预测函数结合起来呢？

结合线性回归函数的预测函数hθ（x）=θTx，假设令z（x）=θTx，则逻辑回归算法的预测函数如下：


下面来解读预测函数。

hθ（x）表示在输入值为x，参数为θ的前提条件下y=1的概率。用概率论的公式可以写成：


上面的概率公式可以读成：在输入x及参数θ条件下y=1的概率，这是个条件概率公式。由概率论的知识可以推导出


对二元分类法来说，这是个非黑即白的世界。