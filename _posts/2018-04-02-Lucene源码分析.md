---
layout: post
categories: Lucene
description: none
keywords: Lucene
---
# Lucene源码分析
Lucene 是一个基于 Java 的全文信息检索工具包，它不是一个完整的搜索应用程序，而是一个为应用程序提供索引和搜索功能。

## Lucene源码项目结构
在lucene-8.7.0目录中有重要的包：
- core: Lucene核心包
- analysis: Lucene分析器的包
- queryparser: Lucene查询分析器的包
- highlighter: Lucene高亮显示的包

lucene源码结构
```
├── analysis #用于文本分析
├── codecs #提供了对倒排索引结构的编码和解码的抽象，可以根据应用选择不同的实现
├── document #提供了一个简单的Document类
├── geo #地理信息
├── index #提供两个类，IndexiWriter:创建文档并将其添加到索引 IndexReader:访问索引中的数据
├── internal
├── package-info.java
├── search #提供数据结构来表示查询。提供QueryParser来从字符串或者xml生成查询结构
├── store # 定义了用于存储持久性数据结构的抽象类Directory。试图有效地使用操作系统磁盘缓冲区缓存
└── util #工具类
```

### analysis
这个包主要用于对query, document 进行解析， 将其拆解为一个个的token, 这个包并不是我们研究的重点， 一个很重要的原因是， 我们往往并不希望用Lucene默认提供的分词手段，这部分往往是离线部分做的工作，每个公司都有自己的分词方式，输入往往是已经预处理好的字段，几乎用不到Lucene提供的工具，而且这部分和其他模块的耦合程度很低。

### codecs
编码类的包，里面囊括了对各类数据的编码、解码的定义与实现，还包括一些类似于BKD Tree跳表之类的数据结构的实现。可以算是核心类了。

简单讲讲每个类都用来干嘛的, 这里面出现了大量的抽象类，也就是只有声明，没有实现，这是一种非常优秀、可拓展的设计，开发者可以自行根据需求，基于这些抽象类来实现一些满足自己需求的类。
```
├── BlockTermState.java    # 记录Term在一个Block中的状态
├── Codec.java  # 这是一个抽象类，定义索引的压缩方式//todo
├── CodecUtil.java  # 用来读取version header的类
├── CompoundFormat.java  # 抽象类，定义压缩格式
├── DocValuesConsumer.java # 抽象类，声明DocValues创建、merge接口
├── DocValuesFormat.java # 抽象类， 定义Docvalue格式//todo
├── DocValuesProducer.java # 抽象类，声明DocValue读取接口
├── FieldInfosFormat.java # 抽象类，声明FieldInfo读写接口
├── FieldsConsumer.java # 抽象类，声明写入所有Fields的写接口和merge接口
├── FieldsProducer.java # 抽象类，//todo 
├── FilterCodec.java # 抽象类， 和Codec构成一种委托者模式， 这是委托者，Codec是受托者
├── LegacyDocValuesIterables.java # 废弃
├── LiveDocsFormat.java # 抽象类，这是对于live/deleted documents 读写操作的声明， todo
├── MultiLevelSkipListReader.java 跳表读取类
├── MultiLevelSkipListWriter.java 跳表写入类
├── MutablePointValues.java # 抽象类， 定义不可变的PointValues类型
├── NormsConsumer.java # 抽象类，声明写Norms信息的方法
├── NormsFormat.java # 抽象类， 定义Norm格式
├── NormsProducer.java # 抽象类，声明NormValue读取接口
├── PointsFormat.java # 抽象类， 声明Points格式定义
├── PointsReader.java  # 抽象类， 声明PointsValue读取方法
├── PointsWriter.java  #抽象类， 声明PointsValue写入方法
├── PostingsFormat.java # 抽象类，声明倒排格式
├── PostingsReaderBase.java # 抽象类， 声明Posting倒排表读取方法
├── PostingsWriterBase.java # 抽象类， 声明Posting倒排表写入方法
├── PushPostingsWriterBase.java  # 相比上面那种多了PushAPI, 是一种SAX API, 上面是DOM API
├── SegmentInfoFormat.java  # 抽象类，声明segmentInfo格式
├── StoredFieldsFormat.java # 抽象类， 声明StoredField格式
├── StoredFieldsReader.java # 抽象类，声明读取StoredField相关方法
├── StoredFieldsWriter.java # 抽象类，声明写入StoredField相关方法
├── TermStats.java  # 数据类，用于记录docFreq和termFreq
├── TermVectorsFormat.java # 抽象类， 声明TermVector相关方法
├── TermVectorsReader.java # 抽象类， 声明读取TermVector相关方法
├── TermVectorsWriter.java #抽象类， 声明写入TermVector相关方法
├── blocktree # TermDict相关编码都在这个目录下
├── compressing # StoredField和TermVector相关的抽象类的最终实现都在这里，一些压缩算法也在这里
├── lucene50 # PostingReader, PostingWriter， SkipReader, SkipWriter最终实现在这里， 
├── lucene60  # PointsReader, PointsWriter，PointsFormat最终实现在这里
├── lucene62 # SegmentInfoFormat最终实现在这里
├── lucene70 # DocValueWriter, DocValueReader， NormsConsumer, NormsProducer最终实现在这里
├── package-info.java
└── perfield # 支持单个Field格式的实现
```
实际上可以大致可以分为两个维度来看：
- 第一个维度是数据
其实整个Lucene把需要处理的数据分为这么几类： 
- PostingList 倒排表，也就是`term->[doc1， doc3, doc5]`这种倒排索引数据 
- BlockTree, 从term和PostingList的映射关系，这种映射一般都用FST这种数据结构来表示，这种数据结构其实是一种树形结构，类似于Tier树，所以Lucene这里就叫BlockTree， 其实我更习惯叫它TermDict。 
- StoredField 存进去的原始信息； 
- DocValue 键值数据，这种数据主要是用来加速对字段的排序、筛选的. 
- TermVector，词向量信息，主要记一个不同term的全局出现频率等信息。 
- Norms，用来存储Normalisation信息， 比如给某些field加权之类的。 
- PointValue 用来加速 range Query的信息。

第二个维度是行为, 也就是定义数据的Writer， Reader， Format本质上就是用于唤起Writer和Reader的一种媒介。

### document
这个包主要是对一些数据类型做一些定义， 比如Field，Docuemnt， Point， DocValue等，以及它们和Int Float String之类基本类型的排列组合。

### geo
关于地理信息的一些实用类

### index
这里面的类非常丰富，也是我认为Lucene非常非常核心的包，包里的文件太多了，我就在这里不一一细讲了

可以分这几个大类来看：
- Reader相关，具体来说包括IndexReader、LeafReader CompositeReader 等等
- DocValue相关，也就是SortedDocValues, SortedNumericDocValues, NumericDocValues, BinaryDocValues，这些需要跟TermsEnum, DocValuesWriter一起看会更清晰一些。
- MergePolicy相关，也就是段合并策略，这些类定义了何时合并？怎么合并？合并的大小等等细节。
- TermsEnum 相关，其实就是定义了一个terms集合类，按照字段顺序放了terms
- IndexDeletionPolicy相关， index删除策略。
- TermsHash相关, 这个类的主要作用是承担TermVectorConsumer,FreqProxTermsWriter的基类
- DocValuesWriter, 这个应该很熟悉了，下辖SortedDocValueWriters, BinaryDocValuesWriters, SortedSetDocValuesWriters, SortedNumericDocValuesWriters, NumericDocValuesWriters
- MergeScheduler相关，定义了段合并的细节，默认采用子类ConcurrentMergeScheduler，即多线程合并类，SerialMergeScheduler基本用不上，即串行去合并分段
- DocValuesFieldUpdates,保存一个段内，所有文档的DocValue的更新信息。
另外还有一些必要的结构体信息，比如SegmentInfo， WriterState等状态信息类，以及Term，DocWriter, IndexWriter等核心类的定义，这些以后都会涉及到

### Search
和检索相关的类都在这里 
- 有一大类都是Query类的实现，Lucene面向开发者提供了各种Query，比如最简单的MatchQuery, 可以做布尔检索的BoolQuery， 支持同义词的SynoymQuery等等，这些后面会介绍 
- 另外一大类是Scorer也就是算分类， Lucene返回的排序顺序是根据算出的得分取TopN来得出的，很多Scorer都有对应的Query场景， 比如PhraseScorer就是在PhraseQuery场景下使用的。 
- Collector相关， 提供了多种收集最终结果的Collector实现， 最常用的是TotalHitCountCollector 
- 其他就是支撑以上这些类的一些工具类，比如DocIDSet,HitQueue等。 
- 有两个文件夹可以注意一下，一个是similarities，提供了一些算相似度的算法包，还有span，用于构建一些高级查询，后续再说。

### Store
和最终落磁盘和读磁盘的一些类都在这里。
我认为Lucene设计的最优秀的一点就在于，它把落盘的逻辑和生成落盘数据的两个行为分开来，提供了DataInput 和DataOutput与磁盘Directory的抽象。 
- DataInput, 提供的是对数据读取的抽象，定义了如何读取vint？如何读取vlong?vfloat等。 
- DataOutput, 提供的是对数据写入的抽象，定义了如果写入vint？如何写入vlong?等等 
- Directory， 提供以何种方式写入数据的方法，比如simple, NIOFSD， mmap等。

### utils
一些编码的实现和算法的实现都在这个库里面了，具体包括: 
- automation, 状态机的实现， 其实是实现了一个正则，支持正则查询必不可少的东西。 
- bkd ，bkd树的实现，为了加速range query用的 
- fst, fst的实现， fst其实目的就是为了提供term->id 的映射，但其拥有检索速度高、内存消耗少、支持前缀查询等优势，后面会说 
- graph， 为automation 提供辅助用的。 
- mutable， 不可变值的实现 
- packed, 提供编码整数的若干种方法，比如可以两个字节一编、 四个字节一编等等，后续会提。 

## lucene源码实例
下面先看一段常见的lucene建立索引和进行搜索的实例
```java
package com.lucene;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.TextField;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;

import java.nio.file.Paths;

public class LuceneDemo {

    /**
     * 建立索引实例
     *
     * @throws Exception
     */
    public void createIndex() throws Exception {
        // 索引存储位置
        String path = "d:/news";
        //要索引的文本
        String text = "This is the text to be indexed .";
        // 创建标准分词器
        Analyzer analyzer = new StandardAnalyzer();
        // 索引存储在硬盘目录中
        Directory directory = FSDirectory.open(Paths.get(path));
        // 索引写入配置
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        config.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
        IndexWriter indexWriter = new IndexWriter(directory, config);
        Document doc = new Document();
        doc.add(new Field("title", text, TextField.TYPE_STORED));
        indexWriter.addDocument(doc);
        indexWriter.close();
        directory.close();
    }

    /**
     * 搜索实例
     *
     * @throws Exception
     */
    public void search() throws Exception {
        String path = "d:/news";
        IndexReader reader = DirectoryReader.open(FSDirectory.open(Paths.get(path)));
        IndexSearcher searcher = new IndexSearcher(reader);
        ScoreDoc[] hits = null;
        Analyzer analyzer = new StandardAnalyzer();
        Query query = new QueryParser("title", analyzer).parse("text");
        TopDocs results = searcher.search(query, 10);
        hits = results.scoreDocs;
        Document document = null;
        for (int i = 0; i < hits.length; i++) {
            document = searcher.doc(hits[i].doc);
            String title = document.get("title");
            System.out.println(title);
        }
        reader.close();

    }

    public static void main(String[] args) throws Exception {
        new LuceneDemo().createIndex();
        new LuceneDemo().search();
    }


}

```








































