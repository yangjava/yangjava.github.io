---
layout: post
categories: Lucene
description: none
keywords: Lucene
---
# Lucene源码分析
Lucene 是一个基于 Java 的全文信息检索工具包，它不是一个完整的搜索应用程序，而是一个为应用程序提供索引和搜索功能。

## Lucene源码项目结构
在lucene-8.7.0目录中有重要的包：
- core: Lucene核心包
- analysis: Lucene分析器的包
- queryparser: Lucene查询分析器的包
- highlighter: Lucene高亮显示的包

lucene源码结构
```
├── analysis #用于文本分析
├── codecs #提供了对倒排索引结构的编码和解码的抽象，可以根据应用选择不同的实现
├── document #提供了一个简单的Document类
├── geo #地理信息
├── index #提供两个类，IndexiWriter:创建文档并将其添加到索引 IndexReader:访问索引中的数据
├── internal
├── package-info.java
├── search #提供数据结构来表示查询。提供QueryParser来从字符串或者xml生成查询结构
├── store # 定义了用于存储持久性数据结构的抽象类Directory。试图有效地使用操作系统磁盘缓冲区缓存
└── util #工具类
```

### analysis
这个包主要用于对query, document 进行解析， 将其拆解为一个个的token, 这个包并不是我们研究的重点， 一个很重要的原因是， 我们往往并不希望用Lucene默认提供的分词手段，这部分往往是离线部分做的工作，每个公司都有自己的分词方式，输入往往是已经预处理好的字段，几乎用不到Lucene提供的工具，而且这部分和其他模块的耦合程度很低。

### codecs
编码类的包，里面囊括了对各类数据的编码、解码的定义与实现，还包括一些类似于BKD Tree跳表之类的数据结构的实现。可以算是核心类了。

简单讲讲每个类都用来干嘛的, 这里面出现了大量的抽象类，也就是只有声明，没有实现，这是一种非常优秀、可拓展的设计，开发者可以自行根据需求，基于这些抽象类来实现一些满足自己需求的类。
```
├── BlockTermState.java    # 记录Term在一个Block中的状态
├── Codec.java  # 这是一个抽象类，定义索引的压缩方式//todo
├── CodecUtil.java  # 用来读取version header的类
├── CompoundFormat.java  # 抽象类，定义压缩格式
├── DocValuesConsumer.java # 抽象类，声明DocValues创建、merge接口
├── DocValuesFormat.java # 抽象类， 定义Docvalue格式//todo
├── DocValuesProducer.java # 抽象类，声明DocValue读取接口
├── FieldInfosFormat.java # 抽象类，声明FieldInfo读写接口
├── FieldsConsumer.java # 抽象类，声明写入所有Fields的写接口和merge接口
├── FieldsProducer.java # 抽象类，//todo 
├── FilterCodec.java # 抽象类， 和Codec构成一种委托者模式， 这是委托者，Codec是受托者
├── LegacyDocValuesIterables.java # 废弃
├── LiveDocsFormat.java # 抽象类，这是对于live/deleted documents 读写操作的声明， todo
├── MultiLevelSkipListReader.java 跳表读取类
├── MultiLevelSkipListWriter.java 跳表写入类
├── MutablePointValues.java # 抽象类， 定义不可变的PointValues类型
├── NormsConsumer.java # 抽象类，声明写Norms信息的方法
├── NormsFormat.java # 抽象类， 定义Norm格式
├── NormsProducer.java # 抽象类，声明NormValue读取接口
├── PointsFormat.java # 抽象类， 声明Points格式定义
├── PointsReader.java  # 抽象类， 声明PointsValue读取方法
├── PointsWriter.java  #抽象类， 声明PointsValue写入方法
├── PostingsFormat.java # 抽象类，声明倒排格式
├── PostingsReaderBase.java # 抽象类， 声明Posting倒排表读取方法
├── PostingsWriterBase.java # 抽象类， 声明Posting倒排表写入方法
├── PushPostingsWriterBase.java  # 相比上面那种多了PushAPI, 是一种SAX API, 上面是DOM API
├── SegmentInfoFormat.java  # 抽象类，声明segmentInfo格式
├── StoredFieldsFormat.java # 抽象类， 声明StoredField格式
├── StoredFieldsReader.java # 抽象类，声明读取StoredField相关方法
├── StoredFieldsWriter.java # 抽象类，声明写入StoredField相关方法
├── TermStats.java  # 数据类，用于记录docFreq和termFreq
├── TermVectorsFormat.java # 抽象类， 声明TermVector相关方法
├── TermVectorsReader.java # 抽象类， 声明读取TermVector相关方法
├── TermVectorsWriter.java #抽象类， 声明写入TermVector相关方法
├── blocktree # TermDict相关编码都在这个目录下
├── compressing # StoredField和TermVector相关的抽象类的最终实现都在这里，一些压缩算法也在这里
├── lucene50 # PostingReader, PostingWriter， SkipReader, SkipWriter最终实现在这里， 
├── lucene60  # PointsReader, PointsWriter，PointsFormat最终实现在这里
├── lucene62 # SegmentInfoFormat最终实现在这里
├── lucene70 # DocValueWriter, DocValueReader， NormsConsumer, NormsProducer最终实现在这里
├── package-info.java
└── perfield # 支持单个Field格式的实现
```
实际上可以大致可以分为两个维度来看：
- 第一个维度是数据
其实整个Lucene把需要处理的数据分为这么几类： 
- PostingList 倒排表，也就是`term->[doc1， doc3, doc5]`这种倒排索引数据 
- BlockTree, 从term和PostingList的映射关系，这种映射一般都用FST这种数据结构来表示，这种数据结构其实是一种树形结构，类似于Tier树，所以Lucene这里就叫BlockTree， 其实我更习惯叫它TermDict。 
- StoredField 存进去的原始信息； 
- DocValue 键值数据，这种数据主要是用来加速对字段的排序、筛选的. 
- TermVector，词向量信息，主要记一个不同term的全局出现频率等信息。 
- Norms，用来存储Normalisation信息， 比如给某些field加权之类的。 
- PointValue 用来加速 range Query的信息。

第二个维度是行为, 也就是定义数据的Writer， Reader， Format本质上就是用于唤起Writer和Reader的一种媒介。

### document
这个包主要是对一些数据类型做一些定义， 比如Field，Docuemnt， Point， DocValue等，以及它们和Int Float String之类基本类型的排列组合。

### geo
关于地理信息的一些实用类

### index
这里面的类非常丰富，也是我认为Lucene非常非常核心的包，包里的文件太多了，我就在这里不一一细讲了

可以分这几个大类来看：
- Reader相关，具体来说包括IndexReader、LeafReader CompositeReader 等等
- DocValue相关，也就是SortedDocValues, SortedNumericDocValues, NumericDocValues, BinaryDocValues，这些需要跟TermsEnum, DocValuesWriter一起看会更清晰一些。
- MergePolicy相关，也就是段合并策略，这些类定义了何时合并？怎么合并？合并的大小等等细节。
- TermsEnum 相关，其实就是定义了一个terms集合类，按照字段顺序放了terms
- IndexDeletionPolicy相关， index删除策略。
- TermsHash相关, 这个类的主要作用是承担TermVectorConsumer,FreqProxTermsWriter的基类
- DocValuesWriter, 这个应该很熟悉了，下辖SortedDocValueWriters, BinaryDocValuesWriters, SortedSetDocValuesWriters, SortedNumericDocValuesWriters, NumericDocValuesWriters
- MergeScheduler相关，定义了段合并的细节，默认采用子类ConcurrentMergeScheduler，即多线程合并类，SerialMergeScheduler基本用不上，即串行去合并分段
- DocValuesFieldUpdates,保存一个段内，所有文档的DocValue的更新信息。
另外还有一些必要的结构体信息，比如SegmentInfo， WriterState等状态信息类，以及Term，DocWriter, IndexWriter等核心类的定义，这些以后都会涉及到

### Search
和检索相关的类都在这里 
- 有一大类都是Query类的实现，Lucene面向开发者提供了各种Query，比如最简单的MatchQuery, 可以做布尔检索的BoolQuery， 支持同义词的SynoymQuery等等，这些后面会介绍 
- 另外一大类是Scorer也就是算分类， Lucene返回的排序顺序是根据算出的得分取TopN来得出的，很多Scorer都有对应的Query场景， 比如PhraseScorer就是在PhraseQuery场景下使用的。 
- Collector相关， 提供了多种收集最终结果的Collector实现， 最常用的是TotalHitCountCollector 
- 其他就是支撑以上这些类的一些工具类，比如DocIDSet,HitQueue等。 
- 有两个文件夹可以注意一下，一个是similarities，提供了一些算相似度的算法包，还有span，用于构建一些高级查询，后续再说。

### Store
和最终落磁盘和读磁盘的一些类都在这里。
我认为Lucene设计的最优秀的一点就在于，它把落盘的逻辑和生成落盘数据的两个行为分开来，提供了DataInput 和DataOutput与磁盘Directory的抽象。 
- DataInput, 提供的是对数据读取的抽象，定义了如何读取vint？如何读取vlong?vfloat等。 
- DataOutput, 提供的是对数据写入的抽象，定义了如果写入vint？如何写入vlong?等等 
- Directory， 提供以何种方式写入数据的方法，比如simple, NIOFSD， mmap等。

### utils
一些编码的实现和算法的实现都在这个库里面了，具体包括: 
- automation, 状态机的实现， 其实是实现了一个正则，支持正则查询必不可少的东西。 
- bkd ，bkd树的实现，为了加速range query用的 
- fst, fst的实现， fst其实目的就是为了提供term->id 的映射，但其拥有检索速度高、内存消耗少、支持前缀查询等优势，后面会说 
- graph， 为automation 提供辅助用的。 
- mutable， 不可变值的实现 
- packed, 提供编码整数的若干种方法，比如可以两个字节一编、 四个字节一编等等，后续会提。 

## lucene源码实例
下面先看一段常见的lucene建立索引和进行搜索的实例
```java
package com.lucene;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.TextField;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;

import java.nio.file.Paths;

public class LuceneDemo {

    /**
     * 建立索引实例
     *
     * @throws Exception
     */
    public void createIndex() throws Exception {
        // 索引存储位置
        String path = "d:/news";
        //要索引的文本
        String text = "This is the text to be indexed .";
        // 创建标准分词器
        Analyzer analyzer = new StandardAnalyzer();
        // 索引存储在硬盘目录中
        Directory directory = FSDirectory.open(Paths.get(path));
        // 索引写入配置
        IndexWriterConfig config = new IndexWriterConfig(analyzer);
        config.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
        IndexWriter indexWriter = new IndexWriter(directory, config);
        Document doc = new Document();
        doc.add(new Field("title", text, TextField.TYPE_STORED));
        indexWriter.addDocument(doc);
        indexWriter.close();
        directory.close();
    }

    /**
     * 搜索实例
     *
     * @throws Exception
     */
    public void search() throws Exception {
        String path = "d:/news";
        IndexReader reader = DirectoryReader.open(FSDirectory.open(Paths.get(path)));
        IndexSearcher searcher = new IndexSearcher(reader);
        ScoreDoc[] hits = null;
        Analyzer analyzer = new StandardAnalyzer();
        Query query = new QueryParser("title", analyzer).parse("text");
        TopDocs results = searcher.search(query, 10);
        hits = results.scoreDocs;
        Document document = null;
        for (int i = 0; i < hits.length; i++) {
            document = searcher.doc(hits[i].doc);
            String title = document.get("title");
            System.out.println(title);
        }
        reader.close();

    }

    public static void main(String[] args) throws Exception {
        new LuceneDemo().createIndex();
        new LuceneDemo().search();
    }


}

```

要深入理解lucene，首先得理解两个大的基础，一是索引的基础，即索引文件格式（当然还包括索引的）；二是搜索的基础，即搜索算法评分原则。

我们先来回顾一下lucene发展至今的一些能力：
- 索引和搜索能力。
从lucene的各个版本的更新历史来看，lucene的索引和搜索速度从1.0到现在有了多次的提升，比如1.4.1，2.2，2.3，2.4，内存和磁盘占用更小。

官方的数据是：在现代的计算机上索引速度可以到150G/小时，并且只需要1M内存（这个还得看怎么用的吧），由于使用了压缩技术，索引大小仅为文本的20%到30%

- 多种索引存放方式，增量索引，近实时搜索。
增量索引使得索引在第一次建好之后需要维护索引时可以基于第一次的基础维护，而不是每次都只能重建，增量索引可以大大的减少维护成本。近实时搜索是lucene2.9引入的一项重要功能，即文档的即时索引和即时搜索。在2.9以前，建立索引到提交(commit)索引之间有可能会有很长时间的间隔，这段间隔的内容是无法被索引到的，而2.9以后能够在查询的时候实时刷新缓冲区中新增或者删除的文档。

- 过滤，排序，高亮，丰富的查询类型可以处理各种查询case。
排序是实际业务中一项重要的能力，1.4以后开始提供这项能力，默认情况下，lucene通过关联性评分对匹配文档进行降序排序，关联性最好的在最前面，但是可以根据需要来定制类似淘宝的商品排序能力。另外，lucene提供phrase queries, wildcard queries, proximity queries, range queries等等来处理各种查询的情况。

- 跨度查询等技术。 跨度查询提供的是位置感知能力。
比如一段文本中单查询奥巴马和叙利亚，可能查到的是奥巴马演讲，叙利亚局势，是两个不相干的信息。但是如果查询所有靠近奥巴马并且包含叙利亚的文本，可能查出来的就是奥巴马向叙利亚宣战。这个能力可以让我们的查询更加精确。

- 灵活的评分机制。
4.0以前，评分算法都是空间向量模型，难于扩展，而且没有统计到Field。4.0以后，增加了灵活的评分机制，可以支持各种类型的评分模型，比如，基于概率的BM-25，语言模型，随机分歧，基于信息量的模型。根据不同的需要可以定制。

- 利用codec解耦lucene的索引与架构。
解耦的好处不言而喻，可以让lucene支持更多的索引，使得适用范围变大，适应未来的需要，同时也可以自定义索引格式。 可以看出来，以上两个方式都是用类似插件的机制来提高灵活性。

- 多线程并发创建索引。 
4.0以前，虽然可以做到多个线程建索引，但是当索引需要写回硬盘时则需要一个一个顺序flush，而4.0以后，已经可以做到并发写回了。

- 基于有限自动机的模糊匹配算法（FSA算法）
如图，fst匹配了mop, moth, pop, star, stop和top，对于模糊搜索而言，FST就相当于一个SortedMap，但是相对而言，FST占用更少内存，而sortedMap则是重复存了这些字母，而FST的cpu花销相对较大。对于lucene而言，内存利用甚至超过了CPU，因而采用FST是一个比较明智的选择。更多内容请参考文末文档。

## 索引和搜索组件
- Field
域是lucene索引中最细粒度的单位，lucene的对象非常形象化，一篇现实的文档可能会分许多个部分，比如：标题，作者，内容，都是不同的域，如果用数据库打比方，那么Field就是列。

- Document
文档是加入索引的单位，一篇文档由多个域组成，可以把Document理解为一篇虚拟的文档，用数据库来类比，一个Document就是一条记录。

- Directory
Directory是抽象的索引存放位置，有许多子类实现，比如，FSDirectory是最常用的索引放入磁盘，RAMDirectory将索引放到内存，FileSwitchDirectory则是根据文件扩展名在两个文件目录之间切换（试验阶段）等。其中FSDirectory.open会根据不同的平台返回它的不同子类，典型的是solaris和window64-bit会返回MMapDirectory，非window会返回NIOFSDirectory，在windows 32-bit则会使用SimpleDirectory。

- Analyzer
分析器的主要作用是从文本中提取出待使用的语汇单元，并且处理一些无用的信息。提取语汇单元的过程还包括一些额外的处理，比如丢掉停用词（stop words），或者把这些语汇变成小写从而在其后的搜索中做到忽略大小写，又或者从一个单词提取出词干，比如drove，driven在被处理为drive这样搜索drive时就能够同时搜索到这些变化后的词，当然，还有一个比较重要的作用，就是我们的中文分词。而且分析器不仅在索引过程中会用到，同时还会在搜索过程，可以想象，索引过程要将文档提取出语汇单元并索引，而搜索过程又要将待搜索查询分解为语汇单元。所以，分析器非常重要，甚至关系到了搜索质量的好坏。

- IndexWriter
IndexWriter是整个索引过程最为重要的角色和入口，它的许多方法都非常值得推敲和斟酌。通过它，可以将索引事实上写入磁盘。由于它如此重要，在后面会专门撰文对其进行分析。现在你只要知道，如果索引是一个数据库，那么IndexWriter就是对索引进行增删查改的最核心组件了。

- IndexReader
IndexReader是一个相对比较“重”的类，之所以这么说，是因为IndexReader负责打开索引，提供底层API等工作，这些工作相对耗时，所以无论是多线程调优还是各种建议都号召尽量复用IndexReader，减少不必要的性能损失。

- QueryParser
QueryParser被用来解析用户输入的查询表达式，lucene经过这么久的进化，已经有一套类似google的查询表达式语言，QueryParser就是负责解析这套语言的。

上图中QueryParser将用户输入的查询表达式解析过后，将中间的待查找文本送到Analyzer并得到语汇结果，最后将结果包装成查询对象并送往IndexSearcher。

- Query
Query就是QueryParser解析过后的结果对象，如果不通过QueryParser来解析得到Query，则可以自己选择使用各种多样化的Query子类，比如BooleanQuery, PrefixQuery, NumericRangeQuery, SpanQuery等等。

- IndexSearcher
IndexSearcher是与IndexWriter遥相呼应的一个重要的类，方法非常精炼实用，为我们查询提供了可能会用到的API。IndexSearcher的实际工作过程非常“简单”，从上图也可以看出来，它有两方面的输入，一边是索引词库，一边是查询对象。通过一系列的评分算法计算之后，查找索引库中与待查对象关联度较高的文档。如果不仔细考虑，可能会奇怪为什么需要评分算法，实际上，查询结果中大多数都包含相同的待查询关键字，因此很难比较都包含待查关键字的N篇文档从而得出查询结果的排列位置，再加上待查询的基数过于巨大，使得查询结果的排列变得非常重要，越靠前的位置越发珍贵。靠后的结果评分往往较低，并且查找效率不高，所以许多搜索引擎都不提供深度搜索，比如google就不提供查询数量在1000以后的结果。究其原因，其实就是求topK与排序的区别，在结果集数量就较高的情况下（假设为1000000），极端情况下，前1000个结果（即top1000）与top1000000（即整个排序）是有巨大差别的，topK比较好的时间复杂度可以到NlogK，而排序的时间复杂度是NlogN，由于K往往只是N中很小一部分，所以差异是比较显著的。在lucene中，分页的查询速度就可以明显看出来区别。一般靠前的页面查询较快，靠后的查询较慢。

- TopDocs
TopDocs是一个前N个搜索结果的集合，每个结果用一个docId来标识，TopDocs可以得到几个重要的查询结果，比如，总的查询数量，查询结果对象数组以及最大评分。




































