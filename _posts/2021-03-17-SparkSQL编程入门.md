---
layout: post
categories: [Spark]
description: none
keywords: Spark
---
# SparkSQL编程入门


## Spark SQL是什么
Spark SQL是用于结构化数据、半结构化数据处理的Spark高级模块，可用于从各种结构化数据源，例如JSON（半结构化）文件、CSV文件、ORC文件（ORC文件格式是一种Hive的文件存储格式，可以提高Hive表的读、写以及处理数据的性能）、Hive表、Parquest文件（新型列式存储格式，具有降低查询成本、高效压缩等优点，广泛用于大数据存储、分析领域）中读取数据，然后在Spark程序内通过SQL语句对数据进行交互式查询，进而实现数据分析需求，也可通过标准数据库连接器（JDBC/ODBC）连接传统关系型数据库，取出并转化关系数据库表，利用Spark SQL进行数据分析。

这里解释一下结构化数据：结构化数据是指记录内容具有明确的结构信息且数据集内的每一条记录都符合结构规范的数据集合，是由二维表结构来逻辑表达和实现的数据集合。可以类比传统数据库表来理解该定义，所谓的“明确结构”即是由预定义的表头（Schema）表示的每一条记录由哪些字段组成以及各个字段的名称、类型、属性等信息。

## Spark SQL通过什么来实现
若需处理的数据集是典型结构化数据源，可在Spark程序中引入Spark SQL模块，首先读取待处理数据并将其转化为Spark SQL的核心数据抽象——DataFrame，进而调用DataFrame API来对数据进行分析处理，也可以将DataFrame注册成表，直接使用SQL语句在数据表上进行交互式查询。

当计算结果时，Spark底层会使用相同的执行引擎，独立于用来表达计算的API/编程语言（目前Spark SQL主要支持Scala、Python、Java、R），所以开发者可以选择Scala、Python、Java、R中较自己更顺手的编程语言进行Spark SQL学习、开发。

另外，相比于RDD，Spark SQL模块的数据抽象（DataFrame）不仅提供了更加丰富的算子操作，还清楚地知道该数据集包含哪些列，每一列数据的名称、类型，并将这些结构信息（Schema）运用在底层计算、存储和优化中，从而在程序员并没有显式调优的情况下，Spark SQL模块也会自动根据DataFrame提供的结构信息来减少数据读取、提升执行效率以及对执行计划进行优化。除了Spark SQL模块内部自动对计算过程进行丰富、智能地调优外，我们也可以通过手动设置诸多Spark应用运行时的参数来更好地配合Spark集群cpu、内存可用资源以及业务需求等实际情况，进而提升Spark应用的执行效率以及整个Spark集群的健康有效地运行。在Spark内部，Spark SQL能够用于做优化的信息较RDD更多。

## Spark SQL处理数据的优势
Spark SQL是Spark用来操作结构化数据的程序包，在程序中通过引入Spark SQL模块，我们便可以像从前在关系型数据库利用SQL（结构化查询语言）分析关系型数据库表一样简单快捷地在Spark大数据分析平台上对海量结构化数据进行快速分析，而Spark平台屏蔽了底层分布式存储、计算、通信的细节以及作业解析、调度的细节，使我们开发者仅需关注如何利用SQL进行数据分析的程序逻辑即可。

Spark SQL除了为Spark提供了一个SQL接口，还支持开发者将SQL和传统的RDD编程的数据操作方式相结合，不论使用Python、Java还是Scala，开发者都可以在单个应用中同时使用SQL和复杂的数据分析。

另外，Spark SQL与Spark平台上其他高级模块如Spark Streaming（实时计算）、MLlib（机器学习）、GraphX（图计算）有紧密结合的特点，这就意味着Spark SQL可以在任何不同种需求的大数据应用中扮演处理中间结构化数据的角色，从而成为了Spark大数据开发中结构化数据处理领域必不可少的工具。

## Spark SQL数据核心抽象(DataFrame)

### DataFrame概念
DataFrame的定义与RDD类似，即都是Spark平台用以分布式并行计算的不可变分布式数据集合。与RDD最大的不同在于，RDD仅仅是一条条数据的集合，并不了解每一条数据的内容是怎样的，而DataFrame明确的了解每一条数据有几个命名字段组成，即可以形象地理解为RDD是一条条数据组成的一维表，而DataFrame是每一行数据都有共同清晰的列划分的二维表。概念上来说，它和关系型数据库的表或者R和Python中data frame等价，只不过DataFrame在底层实现了更多优化。

从编程角度来说，DataFrame是Spark SQL模块所需处理的结构化数据的核心抽象，即在Spark程序中若想要使用简易的SQL接口对数据进行分析，首先需要将所处理数据源转化为DataFrame对象，进而在DataFrame对象上调用各种API来实现需求。

DataFrame可以从许多结构化数据源加载并构造得到，如：结构化数据文件，Hive中的表，外部数据库，已有的DataFrame API支持多种高级程序语言Scala、Java、Python和R。

另外值得注意的是，正如官方文档所提示的那样，在Java和Scala中，DataFrame其实就是DataSet[Row]，即由表示每一行内容的Row对象组成的DataSet对象，因此大家如果想去官方API手册查询DataFrame丰富的API时，应该在DataSet类下查找。

## RDD和DataFrame的区别
RDD和DataFrame均为Spark平台对数据的一种抽象，一种组织方式，但是两者的地位或者说设计目的却截然不同。RDD是整个Spark平台的存储、计算以及任务调度的逻辑基础，更具有通用性，适用于各类数据源，而DataFrame是只针对结构化数据源的高层数据抽象，其中在DataFrame对象的创建过程中必须指定数据集的结构信息（Schema），所以DataFrame生来便是具有专用性的数据抽象，只能读取具有鲜明结构的数据集。

```
         RDD             DataFrame
+---------------+      +---------------+ 
| +-----------+ |      | +-----------+ | 
| |   Person  | |      | | Name |Age | |
| +-----------+ |      | +-----------+ |
| |  Person   | |      | |String|Int | |
| +-----------+ |      | +-----------+ |
+---------------+      +---------------+
```
如上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person类为类型参数，但Spark平台本身并不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合，DataFrame则是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子操作以外，更重要的特点是利用已知的结构信息来提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。

正是由于RDD并不像DataFrame提供详尽的结构信息，所以RDD提供的API功能上并没有像DataFrame强大丰富且自带优化，所以又称为Low-level API，相比之下，DataFrame被称为high-level的抽象，其提供的API类似于SQL这种特定领域的语言（DSL）来操作数据集。

## RDD、DataFrame使用场景
RDD是Spark的数据核心抽象，DataFrame是Spark四大高级模块之一Spark SQL所处理数据的核心抽象，基于上一章对RDD以及RDD提供的API的学习，我们了解到，所谓的数据抽象，就是当为了解决某一类数据分析问题时，根据问题所涉及的数据结构特点以及分析需求在逻辑上总结出的典型、普适该领域数据的一种抽象，一种泛型，一种可表示该领域待处理数据集的模型。

而RDD是作为Spark平台一种基本、通用的数据抽象，基于其不关注元素内容及结构的特点，我们对结构化数据、半结构化数据、非结构化数据一视同仁，都可转化为由同一类型元素组成的RDD。但是作为一种通用、普适的工具，其必然无法高效、便捷地处理一些专门领域具有特定结构特点的数据，因此，这就是为什么，Spark在推出基础、通用的RDD编程后，还要在此基础上提供四大高级模块来针对特定领域、特定处理需求以及特定结构数据，比如Spark Streaing负责处理流数据，进行实时计算（实时计算），而Spark SQL负责处理结构化数据源，更倾向于大规模数据分析，而MLlib可用于在Spark上进行机器学习。

因此，若需处理的数据是上述的典型结构化数据源或可通过简易处理可形成鲜明结构的数据源，且其业务需求可通过典型的SQL语句来实现分析逻辑，我们可以直接引入Spark SQL模块进行编程。

使用RDD的一般场景：
- 你需要使用low-level的转化操作和行动操作来控制你的数据集；
- 你得数据集非结构化，比如，流媒体或者文本流；
- 你想使用函数式编程来操作你得数据，而不是用特定领域语言（DSL）表达；
- 你不在乎schema，比如，当通过名字或者列处理（或访问）数据属性不在意列式存储格式；
- 你放弃使用DataFrame和Dataset来优化结构化和半结构化数据集。

## Spark SQL编程入门示例
编程来使用Spark SQL接口将是本小节讲述的核心内容。注意本节示例的编程语言采用Scala，Spark版本为2.2.0。

### 程序主入口：SparkSession
Spark SQL模块的编程主入口点是SparkSession，SparkSession对象不仅为用户提供了创建DataFrame对象、读取外部数据源并转化为DataFrame对象以及执行sql查询的API，还负责记录着用户希望Spark应用如何在Spark集群运行的控制、调优参数，是Spark SQL的上下文环境，是运行的基础。

如下代码所示，可以通过SparkSession.builder()创建一个基本的SparkSession对象，并为该Spark SQL应用配置一些初始化参数，例如设置应用的名称以及通过config方法配置相关运行参数。
```
import org.apache.spark.sql.SparkSession;

public class MySparkApp {
  public static void main(String[] args) {

    // 创建一个 SparkSession 对象
    SparkSession spark = SparkSession.builder()
      .appName("MySparkApp")
      .config("spark.some.config.option", "some-value")
      .getOrCreate();

    // ... 此处省略使用 SparkSession 对象进行数据处理的代码 ...

    // 关闭 SparkSession 对象
    spark.stop();
  }
}
```
在上述代码中，我们首先通过调用 SparkSession.builder() 方法创建了一个 SparkSession.Builder 对象，然后为该构建器设置了一些初始参数，例如将应用程序的名称设置为 "MySparkApp"，并通过 config() 方法设置了一些运行参数。

最后，调用 getOrCreate() 方法来创建一个 SparkSession 对象。如果该对象已经存在，则直接返回它；否则，将根据这些参数创建一个新的 SparkSession 对象。

Spark 2.0中的SparkSession为Hive提供了强大的内置支持，包括使用HiveQL编写查询语句，访问Hive UDF以及从Hive表读取数据的功能。若是仅以学习为目的去测试这些功能时，并不需要在集群中特意安装Hive即可在Spark本地模式下测试Hive支持。

### 创建DataFrame
应用程序使用上一步创建的SparkSession对象提供的API，可以从现有的RDD，Hive表或其他结构化数据源中创建DataFrame对象。

### DataFrame基本操作
在成功地将结构化数据源转化为DataFrame对象后，DataFrame为我们提供了灵活、强大且底层自带优化的API，例如select、where、orderBy、groupBy、limit、union这样的算子操作，对于熟悉SQL的读者来说，看到DataFrame提供这一系列算子会感到十分熟悉，而DataFrame正是将SQL select语句的各个组成部分封装为同名API，用以帮助程序员通过select、where、orderBy等DataFrame API灵活地组合实现sql一样的逻辑表达。因此，DataFrame编程仅需像SQL那样简单地对计算条件、计算需求、最终所需结果进行声明式的描述即可，而不需要像RDD编程那样一步步地对数据集进行原始操作。

接下来通过几个实例来看一看DataFrame API的使用。以下实例仍沿用上一小节根据记录着学生姓名、年龄、身高、体重的JSON文件内容所创建的DataFrame对象——df。

①以树格式输出DataFrame对象的结构信息（Schema）。
```
```

②通过DataFrame的select（）方法查询数据集中name这一列。

③组合使用DataFrame对象的select()、where()、orderBy()方法查找身高大于175cm同学的姓名、和下一学年的年龄以及体重情况并且使结果集内记录按照age字段进行升序排序。

### 执行SQL查询
SparkSession为用户提供了直接执行sql语句的SparkSession.sql(sqlText：String)方法，sql语句可直接作为字符串传入sq()方法中，sql查询所得到结果依然为DataFrame对象。在Spark SQL模块上直接执行sql语句的查询需要首先将标志着结构化数据源的DataFrame对象注册成临时表，进而在sql语句中对该临时表进行查询操作，具体步骤如下例所示：

由上述操作，可看出DataFrame是Spark SQL核心的数据抽象，读取的数据源需要转化成DataFrame对象，才能利用DataFrame各种API进行丰富操作，也可将DataFrame注册成临时表，从而直接执行SQL查询，而DataFrame上的操作之后返回的也是DataFrame对象。

另外，因为本小结所讲述的是如何通过SparkSession提供的SQL接口直接进行SQL查询，而关于具体完成业务需求所需的SQL语句如何来编写，大家可以直接百度查询相关SQL教程进行学习。Spark SQL的SQL接口全面支持SQL的select标准语法，包括SELECT DISTINCT、from子句、where子句、order by字句、group by子句、having子句、join子句，还有典型的SQL函数，例如avg()、count()、max()、min()等，除此之外，Spark SQL在此基础上还提供了大量功能强大的可用函数，可嵌入sql语句中使用，有聚合类函数、时间控制类函数、数学统计类函数、字符串列控制类函数等，感兴趣或有这方面分析需求的读者具体可查看官方文档http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$。

### 全局临时表
全局临时表（global temporary view）于临时表（temporary view）是相对的，全局临时表的作用范围是某个Spark应用程序内所有会话（SparkSession），它会持续存在，在所有会话中共享，直到该Spark应用程序终止

因此，若在同一个应用中不同的session中需重用一个临时表，不妨将其注册为全局临时表，可避免多余I/O，提高系统执行效率，当然如果某个临时表只在整个应用中的某个session中需使用，仅需注册为局部临时表，避免不必要的在内存中存储全局临时表

另外，全局临时表与系统保留的数据库global_temp相关联，引用时需用global_temp标识，例如： SELECT * FROM global_temp.view1。

### Dataset
这里之所以拿出一节的篇幅来单独介绍Dataset，不仅因为Spark SQL的核心数据抽象DataFrame正是特殊的Dataset，还因为功能强大且更高效的Dataset将会逐渐取代RDD成为我们在Spark上开发编程主要使用的API，因此我们需要简要地了解Dataset

DataFrame等价于Dataset[Row]，也就是之前调用SparkSession.read接口读取数据源实际上是将结构化数据文件的每一行作为一个Row对象，最后由众多Row对象和对应的结构信息组成了Dataset对象。除了定义Row类型的Dataset，还可以是其他类型甚至是用户自定义类型的Dataset[T]。

另外，Dataset[T]中对象的序列化并不使用Java标准序列化或Kryo，而是使用专门的编码器对对象进行序列化以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换为字节，但编码器是根据Dataset[T]的元素类型（T）动态生成，并且允许Spark无须将字节反序列化回对象的情况下即可执行许多操作（如过滤、排序和散列），因此避免了不必要的反序列化导致的资源浪费，更加高效。

通过以下实例进一步了解Dataset的创建和Dataset[T]与DataFrame之间的转化：

### 将RDDs转化为DataFrame
除了调用SparkSesion.read().json/csv/orc/parquet/jdbc方法从各种外部结构化数据源创建DataFrame对象外，Spark SQL还支持将已有的RDD转化为DataFrame对象，但是需要注意的是，并不是由任意类型对象组成的RDD均可转化为DataFrame对象，只有当组成RDD[T]的每一个T对象内部具有公有且鲜明的字段结构时，才能隐式或显式地总结出创建DataFrame对象所必要的结构信息（Schema）进行转化，进而在DataFrame上调用RDD所不具备的强大丰富的API，或执行简洁的SQL查询。

Spark SQL支持将现有RDDs转换为DataFrame的两种不同方法，其实也就是隐式推断或者显式指定DataFrame对象的Schema。









