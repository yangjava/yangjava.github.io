---
layout: post
categories: Lucene
description: none
keywords: Lucene
---
# Lucene分析器
分析器是lucene中非常重要的一个组件，许多包都是分析器的子包，这是因为分析器需要支持很多不同的语言。

## lucene中的分析器
分析器可能会做的事情有：将文本拆分为单词，去除标点，将字母变为小写，去除停用词，词干还原，词形归并，敏感词过滤等等。lucene中默认自带的分析器有4个：WhitespaceAnalyzer,SimpleAnalyzer,StopAnalyzer, StandardAnalyzer，分别用来过滤空白字符，过滤空白符并自动变小写，去掉停用词，标准化分词。

其中，最常用的是StandardAnalyzer。分析器之所以能做这么多事，与之简洁而强大的类设计是分不开的。

Analyzer中最重要的就是tokenStream方法，它得到一个TokenStream对象。这个方法最开始从reuseStrategy中取得一个TokenStreamComponent，reuseStrategy是重用的策略，默认有两个实现GLOBAL_REUSE_STRATEGY和PER_FIELD_REUSE_STRATEGY。

前者让所有field共用一个TokenStreamComponent，后者每个field一个TokenStreamComponent，而实际存储这些Component的地方是Analyzer中的storedValue。接下来初始化reader并设置到component。最后从component得到TokenStream
```

  public final TokenStream tokenStream(final String fieldName,
                                       final Reader reader) throws IOException {
    TokenStreamComponents components = reuseStrategy.getReusableComponents(this, fieldName);
    final Reader r = initReader(fieldName, reader);
    if (components == null) {
      components = createComponents(fieldName, r);
      reuseStrategy.setReusableComponents(this, fieldName, components);
    } else {
      components.setReader(r);
    }
    return components.getTokenStream();
  }
```

## 分词
接下来来看看分析器的一个比较重要的功能，分词。所谓分词，就是为了便于倒排索引的查询而将一个句子切分为一个一个的term，切分的最好标准就是每个词尽量符合它在句子中语义，但实际上往往是很难达到的。大家可以试想一下，如果是一句英文的句子，那么一种简单的分词方式可以用split(" ")，这样英文中的每个单词就是一个term。但是如果是一句中文，我们能够每个字一个term吗？如果这样，那搜素出来的肯定牛头不对马嘴。因为汉语中的单词通常是词组，词组才是组成确定语义的单位。因而如何分词就是需要注意的问题了。

关于中文分词，目前大概有这样几种方法：
### 基于字符串匹配的分词方法。
比较著名的几个开源项目有IkAnalyzer和paoding。

这种分词方法简单来讲就是用待分词的句子与词典中的词进行匹配，找出最合适的匹配。这种方式实现的分词，优点是快速；但制约因素是词典本身要好，而且不在词典中的词就无法匹配，因为是纯粹匹配，碰上歧义时分词器本身也不认识到底应该识别哪一个，因而消除歧义的能力较差。比如知乎上的这样一段测试文本，中间有很多歧义的部分：“工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作”。这样的识别对这类分词器而言是比较大的挑战。

分词按照方向可以分为正向或者逆向，按照匹配的贪婪程度，可以分为最大匹配，最少次数划分（有分词理论），最细粒度匹配。之所以会有这些划分，是因为基于字符串匹配的算法通常能够解决一类问题，但是另一类会差一些。而按照这样的划分，能够给予分词一定的启发式规则。

以上面的句子为例，如果随意划分，那么开头就会是"工信/处女/干事..."，显然是有问题的。

按照最大匹配划分，就是"工信处/女/干事"，但是最大匹配划分也有问题，matrix67的文中有个例子"北京大学生前来应聘"会被划分为"北京大学/生前/来/应聘"，这也是有问题的。

所以有逆向的匹配，比如上面这句，就可以匹配成"北京/大学生/前来/应聘"。

另一种划分方式，就是最细粒度划分，还是以最初的歧义句子为例，它可以被最细粒度划分为"工信/处女/干事/每月/月经/经过..."，可以看到它并没有消除歧义，只是增加了搜索到这个句子的词。

以上这些方式实际上都是可以举出反例的，其实就是让中国人来听，汉语也只有76%的准确度。

讲讲几个开源项目IkAnalyzer, paoding和mmseg。

IkAnalyzer是用的正向迭代最细粒度切分算法以及一个智能分词法，目前IkAnalyzer实际上是比较活跃的，作者也经常更新。

paoding实际上在2010年更新了一版支持lucene3.0之后就销声匿迹了。

mmseg的算法也挺有意思，它有下面4条启发式的消除歧义规则：

1）备选词组合的长度之和最大。
2）备选词组合的平均词长最大；
3）备选词组合的词长变化最小；
4）备选词组合中，单字词的出现频率统计值最高。
最开始它从左到右扫描一遍，识别出3个词的不同组合，然后用上面4条规则匹配出最好的一个词组，然后再次用这4条规则匹配剩下的词，这样消歧能力有明显的提高。参考中有篇文章介绍mmseg，不过居然是96年发表的

## 基于统计和机器学习的分词。
比如CRF，HMM(隐式马尔科夫模型)，MEMM(最大熵隐马模型)。

这种分词方式在最初需要提供一个训练集去喂，这个训练集就是已经标注好的分词词组，然后在分词阶段通过构建模型来计算各种可能分法的概率，通过概率来最终判断。实际上，这是目前公认的比较准确的方式。CRF即条件随机场(Conditional Random Field)，在《数学之美》中已经对其描述过。可以想象，这个训练集对于分词的准确性也是非常重要的。CRF比隐马模型好在，隐马模型由于简化，每个观察值只考虑到了当前状态，而条件随机场则是考虑到了前后的状态。通俗的讲，隐马模型对于现在的输出也只考虑现在的条件；而CRF对于现在的输出需要考虑过去，现在和未来的条件。如图，上面是隐马模型，对于观测值x2，只取决于产生的状态y2；下图则是条件随机场，x2不仅取决于y2，还取决于y1以及y3.


CRF最初是被用来对一个句子做句法成分分析的，比如：我爱地球，通过条件随机场能判断出句子成分是：主语/谓语/宾语。而这个判断的过程是一个分类的过程。如果我们将这个分类换成一个4标记B, S, M, E的方式

词首，常用B表示
词中，常用M表示
词尾，常用E表示
单子词，常用S表示
同样是“我爱地球”，每个字会被分类为"我/S爱/S地/B球/E"，从而正确的区分出每个单词。而这个划分用的是著名的维特比算法。维特比算法是动态规划算法，并不特别难。这里具体就不描述了。可以参考相关资料。

## 流程分析
借助于 ES 分词DSL 看一下 analyzer的组成：
```
# 可以看出一个 analyzer 由 char_filter(任意个) + tokenier(固定一个) + filter(任意个)
# char_filter ：对原文中的char 进行中转换
# tokenier ： 将接收到的 text 切分出一系列的 token
# filter ：对tokenier 切分出来的结果的增加、删除、更改 操作。 
POST _analyze
{
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "٠ => 0",
        "١ => 1",
        "٢ => 2",
        "٣ => 3",
        "٤ => 4",
        "٥ => 5"
      ]
    },
    {
      "type": "pattern_replace",
      "pattern": """(\d+)-(?=\d)""",
      "replacement": "$1_"
    }
  ],
  "tokenizer": {
    "type": "whitespace"
  },
  "filter": [
    {
      "type": "lowercase"
    },
    {
      "type": "stop"
    }
  ],
  "text": "My license plate is ٢٥-٠١٥"
} 
# response
{
  "tokens" : [
    {
      "token" : "my",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "license",
      "start_offset" : 3,
      "end_offset" : 10,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "plate",
      "start_offset" : 11,
      "end_offset" : 16,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "25_015",
      "start_offset" : 20,
      "end_offset" : 26,
      "type" : "word",
      "position" : 4
    }
  ]
}
```

## CharFilter
从类图继承来看CharFilter 本身为java.io.Reader 子类，同时其中的 属性 input 是 java.io.Reader，可以看出为 CharFilter 就是 java.io.Reader 代理类。其中需要代理的方法就是java.io.Reader#read() 方法。 其具体子类在实现种按照自己逻辑，对原始的文本进行 char 变换。
```java
public abstract class CharFilter extends Reader {
  // 用于承接输入的 text 
  protected final Reader input;

  // 构造函数
  public CharFilter(Reader input) {
    super(input);
    this.input = input;
  }

  // 在子类中需要改变原始 text 的内容，但是为了避免 offset 引起变化，要做出修正
  protected abstract int correct(int currentOff);

  // 得到修正后的 offset， 并且是向上链式修正
  public final int correctOffset(int currentOff) {
    final int corrected = correct(currentOff);
    return (input instanceof CharFilter)
        ? ((CharFilter) input).correctOffset(corrected)
        : corrected;
  }
}
```

## Tokenizer && TokenFilter
Tokenizer： 继承自 TokenStream,字段 input 用于接收 CharFilter或 Reader的输入。输出为切词之后的 token。

TokenFilter： 同样继承自 TokenStream, 但是数据源input对应的是 TokenStream(也就说上游为 Tokenizer 或 TokenFilter ) 。作用是对 Tokenizer或TokenFilter处理后的 切词结果 token 做各种处理。
```
// 这个类里面额外设置了 其他字段，用于规范用户使用 setReader -> reset -> close 的使用流程。避免数据混入或 内存泄露
public abstract class Tokenizer extends TokenStream {
  /** The text source for this Tokenizer. */
  protected Reader input = ILLEGAL_STATE_READER;

  /** Pending reader: not actually assigned to input until reset() */
  private Reader inputPending = ILLEGAL_STATE_READER;

  /**
   * Expert: Set a new reader on the Tokenizer. Typically, an analyzer (in its tokenStream method)
   * will use this to re-use a previously created tokenizer.
   */
  public final void setReader(Reader input) {
    if (input == null) {
      throw new NullPointerException("input must not be null");
    } else if (this.input != ILLEGAL_STATE_READER) {
      throw new IllegalStateException("TokenStream contract violation: close() call missing");
    }
    this.inputPending = input;
    setReaderTestPoint();
  }

  @Override
  public void reset() throws IOException {
    super.reset();
    input = inputPending;
    inputPending = ILLEGAL_STATE_READER;
  }

  @Override
  public void close() throws IOException {
    input.close();
    // LUCENE-2387: don't hold onto Reader after close, so
    // GC can reclaim
    inputPending = input = ILLEGAL_STATE_READER;
  }

  private static final Reader ILLEGAL_STATE_READER =
      new Reader() {
        @Override
        public int read(char[] cbuf, int off, int len) {
          throw new IllegalStateException(
              "TokenStream contract violation: reset()/close() call missing, "
                  + "reset() called multiple times, or subclass does not call super.reset(). "
                  + "Please see Javadocs of TokenStream class for more information about the correct consuming workflow.");
        }

        @Override
        public void close() {}
      };
}
```

TokenFilter 简单看一下，没啥可说的
```
public abstract class TokenFilter extends TokenStream {
  /** The source of tokens for this filter. */
  protected final TokenStream input;

  /** Construct a token stream filtering the given input. */
  protected TokenFilter(TokenStream input) {
    super(input);
    this.input = input;
  }

  @Override
  public void end() throws IOException {
    input.end();
  }

  @Override
  public void close() throws IOException {
    input.close();
  }

  @Override
  public void reset() throws IOException {
    input.reset();
  }
}
```

Tokenizer 主动向 Attribute 中添加值
```
public final class StandardTokenizer extends Tokenizer {

  // this tokenizer generates three attributes:
  // term offset, positionIncrement and type
  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
  private final PositionIncrementAttribute posIncrAtt =
      addAttribute(PositionIncrementAttribute.class);
  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);

  // Tokenizer 在调用 incrementToken() 时，添加值
  @Override
  public final boolean incrementToken() throws IOException {
    clearAttributes();
    skippedPositions = 0;

    while (true) {
      int tokenType = scanner.getNextToken();

      if (tokenType == StandardTokenizerImpl.YYEOF) {
        return false;
      }

      if (scanner.yylength() <= maxTokenLength) {
        posIncrAtt.setPositionIncrement(skippedPositions + 1);
        scanner.getText(termAtt);
        final int start = scanner.yychar();
        offsetAtt.setOffset(correctOffset(start), correctOffset(start + termAtt.length()));
        typeAtt.setType(StandardTokenizer.TOKEN_TYPES[tokenType]);
        return true;
      } else
        // When we skip a too-long term, we still increment the
        // position increment
        skippedPositions++;
    }
  }
    
}
```

tokenFilter 改变其中的数据
```
public class LowerCaseFilter extends TokenFilter {
  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);

  public LowerCaseFilter(TokenStream in) {
    super(in);
  }

  @Override
  public final boolean incrementToken() throws IOException {
    if (input.incrementToken()) {
      CharacterUtils.toLowerCase(termAtt.buffer(), 0, termAtt.length());
      return true;
    } else {
      return false;
    }
  }
}
```

## Analyzer
analyzer 作用是将 各种 CharFilter 、Tokenier 、TokenFilter 组合起来使用，构成一个复合体。
```
// StandardAnalyzer 中的 createComponents() 将各种组件组合在一起
public final class StandardAnalyzer extends StopwordAnalyzerBase {

  @Override
  protected TokenStreamComponents createComponents(final String fieldName) {
    final StandardTokenizer src = new StandardTokenizer();
    src.setMaxTokenLength(maxTokenLength);
    TokenStream tok = new LowerCaseFilter(src);
    tok = new StopFilter(tok, stopwords);
    return new TokenStreamComponents(
        r -> {
          src.setMaxTokenLength(StandardAnalyzer.this.maxTokenLength);
          src.setReader(r);
        },
        tok);
  }
}
```

Analyzer 的线程安全性
```
public abstract class Analyzer implements Closeable {

  private final ReuseStrategy reuseStrategy;
  // 线程安全的保障
  CloseableThreadLocal<Object> storedValue = new CloseableThreadLocal<>();
  // 默认使用全局策略，适用于只有一个字段类型
  public Analyzer() {
    this(GLOBAL_REUSE_STRATEGY);
  }

  public final TokenStream tokenStream(final String fieldName,
                                       final Reader reader) {
    TokenStreamComponents components = reuseStrategy.getReusableComponents(this, fieldName);
    final Reader r = initReader(fieldName, reader);
    if (components == null) {
      components = createComponents(fieldName);
      reuseStrategy.setReusableComponents(this, fieldName, components);
    }
    components.setReader(r);
    return components.getTokenStream();
  }
  

  @Override
  public void close() {
    if (storedValue != null) {
      storedValue.close();
      storedValue = null;
    }
  }


  public static abstract class ReuseStrategy {

    /** Sole constructor. (For invocation by subclass constructors, typically implicit.) */
    public ReuseStrategy() {}


    public abstract TokenStreamComponents getReusableComponents(Analyzer analyzer, String fieldName);

    public abstract void setReusableComponents(Analyzer analyzer, String fieldName, TokenStreamComponents components);

    protected final Object getStoredValue(Analyzer analyzer) {
      if (analyzer.storedValue == null) {
        throw new AlreadyClosedException("this Analyzer is closed");
      }
      return analyzer.storedValue.get();
    }

    protected final void setStoredValue(Analyzer analyzer, Object storedValue) {
      if (analyzer.storedValue == null) {
        throw new AlreadyClosedException("this Analyzer is closed");
      }
      analyzer.storedValue.set(storedValue);
    }

  }
  // 全局使用一个 组件，适用于一个字段类型
  public static final ReuseStrategy GLOBAL_REUSE_STRATEGY = new ReuseStrategy() {

    @Override
    public TokenStreamComponents getReusableComponents(Analyzer analyzer, String fieldName) {
      return (TokenStreamComponents) getStoredValue(analyzer);
    }

    @Override
    public void setReusableComponents(Analyzer analyzer, String fieldName, TokenStreamComponents components) {
      setStoredValue(analyzer, components);
    }
  };

  // 每个字段使用一个组件，适用于多个字段类型
  public static final ReuseStrategy PER_FIELD_REUSE_STRATEGY = new ReuseStrategy() {

    @SuppressWarnings("unchecked")
    @Override
    public TokenStreamComponents getReusableComponents(Analyzer analyzer, String fieldName) {
      Map<String, TokenStreamComponents> componentsPerField = (Map<String, TokenStreamComponents>) getStoredValue(analyzer);
      return componentsPerField != null ? componentsPerField.get(fieldName) : null;
    }

    @SuppressWarnings("unchecked")
    @Override
    public void setReusableComponents(Analyzer analyzer, String fieldName, TokenStreamComponents components) {
      Map<String, TokenStreamComponents> componentsPerField = (Map<String, TokenStreamComponents>) getStoredValue(analyzer);
      if (componentsPerField == null) {
        componentsPerField = new HashMap<>();
        setStoredValue(analyzer, componentsPerField);
      }
      componentsPerField.put(fieldName, components);
    }
  };

}
```

## 使用样例
```
public class FCPAnalyzerDemo {

    public static void main(String[] args) {
//        Analyzer ikAnalyzer = new IKAnalyzer(false);
        DefaultConfig.getInstance();
        Dictionary.initial(DefaultConfig.getInstance());

        Analyzer analyzer = null;
        analyzer = new StandardAnalyzer();
        analyzer = new FCPAnalyzer();
//        analyzer = new WhitespaceAnalyzer();
        //获取Lucene的TokenStream对象
        TokenStream ts = null;
        try {

            String txt = "人 民*?解放軍" + System.getProperty("line.separator");
            ts = analyzer.tokenStream("myfield", new StringReader(txt));
            //获取词元位置属性
            OffsetAttribute offset = ts.addAttribute(OffsetAttribute.class);
            //获取词元文本属性
            CharTermAttribute term = ts.addAttribute(CharTermAttribute.class);
            //获取词元文本属性
            TypeAttribute type = ts.addAttribute(TypeAttribute.class);
            // position
            PositionIncrementAttribute posIncrAtt = ts.addAttribute(PositionIncrementAttribute.class);
            //重置TokenStream（重置StringReader）
            ts.reset();
            //迭代获取分词结果
            int position = -1;
            while (ts.incrementToken()) {
                position += posIncrAtt.getPositionIncrement();
                System.out.println(offset.startOffset() + " - " + offset.endOffset() + ", position : " + position+ " , " + term.toString() + " | " + type.type());
            }
            // tokenizer重置各种Attribute中的属性
            ts.end();
            // tokenizer close input，然后删除input引用， 使其可以被GC
            ts.close();

            System.out.println("------------------------------------");

            ts = analyzer.tokenStream("myfield", new StringReader(txt));
            //重置TokenStream（重置StringReader）
            ts.reset();
            //迭代获取分词结果
            position = -1;
            while (ts.incrementToken()) {
                position += posIncrAtt.getPositionIncrement();
                System.out.println(offset.startOffset() + " - " + offset.endOffset() + ", position : " + position+ " , " + term.toString() + " | " + type.type());
            }
            // tokenizer重置各种Attribute中的属性
            ts.end();
            // tokenizer close input，然后删除input引用， 使其可以被GC
            ts.close();
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            // 释放当前线程使用的资源
            if(analyzer != null){
                analyzer.close();
            }
        }
    }
}
```
