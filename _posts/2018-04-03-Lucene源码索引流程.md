---
layout: post
categories: [Lucene]
description: none
keywords: Lucene
---
# Lucene源码索引流程
Lucene 是一个基于 Java 的全文信息检索工具包，它不是一个完整的搜索应用程序，而是一个为应用程序提供索引和搜索功能。

## Lucene索引的构建过程


- 词库分析器Analyzer创建（需要注意的是使用哪种Analyzer进行索引查询，创建的时候也要使用对应的索引器，否则查询结果有问题）
- IndexWriterConfig对象创建,并获取IndexWriter对象
  - 判断是覆盖索引还是追加索引，如果是覆盖索引indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
  - 如果追加indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);

- 遍历根据要索引的对象列表，对单个对象的field进行lucene相关field构建，添加到Document对象中
- IndexWriter对索引进行写入；
- IndexWriter执行commit()和close()结束索引创建过程

## Lucene源码创建索引

### 創建索引文件目录
首先，FSDirectory的open函数用来打开索引文件夹，用来存放后面生成的索引文件，代码如下，
```
  public static FSDirectory open(Path path) throws IOException {
    return open(path, FSLockFactory.getDefault());
  }
  
  public static FSDirectory open(Path path, LockFactory lockFactory) throws IOException {
    if (Constants.JRE_IS_64BIT && MMapDirectory.UNMAP_SUPPORTED) {
      return new MMapDirectory(path, lockFactory);
    } else if (Constants.WINDOWS) {
      return new SimpleFSDirectory(path, lockFactory);
    } else {
      return new NIOFSDirectory(path, lockFactory);
    }
  }
```
索引的构建过程描述如下：
- 判断JRE版本是否为64位和是否支持堆外内存，并创建
  - 如果满足条件，创建MMapDirectory，此种Directory可以有效的利用虚拟机内存地址空间 ；
  - 如果不满足以上条件，判断系统是否是windows,如果满足条件，创建SimpleFSDirectory，此种directory提供了性能不太高的多线程支持，lucene推荐使用NIOFSDirectory或者MMapDirectory来替代之；
  - 如果以上均不满足，创建NIOFSDirectory对象
 
### 创建IndexWriterConfig
IndexWriterConfig内提供了一些供高级玩家做性能调优和功能定制的核心参数，我们列几个主要的看下：
- IndexDeletionPolicy
Lucene开放对commit point的管理，通过对commit point的管理可以实现例如snapshot等功能。Lucene默认配置的DeletionPolicy，只会保留最新的一个commit point。
- Similarity
搜索的核心是相关性，Similarity是相关性算法的抽象接口，Lucene默认实现了TF-IDF和BM25算法。相关性计算在数据写入和搜索时都会发生，数据写入时的相关性计算称为Index-time boosting，计算Normalizaiton并写入索引，搜索时的相关性计算称为query-time boosting。
- MergePolicy
Lucene内部数据写入会产生很多Segment，查询时会对多个Segment查询并合并结果。所以Segment的数量一定程度上会影响查询的效率，所以需要对Segment进行合并，合并的过程就称为Merge，而何时触发Merge由MergePolicy决定。
- MergeScheduler
当MergePolicy触发Merge后，执行Merge会由MergeScheduler来管理。Merge通常是比较耗CPU和IO的过程，MergeScheduler提供了对Merge过程定制管理的能力。
- Codec
Codec可以说是Lucene中最核心的部分，定义了Lucene内部所有类型索引的Encoder和Decoder。Lucene在Config这一层将Codec配置化，主要目的是提供对不同版本数据的处理能力。对于Lucene用户来说，这一层的定制需求通常较少，能玩Codec的通常都是顶级玩家了。
- IndexerThreadPool
管理IndexWriter内部索引线程（DocumentsWriterPerThread）池，这也是Lucene内部定制资源管理的一部分。
- FlushPolicy
FlushPolicy决定了In-memory buffer何时被flush，默认的实现会根据RAM大小和文档个数来判断Flush的时机，FlushPolicy会在每次文档add/update/delete时调用判定。
- MaxBufferedDoc
Lucene提供的默认FlushPolicy的实现FlushByRamOrCountsPolicy中允许DocumentsWriterPerThread使用的最大文档数上限，超过则触发Flush。
- RAMBufferSizeMB
Lucene提供的默认FlushPolicy的实现FlushByRamOrCountsPolicy中允许DocumentsWriterPerThread使用的最大内存上限，超过则触发flush。
- RAMPerThreadHardLimitMB
除了FlushPolicy能决定Flush外，Lucene还会有一个指标强制限制DocumentsWriterPerThread占用的内存大小，当超过阈值则强制flush。
- Analyzer：即分词器，这个通常是定制化最多的，特别是针对不同的语言。

接下来构造了SimpleAnalyzer，然后根据构造的SimpleAnalyzer创建一个IndexWriterConfig，其构造函数直接调用了其父类LiveIndexWriterConfig的构造函数，
```
  // used by IndexWriterConfig
  LiveIndexWriterConfig(Analyzer analyzer) {
    this.analyzer = analyzer;
    ramBufferSizeMB = IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB;
    maxBufferedDocs = IndexWriterConfig.DEFAULT_MAX_BUFFERED_DOCS;
    mergedSegmentWarmer = null;
    delPolicy = new KeepOnlyLastCommitDeletionPolicy();
    commit = null;
    useCompoundFile = IndexWriterConfig.DEFAULT_USE_COMPOUND_FILE_SYSTEM;
    openMode = OpenMode.CREATE_OR_APPEND;
    similarity = IndexSearcher.getDefaultSimilarity();
    mergeScheduler = new ConcurrentMergeScheduler();
    indexingChain = DocumentsWriterPerThread.defaultIndexingChain;
    codec = Codec.getDefault();
    if (codec == null) {
      throw new NullPointerException();
    }
    infoStream = InfoStream.getDefault();
    mergePolicy = new TieredMergePolicy();
    flushPolicy = new FlushByRamOrCountsPolicy();
    readerPooling = IndexWriterConfig.DEFAULT_READER_POOLING;
    perThreadHardLimitMB = IndexWriterConfig.DEFAULT_RAM_PER_THREAD_HARD_LIMIT_MB;
    maxFullFlushMergeWaitMillis = IndexWriterConfig.DEFAULT_MAX_FULL_FLUSH_MERGE_WAIT_MILLIS;
  }
  
```
LiveIndexWriterConfig构造函数又创建并保存了一系列组件。

### IndexWriter
IndexWriter提供很简单的几种操作接口。IndexWrite的提供的核心API如下：
- addDocument
比较纯粹的一个API，就是向Lucene内新增一个文档。Lucene内部没有主键索引，所有新增文档都会被认为一个新的文档，分配一个独立的docId。
下面的api主要是加入一个或多个文档，以及对这篇文档使用的analyzer。实际上，增加文档在背后是调用了更新这一操作。它所提供的接口无非是单文档和多文档，有分析器和无分析器的区别。
```
public void addDocument(IndexDocument doc) throws IOException
public void addDocument(IndexDocument doc, Analyzer analyzer) throws IOException
public void addDocuments(Iterable<? extends IndexDocument> docs) throws IOException
public void addDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer) throws IOException
```
- updateDocuments
更新文档，但是和数据库的更新不太一样。数据库的更新是查询后更新，Lucene的更新是查询后删除再新增。流程是先delete by term，后add document。但是这个流程又和直接先调用delete后调用add效果不一样，只有update能够保证在Thread内部删除和新增保证原子性，详细流程在下一章节会细说。
```
public void updateDocuments(Term delTerm, Iterable<? extends IndexDocument> docs) throws IOException
public void updateDocuments(Term delTerm, Iterable<? extends IndexDocument> docs, Analyzer analyzer) throws IOException
public void updateDocument(Term term, IndexDocument doc) throws IOException
public void updateDocument(Term term, IndexDocument doc, Analyzer analyzer)
      throws IOException
```
- deleteDocument
删除文档，支持两种类型删除，by term和by query。在IndexWriter内部这两种删除的流程不太一样。
```
public void deleteDocuments(Term term) throws IOException
public void deleteDocuments(Term... terms) throws IOException
public void deleteDocuments(Query query) throws IOException
public void deleteDocuments(Query... queries) throws IOException
public void deleteAll() throws IOException
```
第一种方式是删除所有包含这个词的文档，第二种方式是删除所有包含这几个词的文档，第三种方式是删除所有符合该查询的文档，第四种方式是删除所有符合这组查询的文档，第五种方式是直接删除所有文档

- flush
触发强制flush，将所有Thread的In-memory buffer flush成segment文件，这个动作可以清理内存，强制对数据做持久化。
- prepareCommit/commit/rollback
commit后数据才可被搜索，commit是一个二阶段操作，prepareCommit是二阶段操作的第一个阶段，也可以通过调用commit一步完成，rollback提供了回滚到last commit的操作。
- maybeMerge/forceMerge
maybeMerge触发一次MergePolicy的判定，而forceMerge则触发一次强制merge。

接下来根据刚刚创建的IndexWriterConfig创建一个IndexWriter，IndexWriter时lucene创建索引最为核心的类，
```
  public IndexWriter(Directory d, IndexWriterConfig conf) throws IOException {
    if (d instanceof FSDirectory && ((FSDirectory) d).checkPendingDeletions()) {
      throw new IllegalArgumentException();
    }

    conf.setIndexWriter(this);
    config = conf;
    infoStream = config.getInfoStream();
    writeLock = d.obtainLock(WRITE_LOCK_NAME);

    boolean success = false;
    try {
      directoryOrig = d;
      directory = new LockValidatingDirectoryWrapper(d, writeLock);
      mergeDirectory = addMergeRateLimiters(directory);

      analyzer = config.getAnalyzer();
      mergeScheduler = config.getMergeScheduler();
      mergeScheduler.setInfoStream(infoStream);
      codec = config.getCodec();

      bufferedUpdatesStream = new BufferedUpdatesStream(infoStream);
      poolReaders = config.getReaderPooling();

      OpenMode mode = config.getOpenMode();
      boolean create;
      if (mode == OpenMode.CREATE) {
        create = true;
      } else if (mode == OpenMode.APPEND) {
        create = false;
      } else {
        create = !DirectoryReader.indexExists(directory);
      }
      boolean initialIndexExists = true;
      String[] files = directory.listAll();
      IndexCommit commit = config.getIndexCommit();

      StandardDirectoryReader reader;
      if (commit == null) {
        reader = null;
      } else {
        reader = commit.getReader();
      }

      if (create) {

        if (config.getIndexCommit() != null) {
          if (mode == OpenMode.CREATE) {
            throw new IllegalArgumentException();
          } else {
            throw new IllegalArgumentException();
          }
        }

        SegmentInfos sis = null;
        try {
          sis = SegmentInfos.readLatestCommit(directory);
          sis.clear();
        } catch (IOException e) {
          initialIndexExists = false;
          sis = new SegmentInfos();
        }
        segmentInfos = sis;
        rollbackSegments = segmentInfos.createBackupSegmentInfos();
        changed();

      } else if (reader != null) {
        ...
      } else {
        ...
      }

      pendingNumDocs.set(segmentInfos.totalMaxDoc());
      globalFieldNumberMap = getFieldNumberMap();
      config.getFlushPolicy().init(config);
      docWriter = new DocumentsWriter(this, config, directoryOrig, directory);
      eventQueue = docWriter.eventQueue();
      synchronized(this) {
        deleter = new IndexFileDeleter(files, directoryOrig, directory,
                                       config.getIndexDeletionPolicy(),
                                       segmentInfos, infoStream, this,
                                       initialIndexExists, reader != null);
        assert create || filesExist(segmentInfos);
      }

      if (deleter.startingCommitDeleted) {
        changed();
      }

      if (reader != null) {
        ...
      }

      success = true;

    } finally {
      if (!success) {
        IOUtils.closeWhileHandlingException(writeLock);
        writeLock = null;
      }
    }
  }
```

IndexWriter构造函数首先通过checkPendingDeletions函数删除被标记的文件，checkPendingDeletions函数定义在FSDirectory中，如下所示
```
  public boolean checkPendingDeletions() throws IOException {
    deletePendingFiles();
    return pendingDeletes.isEmpty() == false;
  }

  public synchronized void deletePendingFiles() throws IOException {
    if (pendingDeletes.isEmpty() == false) {
      for(String name : new HashSet<>(pendingDeletes)) {
        privateDeleteFile(name, true);
      }
    }
  }

  private void privateDeleteFile(String name, boolean isPendingDelete) throws IOException {
    try {
      Files.delete(directory.resolve(name));
      pendingDeletes.remove(name);
    } catch (NoSuchFileException | FileNotFoundException e) {

    } catch (IOException ioe) {

    }
  }
```
checkPendingDeletions函数最后调用Files的delete函数删除保存在pendingDeletes的文件。

回到IndexWriter的构造函数中，接下来通过infoStream获得在LiveIndexWriterConfig构造函数中创建的NoOutput，该infoStream用来显示信息，然后调用FSDirectory的obtainLock函数获得文件的写锁，这里就不往下分析了。

回到IndexWriter的构造函数中，接下来会经过一系列的创建和赋值操作，假设create为true，即表示第一次创建或者重新创建索引，然后会通过SegmentInfos的readLatestCommit函数读取段信息，
```
  public static final SegmentInfos readLatestCommit(Directory directory) throws IOException {
    return new FindSegmentsFile<SegmentInfos>(directory) {
      @Override
      protected SegmentInfos doBody(String segmentFileName) throws IOException {
        return readCommit(directory, segmentFileName);
      }
    }.run();
  }
```

SegmentInfos的readLatestCommit函数创建了一个FindSegmentsFile并调用其run函数，定义如下，
```
    public T run() throws IOException {
      return run(null);
    }

    public T run(IndexCommit commit) throws IOException {
      long lastGen = -1;
      long gen = -1;
      IOException exc = null;

      for (;;) {
        lastGen = gen;
        String files[] = directory.listAll();
        String files2[] = directory.listAll();
        Arrays.sort(files);
        Arrays.sort(files2);
        if (!Arrays.equals(files, files2)) {
          continue;
        }
        gen = getLastCommitGeneration(files);
        if (gen == -1) {
          throw new IndexNotFoundException();
        } else if (gen > lastGen) {
          String segmentFileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS, "", gen);

          try {
            T t = doBody(segmentFileName);
            return t;
          } catch (IOException err) {

          }
        } else {
          throw exc;
        }
      }
    }
```
这里的泛型T就是SegmentInfos，run函数首先调用getLastCommitGeneration函数获得gen信息，假设索引文件夹下有一个文件名为segments_6的文件，则getLastCommitGeneration最后会返回6赋值到gen中，接下来，如果gen大于lastGen，就表示段信息有更新了，这时候就要通过doBody函数读取该segments_6文件的信息，并返回一个SegmentInfos。

根据前面readLatestCommit的代码，doBody函数最后会调用readCommit函数，定义在SegmentInfos中，代码如下
```
  public static final SegmentInfos readCommit(Directory directory, String segmentFileName) throws IOException {
    long generation = generationFromSegmentsFileName(segmentFileName);
    try (ChecksumIndexInput input = directory.openChecksumInput(segmentFileName, IOContext.READ)) {
      return readCommit(directory, input, generation);
    }
  }
```
readCommit函数首先创建一个ChecksumIndexInput，然后通过readCommit函数读取段信息并返回一个SegmentInfos，这里的readCommit函数和具体的segments_*文件格式和协议相关，这里就不往下看了。最后返回的SegmentInfos保存了段信息。

回到IndexWriter的构造函数中，如果readLatestCommit函数返回的SegmentInfos不为空，就调用其clear清空，如果是第一次创建索引，就会构造一个SegmentInfos，SegmentInfos的构造函数为空函数。接下来调用SegmentInfos的createBackupSegmentInfos函数备份其中的SegmentCommitInfo信息列表，该备份主要是为了回滚rollback操作使用。IndexWriter然后调用changed表示段信息发生了变化。

继续往下看IndexWriter的构造函数，pendingNumDocs函数记录了索引记录的文档总数，globalFieldNumberMap记录了该段中Field的相关信息，getFlushPolicy返回在LiveIndexWriterConfig构造函数中创建的FlushByRamOrCountsPolicy，然后通过FlushByRamOrCountsPolicy的init函数进行简单的赋值。再往下创建了一个DocumentsWriter，并获得其事件队列保存在eventQueue中。IndexWriter的构造函数接下来会创建一个IndexFileDeleter，IndexFileDeleter用来管理索引文件，例如添加引用计数，在多线程环境下操作索引文件时可以保持同步性。

### 创建索引
IndexWriter::addDocument函数不仅添加文档数据，而且创建了索引，下面来看。
```
  public void addDocument(Iterable<? extends IndexableField> doc) throws IOException {
    updateDocument(null, doc);
  }
  public void updateDocuments(Term delTerm, Iterable<? extends Iterable<? extends IndexableField>> docs) throws IOException {
    ensureOpen();
    try {
      boolean success = false;
      try {
        if (docWriter.updateDocuments(docs, analyzer, delTerm)) {
          processEvents(true, false);
        }
        success = true;
      } finally {

      }
    } catch (AbortingException | VirtualMachineError tragedy) {

    }
  }
```

addDocument进而调用updateDocuments函数完成索引的添加。传入的参数delTerm与更新索引和删除操作有关，这里为null，不管它。updateDocuments函数首先通过ensureOpen函数确保IndexWriter未被关闭，然后就调用DocumentsWriter的updateDocuments函数，代码如下，

IndexWriter::updateDocuments->DocumentsWriter::updateDocuments
```
  boolean updateDocuments(final Iterable<? extends Iterable<? extends IndexableField>> docs, final Analyzer analyzer,final Term delTerm) throws IOException, AbortingException {

    boolean hasEvents = preUpdate();

    final ThreadState perThread = flushControl.obtainAndLock();
    final DocumentsWriterPerThread flushingDWPT;

    try {
      ensureOpen();
      ensureInitialized(perThread);
      assert perThread.isInitialized();
      final DocumentsWriterPerThread dwpt = perThread.dwpt;
      final int dwptNumDocs = dwpt.getNumDocsInRAM();
      try {
        dwpt.updateDocuments(docs, analyzer, delTerm);
      } catch (AbortingException ae) {

      } finally {
        numDocsInRAM.addAndGet(dwpt.getNumDocsInRAM() - dwptNumDocs);
      }
      final boolean isUpdate = delTerm != null;
      flushingDWPT = flushControl.doAfterDocument(perThread, isUpdate);
    } finally {
      perThreadPool.release(perThread);
    }

    return postUpdate(flushingDWPT, hasEvents);
  }
```

updateDocuments首先调用preUpdate函数处理没有写入硬盘的数据，代码如下。
IndexWriter::updateDocuments->DocumentsWriter::updateDocuments->preUpdate
```
  private boolean preUpdate() throws IOException, AbortingException {
    ensureOpen();
    boolean hasEvents = false;
    if (flushControl.anyStalledThreads() || flushControl.numQueuedFlushes() > 0) {
      do {
        DocumentsWriterPerThread flushingDWPT;
        while ((flushingDWPT = flushControl.nextPendingFlush()) != null) {
          hasEvents |= doFlush(flushingDWPT);
        }

        flushControl.waitIfStalled();
      } while (flushControl.numQueuedFlushes() != 0);

    }
    return hasEvents;
  }
```
flushControl是在DocumentsWriter构造函数中创建的DocumentsWriterFlushControl。preUpdate函数从DocumentsWriterFlushControl中逐个取出DocumentsWriterPerThread，因为在lucene中只能有一个IndexWriter获得文件锁并操作索引文件，但是实际中对文档的索引需要多线程进行，DocumentsWriterPerThread就代表一个索引文档的线程。获取到DocumentsWriterPerThread之后，就通过doFlush将DocumentsWriterPerThread内存中的索引数据写入硬盘文件里。关于doFlush函数的分析，留在后面的章节。

回到DocumentsWriter的updateDocuments函数中，接下来通过DocumentsWriterFlushControl的obtainAndLock函数获得一个DocumentsWriterPerThread，DocumentsWriterPerThread被封装在ThreadState中，obtainAndLock函数的代码如下，
IndexWriter::updateDocuments->DocumentsWriter::updateDocuments->DocumentsWriterFlushControl::obtainAndLock
```
  ThreadState obtainAndLock() {
    final ThreadState perThread = perThreadPool.getAndLock(Thread
        .currentThread(), documentsWriter);
    boolean success = false;
    try {
      if (perThread.isInitialized()
          && perThread.dwpt.deleteQueue != documentsWriter.deleteQueue) {
        addFlushableState(perThread);
      }
      success = true;
      return perThread;
    } finally {

    }
  }
```
obtainAndLock函数中的perThreadPool是在LiveIndexWriterConfig中创建的DocumentsWriterPerThreadPool，其对应的getAndLock函数如下，
IndexWriter::updateDocuments->DocumentsWriter::updateDocuments->DocumentsWriterFlushControl::obtainAndLock->DocumentsWriterPerThreadPool::getAndLock
```
  ThreadState getAndLock(Thread requestingThread, DocumentsWriter documentsWriter) {
    ThreadState threadState = null;
    synchronized (this) {
      if (freeList.isEmpty()) {
        return newThreadState();
      } else {
        threadState = freeList.remove(freeList.size()-1);
        if (threadState.dwpt == null) {
          for(int i=0;i<freeList.size();i++) {
            ThreadState ts = freeList.get(i);
            if (ts.dwpt != null) {
              freeList.set(i, threadState);
              threadState = ts;
              break;
            }
          }
        }
      }
    }
    threadState.lock();
    return threadState;
  }
```
getAndLock函数概括来说，就是如果freeList不为空，就从中取出成员变量dwpt不为空的ThreadState，否则就创建一个新的ThreadState，创建ThreadState对应的newThreadState函数如下，
IndexWriter::updateDocuments->DocumentsWriter::updateDocuments->DocumentsWriterFlushControl::obtainAndLock->DocumentsWriterPerThreadPool::getAndLock->newThreadState
```
  private synchronized ThreadState newThreadState() {
    while (aborted) {
      try {
        wait();
      } catch (InterruptedException ie) {
        throw new ThreadInterruptedException(ie);        
      }
    }
    ThreadState threadState = new ThreadState(null);
    threadState.lock();
    threadStates.add(threadState);
    return threadState;
  }
```
newThreadState函数创建一个新的ThreadState，并将其添加threadStates中。ThreadState的构造函数很简单，这里就不往下看了。

回到DocumentsWriterFlushControl的obtainAndLock函数中，如果新创建的ThreadState中的dwpt为空，因此isInitialized返回false，obtainAndLock直接返回刚刚创建的ThreadState。

再回到DocumentsWriter的updateDocuments函数中，接下来通过ensureInitialized函数初始化刚刚创建的ThreadState中的dwpt成员变量，ensureInitialized函数的定义如下
IndexWriter::updateDocuments->DocumentsWriter::updateDocuments->ensureInitialized
```
  private void ensureInitialized(ThreadState state) throws IOException {
    if (state.dwpt == null) {
      final FieldInfos.Builder infos = new FieldInfos.Builder(writer.globalFieldNumberMap);
      state.dwpt = new DocumentsWriterPerThread(writer, writer.newSegmentName(), directoryOrig,
                                                directory, config, infoStream, deleteQueue, infos,
                                                writer.pendingNumDocs, writer.enableTestPoints);
    }
  }
```
ensureInitialized函数会创建一个DocumentsWriterPerThread并赋值给ThreadState的dwpt成员变量，DocumentsWriterPerThread的构造函数很简单，这里就不往下看了。

初始化完ThreadState的dwpt后，updateDocuments函数继续调用刚刚创建的DocumentsWriterPerThread的updateDocuments函数来索引文档，定义如下，
IndexWriter::updateDocuments->DocumentsWriter::updateDocuments->DocumentsWriterPerThread::updateDocuments
```
  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {
    docState.analyzer = analyzer;
    int docCount = 0;
    boolean allDocsIndexed = false;
    try {

      for(Iterable<? extends IndexableField> doc : docs) {
        reserveOneDoc();
        docState.doc = doc;
        docState.docID = numDocsInRAM;
        docCount++;

        boolean success = false;
        try {
          consumer.processDocument();
          success = true;
        } finally {

        }
        finishDocument(null);
      }
      allDocsIndexed = true;

    } finally {

    }

    return docCount;
  }
```
DocumentsWriterPerThread的updateDocuments函数首先调用reserveOneDoc查看索引中的文档数是否超过限制，文档的信息被封装在成员变量DocState中，然后调用consumer的processDocument函数继续处理，consumer被定义为DefaultIndexingChain。

DefaultIndexingChain的processDocument函数
DefaultIndexingChain是一个默认的索引处理链，下面来看它的processDocument函数。
IndexWriter::updateDocuments->DocumentsWriter::updateDocuments->DocumentsWriterPerThread::updateDocuments->DefaultIndexingChain::processDocument
```
  public void processDocument() throws IOException, AbortingException {

    int fieldCount = 0;
    long fieldGen = nextFieldGen++;
    termsHash.startDocument();
    fillStoredFields(docState.docID);
    startStoredFields();

    boolean aborting = false;
    try {
      for (IndexableField field : docState.doc) {
        fieldCount = processField(field, fieldGen, fieldCount);
      }
    } catch (AbortingException ae) {

    } finally {
      if (aborting == false) {
        for (int i=0;i<fieldCount;i++) {
          fields[i].finish();
        }
        finishStoredFields();
      }
    }

    try {
      termsHash.finishDocument();
    } catch (Throwable th) {

    }
  }
```
可以看到这几个主要的组件的组织非常有逻辑，基本上保持着朴素的分治思想，他们的主要作用如下：
- DocFieldProcessor: 主要是处理遇到的单个文档，也就是说每碰到一个文档就会有一个DocFieldProcessor来进行相应的处理。
- DocFieldProcessorPerField: 主要是将文档按field来归类分好，以便将任务分解，经过DocFieldProcessorPerField，可以发现整个文档已经被归类为了许多fields，接下来可以分别对每个field的进行处理
- DocInverterPerField: 主要是对每个field中的fieldsData做语汇单元的分析处理，经过这步以后，每次处理的就只是一个term
- TermHashPerField: 记录将上一步的term映射到缓冲区中后的各种信息，它会生成部分的postingsArray以及填充term的文本缓冲区。TermHashPerField中最重要的结构就是postingsArray，可以把它理解为二维数组，如图：termID其实就是获取每条记录的key，每条记录其实就是记录每个词的一些元信息。其中最重要的是textStarts，byteStarts，intStarts以及termFreqs，termFreqs很好理解，其实就是记录了词频，比如“的”字出现了3次，因此termFreqs为3.
那么textStarts，byteStarts，intStarts分别是什么呢？
在lucene3中，有三个重要的结构作为索引在内存的缓冲，它们分别是IntBlockPool, ByteBlockPool和TextBlockPool，其中IntBlockPool存在的目的是指示docid（文档号）,freq（词频）和prox（位置）在ByteBlockPool中的位置，而byteBlockPool专门存放docId,freq,prox，因为存放的字节可能不等，所以需要IntBlockPool的位置指示；而TextBlockPool则存放真正的term文本。而到了lucene4，这个结构有了稍微的改变，TextBlockPool合并到了ByteBlockPool。下面，我们对这两个重要的缓冲区和postingArray的关系展开说明，事实上，只需要了解这两个缓冲区的数据结构，便能清楚的知道这个整体的结构。

从之前的postingsArray来看，我们有textStarts，它主要作用是指示term文本内容的位置，这样，每个term都能根据textStarts迅速定位到term文本；从图中可以看出来，对应“的”的textStarts指示的位置就能够定位到ByteBlockPool中第116个字节，这里首先指示了接下来三个字节是我们的文本，于是，从117到119就可以确定是文本了。接下来byteStarts指示的是docid,freq和prox开始的位置，即ByteBlockPool的第120个位置；IntBlockPool紧邻的两个数值分别指示docid,freq与prox，intStarts指示13，于是位于IntBlockPool的第13个字节120就是docid,freq在ByteBlockPool中的位置，位于128的就是prox的下一个待写入位置。如此一来，我们就完成了从postingsArray到Pool的映射。

当然，这里远远没有了解所有问题。比如docid,freq和prox为什么都以16结尾，他们每块为什么都是5个字节？如果同一个term重复了多次，docid和prox会有多份，应当如何记录？为什么在这个例子中ByteBlockPool第一个term只有docid,freq而没有prox？
- FreqProxTermsWriterPerField: 将最终分析的prox, offset以及lastDocIDs, lastDocCodes等信息记录下来。

当添加或者更新完毕之后，这些数据会立即写入磁盘吗？答案是不会。lucene会在flush的时候将缓冲区的数据写入磁盘。FreqProxTermWriterPerField中的flush方法就负责这件事。

DefaultIndexingChain中的termsHash在DefaultIndexingChain的构造函数中被定义为FreqProxTermsWriter，其startDocument最终会调用FreqProxTermsWriter以及TermVectorsConsumer的startDocument函数作一些简单的初始化工作，FreqProxTermsWriter和TermVectorsConsumer分别在内存中保存了词和词向量的信息。DefaultIndexingChain的processDocument函数接下来通过fillStoredFields继续完成一些初始化工作。
DefaultIndexingChain::processDocument->fillStoredFields
```
  private void fillStoredFields(int docID) throws IOException, AbortingException {
    while (lastStoredDocID < docID) {
      startStoredFields();
      finishStoredFields();
    }
  }

  private void startStoredFields() throws IOException, AbortingException {
    try {
      initStoredFieldsWriter();
      storedFieldsWriter.startDocument();
    } catch (Throwable th) {
      throw AbortingException.wrap(th);
    }
    lastStoredDocID++;
  }

  private void finishStoredFields() throws IOException, AbortingException {
    try {
      storedFieldsWriter.finishDocument();
    } catch (Throwable th) {

    }
  }
```
startStoredFields函数中的initStoredFieldsWriter函数用来创建一个StoredFieldsWriter，用来存储Field域中的值，定义如下，
DefaultIndexingChain::processDocument->fillStoredFields->startStoredFields->initStoredFieldsWriter
```
  private void initStoredFieldsWriter() throws IOException {
    if (storedFieldsWriter == null) {
      storedFieldsWriter = docWriter.codec.storedFieldsFormat().fieldsWriter(docWriter.directory, docWriter.getSegmentInfo(), IOContext.DEFAULT);
    }
  }
```
这里的docWriter是DocumentsWriterPerThread，其成员变量codec会被初始化为Lucene60Codec，其storedFieldsFormat函数返回一个Lucene50StoredFieldsFormat，这些类的名称都是为了兼容使用的。Lucene50StoredFieldsFormat的fieldsWriter函数如下，
DefaultIndexingChain::processDocument->fillStoredFields->startStoredFields->initStoredFieldsWriter->Lucene50StoredFieldsFormat::fieldsWriter
```
  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
    String previous = si.putAttribute(MODE_KEY, mode.name());
    return impl(mode).fieldsWriter(directory, si, context);
  }

```
mode的默认值为BEST_SPEED，impl根据该mode会返回一个CompressingStoredFieldsFormat实例，CompressingStoredFieldsFormat的fieldsWriter函数最后会创建一个CompressingStoredFieldsWriter并返回，下面简单看一下CompressingStoredFieldsWriter的构造函数，
```
  public CompressingStoredFieldsWriter(Directory directory, SegmentInfo si, String segmentSuffix, IOContext context,
      String formatName, CompressionMode compressionMode, int chunkSize, int maxDocsPerChunk, int blockSize) throws IOException {
    assert directory != null;
    this.segment = si.name;
    this.compressionMode = compressionMode;
    this.compressor = compressionMode.newCompressor();
    this.chunkSize = chunkSize;
    this.maxDocsPerChunk = maxDocsPerChunk;
    this.docBase = 0;
    this.bufferedDocs = new GrowableByteArrayDataOutput(chunkSize);
    this.numStoredFields = new int[16];
    this.endOffsets = new int[16];
    this.numBufferedDocs = 0;

    boolean success = false;
    IndexOutput indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION), 
                                                                     context);
    try {
      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION),context);

      final String codecNameIdx = formatName + CODEC_SFX_IDX;
      final String codecNameDat = formatName + CODEC_SFX_DAT;
      CodecUtil.writeIndexHeader(indexStream, codecNameIdx, VERSION_CURRENT, si.getId(), segmentSuffix);
      CodecUtil.writeIndexHeader(fieldsStream, codecNameDat, VERSION_CURRENT, si.getId(), segmentSuffix);

      indexWriter = new CompressingStoredFieldsIndexWriter(indexStream, blockSize);
      indexStream = null;

      fieldsStream.writeVInt(chunkSize);
      fieldsStream.writeVInt(PackedInts.VERSION_CURRENT);

      success = true;
    } finally {

    }
  }
```
简单地说，CompressingStoredFieldsWriter构造函数会创建对应的.fdt和.fdx文件，并写入相应的头信息。

回到DefaultIndexingChain的startStoredFields函数中，构造完CompressingStoredFieldsWriter后，会调用其startDocument函数，该函数为空。再回头看finishStoredFields函数，该函数会通过刚刚构造的CompressingStoredFieldsWriter的finishDocument函数进行一些统计工作，在满足条件时，通过flush函数将内存中的索引信息写入到硬盘文件中，flush函数的源码留在后面的章节分析。
DefaultIndexingChain::processDocument->fillStoredFields->finishStoredFields->CompressingStoredFieldsWriter::finishDocument
```
  public void finishDocument() throws IOException {
    if (numBufferedDocs == this.numStoredFields.length) {
      final int newLength = ArrayUtil.oversize(numBufferedDocs + 1, 4);
      this.numStoredFields = Arrays.copyOf(this.numStoredFields, newLength);
      endOffsets = Arrays.copyOf(endOffsets, newLength);
    }
    this.numStoredFields[numBufferedDocs] = numStoredFieldsInDoc;
    numStoredFieldsInDoc = 0;
    endOffsets[numBufferedDocs] = bufferedDocs.length;
    ++numBufferedDocs;
    if (triggerFlush()) {
      flush();
    }
  }
```
再回到DefaultIndexingChain的processDocument函数中，接下来遍历文档的Field域，并通过processField函数逐个处理每个域，processField函数的定义如下，
DefaultIndexingChain::processDocument->processField
```
  private int processField(IndexableField field, long fieldGen, int fieldCount) throws IOException, AbortingException {
    String fieldName = field.name();
    IndexableFieldType fieldType = field.fieldType();

    PerField fp = null;
    if (fieldType.indexOptions() != IndexOptions.NONE) {

      fp = getOrAddField(fieldName, fieldType, true);
      boolean first = fp.fieldGen != fieldGen;
      fp.invert(field, first);

      if (first) {
        fields[fieldCount++] = fp;
        fp.fieldGen = fieldGen;
      }
    } else {
      verifyUnIndexedFieldType(fieldName, fieldType);
    }

    if (fieldType.stored()) {
      if (fp == null) {
        fp = getOrAddField(fieldName, fieldType, false);
      }
      if (fieldType.stored()) {
        try {
          storedFieldsWriter.writeField(fp.fieldInfo, field);
        } catch (Throwable th) {
          throw AbortingException.wrap(th);
        }
      }
    }

    return fieldCount;
  }
```
processField函数中的getOrAddField函数用来根据Field的name信息创建或获得一个PerField，代码如下，
DefaultIndexingChain::processDocument->processField->getOrAddField
```
  private PerField getOrAddField(String name, IndexableFieldType fieldType, boolean invert) {

    final int hashPos = name.hashCode() & hashMask;
    PerField fp = fieldHash[hashPos];
    while (fp != null && !fp.fieldInfo.name.equals(name)) {
      fp = fp.next;
    }

    if (fp == null) {
      FieldInfo fi = fieldInfos.getOrAdd(name);
      fi.setIndexOptions(fieldType.indexOptions());

      fp = new PerField(fi, invert);
      fp.next = fieldHash[hashPos];
      fieldHash[hashPos] = fp;
      totalFieldCount++;

      if (totalFieldCount >= fieldHash.length/2) {
        rehash();
      }

      if (totalFieldCount > fields.length) {
        PerField[] newFields = new PerField[ArrayUtil.oversize(totalFieldCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
        System.arraycopy(fields, 0, newFields, 0, fields.length);
        fields = newFields;
      }

    } else if (invert && fp.invertState == null) {
      fp.fieldInfo.setIndexOptions(fieldType.indexOptions());
      fp.setInvertState();
    }

    return fp;
  }
```
成员变量fieldHash是一个hash桶，用来存储PerField，PerField中的FieldInfo保存了Field的名称name等相应信息，并被选择适当的位置插入到fieldHash中，getOrAddField函数也会在适当的时候扩充hash桶，最后返回一个hash桶中指向新创建的PerField的指针。
回到processField函数中，接下来通过invert方法调用analyzer解析Field，这部分代码比较复杂，留到下一章分析。

回到processField函数中，假设fieldType为TYPE_STORED，而TYPE_STORED对应的stored函数返回true，表示需要对该域的值进行存储，因此接下来调用StoredFieldsWriter的writeField函数保存对应Field的值。根据本章前面的分析，这里的storedFieldsWriter为CompressingStoredFieldsWriter，其writeField函数如下，
DefaultIndexingChain::processDocument->processField->CompressingStoredFieldsWriter::writeField
```
  public void writeField(FieldInfo info, IndexableField field)
      throws IOException {

    ...

    string = field.stringValue();

    ...

    if (bytes != null) {
      bufferedDocs.writeVInt(bytes.length);
      bufferedDocs.writeBytes(bytes.bytes, bytes.offset, bytes.length);
    } else if (string != null) {
      bufferedDocs.writeString(string);
    } else {
      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
        bufferedDocs.writeZInt(number.intValue());
      } else if (number instanceof Long) {
        writeTLong(bufferedDocs, number.longValue());
      } else if (number instanceof Float) {
        writeZFloat(bufferedDocs, number.floatValue());
      } else if (number instanceof Double) {
        writeZDouble(bufferedDocs, number.doubleValue());
      } else {
        throw new AssertionError("Cannot get here");
      }
    }
  }

```
假设需要存储的Field的值为String类型，因此这里最终会调用bufferedDocs的writeString函数，bufferedDocs在CompressingStoredFieldsWriter的构造函数中被设置为GrowableByteArrayDataOutput，其writeString函数就是将对应的String值缓存在内存的某个结构中。在特定的时刻通过flush函数将这些数据存入.fdt文件中。

回到DefaultIndexingChain的processDocument函数中，接下来遍历fields，取出前面在processField函数中创建的各个PerField，并调用其finish函数，该函数深入看较为复杂，但没有特别重要的内容，这里就不往下看了。processDocument最后会调用finishStoredFields函数，该函数前面已经分析过了，主要是在必要的时候触发一次flush操作。

回到DocumentsWriterPerThread的updateDocuments函数中，接下来清空刚刚创建的DocState，并调用finishDocument处理一些需要删除的数据，这里先不管该函数。

再向上回到DocumentsWriter的updateDocuments函数中，接下来的numDocsInRAM保存了当前有多少文档被索引了，然后再调用DocumentsWriterFlushControl的doAfterDocument函数继续处理，，本章暂时不管它。然后updateDocuments函数调用release函数释放开始创建的ThreadState，即将其放入一个freeList中用来给后面的程序调用。最后通过postUpdate函数选择相应的DocumentsWriterPerThread并调用其doFlush函数将索引数据写入硬盘文件中。
IndexWriter::updateDocuments->DocumentsWriter::updateDocuments->
```
  private boolean postUpdate(DocumentsWriterPerThread flushingDWPT, boolean hasEvents) throws IOException, AbortingException {
    hasEvents |= applyAllDeletes(deleteQueue);
    if (flushingDWPT != null) {
      hasEvents |= doFlush(flushingDWPT);
    } else {
      final DocumentsWriterPerThread nextPendingFlush = flushControl.nextPendingFlush();
      if (nextPendingFlush != null) {
        hasEvents |= doFlush(nextPendingFlush);
      }
    }

    return hasEvents;
  }
```
再向上回到IndexWriter的updateDocument中，接下来调用processEvents触发相应的事件，代码如下，
IndexWriter::updateDocuments->processEvents
```
  private boolean processEvents(boolean triggerMerge, boolean forcePurge) throws IOException {
    return processEvents(eventQueue, triggerMerge, forcePurge);
  }

  private boolean processEvents(Queue<Event> queue, boolean triggerMerge, boolean forcePurge) throws IOException {
    boolean processed = false;
    if (tragedy == null) {
      Event event;
      while((event = queue.poll()) != null)  {
        processed = true;
        event.process(this, triggerMerge, forcePurge);
      }
    }
    return processed;
  }
```
processEvents函数会一次从Queue队列中取出Event事件，并调用其process函数进行处理。

















































































