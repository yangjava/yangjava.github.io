---
layout: post
categories: [Lucene]
description: none
keywords: Lucene
---
# Lucene源码索引设计


## 倒排索引结构？
大家已经非常熟悉倒排索引的数据结构。
```
term1 -> <doc1, doc2, doc_n>
term2 -> <doc-x,doc_y, ..>
```
正排索引：文档ID到文档内容，文档字段（Field）的关联关系。
倒排索引： 分词（Term)到文档ID的映射关系。

Lucene在实际存储倒排索引时，需要考虑支持一些复杂搜索场景，从而存储的信息会更加丰富一些。它会记录term在一个文档doc中出现的频度，以及出现的位置信息。

## Posting结构
一个Doc文档由若干个Fields组成，每个字段的值通过分词解析后输出一串terms。Lucene为每个term初始化一个Posting数据结构，用来表达term在当前Segment索引下的信息。

```
/* Used to track postings for a single term.  One of these
   * exists per unique term seen since the last flush. */
  private final static class Posting {
    int textStart;                                  
    // Address into char[] blocks where our text is stored
    int docFreq;                                    
    // # times this term occurs in the current doc
    int freqStart;                                  
    // Address of first byte[] slice for freq
    int freqUpto;                                   
    // Next write address for freq
    int proxStart;                                  
    // Address of first byte[] slice
    int proxUpto;                                   
    // Next write address for prox
    int lastDocID;                                  
    // Last docID where this term occurred
    int lastDocCode;                                
    // Code for prior doc
    int lastPosition;                               
    // Last position where this term occurred
    PostingVector vector;                           
    // Corresponding PostingVector instance
  }
```
Posting结构存储了每个term在当前segment中关键信息，比如当前term的字符串值（由CharBlockPool缓冲存储），当前segment中哪些文档包含了当前的term，当前term在每个文档中出现具备位置的信息（由ByteBlockPool缓冲存储）。

倒排索引项( Posting )主要包含如下信息:

- 文档ID，用于获取原始信息
- 单词频率(Term Frequency )，记录该单词在该文档中的出现次数
- 位置( Position)
- 偏移( Offset )
上述信息是存储在一个动态扩展的一段内存空间中，分别由ByteBlockPool或者CharBlockPool来组织与管理。而Posting中大多数字段是指向ByteBlockPool缓存的地址偏移值。在我看来，PostingIndex命名会更加准确。

```
 private final class ByteBlockPool {
    public byte[][] buffers = new byte[10][];
    int bufferUpto = -1;                        
   // Which buffer we are upto
    public int byteUpto = BYTE_BLOCK_SIZE;             
   // Where we are in head buffer

    public byte[] buffer;                              
   // Current head buffer
    public int byteOffset = -BYTE_BLOCK_SIZE;          
   // Current head offset
```

## ByteBlockPool 动态数组
通过它的字段byte[] [] buffers定义，可知ByteBlockPool是一个可以动态增长的数组，它由多个一维buffer构成的二维数组。当一个buffer写满后，需要继续申请新的buffer空间。

## Slice链表
索引在构建过程中，本质是将文档字段内容进行分词，拆分成多个term，并为每一个term初始化Posting结构，建立term分词在segment中的倒排索引结构。

每个term需要独立的空间来存储Posting信息。一个term可能会出现在多个文档中，而且在每个文档中出现的次数与位置都无法确定，所以Lucene是无法提前预知一个term需要多大的空间来存储倒排索引的信息。

为此Lucene在ByteBlockPool数据结构基础之上设计了可变长的逻辑结构，这个结构就是Slice链表，它的节点称之为Slice节点。Lucene将Slice链表分成十级，各级Slice节点的长度定义在levelSizeArray数组中，逐层递增，十层之后链表元素的长度就恒定。newSlice函数用来向ByteBlockPool二维数组物理空间中申请一个大小为size的slice节点。

```
 public int newSlice(final int size) {
      if (byteUpto > BYTE_BLOCK_SIZE-size)
        nextBuffer();
      final int upto = byteUpto;
      byteUpto += size;
      buffer[byteUpto-1] = 16;
      return upto;
    }
```
每个Slice节点的最后4个字节（next指针）用来存储下一个Slice节点的首地址，从而构成Slice链表。Slice节点是可以跨越多个Buffer （本质就是byte[]一维字节数组）。从而在ByteBlockPool动态二维数组物理空间之上构建一个逻辑上连接的内存块Slice链表。

```
 // Size of each slice.  These arrays should be at most 16
  // elements.  First array is just a compact way to encode
  // X+1 with a max.  Second array is the length of each
  // slice, ie first slice is 5 bytes, next slice is 14
  // bytes, etc.
  final static int[] nextLevelArray = {1, 2, 3, 4, 5, 6, 7, 8, 9, 9};
  final static int[] levelSizeArray = {5, 14, 20, 30, 40, 40, 80, 80, 120, 200};
```
如果将逻辑Slice链表理解成分布式文件系统中的文件，每个Slice节点则是文件的数据块(data chunk)。不同的是，文件系统中的数据块是大小固定的，而在Slice节点的长度是分层级（非固定大小的数据块）。

## Posting VS. ByteBlockPool
ByteBlockPool是一个ThreadState所共享的动态二维物理空间，用来存储倒排索引信息，比如分词出现的文档列表，频度和位置 。

Posting是用来索引ByteBlockPool缓冲的结构信息。多个term分词的倒排索引信息是可以共享ByteBlockPool二维物理空间。

回头再来看Posting字段的分类：

<textStart>：代表的term分词的value构成一个slice链表
<docID, docFrq>：二元组构成一个slice链表，代表term出现在哪些文档中，以及出现的频度。
<ProxStart,payload>： 二元组构成一个slice链表，代表term在出现文档中的具体位置，以及文档payload.
对于每个term分词可以形象理解成下图组织：

## ThreadState
Lucene会为每个插入文档创建ThreadState对象，允许多个线程并行分析文件Doc并插入到一个segment索引中。

ThreadState负责解析一个文档下所有的字段内容进行分词term后，并创建对应的倒排索引信息，比如<docID,docFrq> slice链表，<Pos,Payload>slice链表信息统一存储共享在ThreadState上下文中。
```
private final class ThreadState {
    Posting[] postingsFreeList;           
  // Free Posting instances
    int postingsFreeCount;

    RAMOutputStream tvfLocal = new RAMOutputStream();    
  // Term vectors for one doc
    RAMOutputStream fdtLocal = new RAMOutputStream();    
  // Stored fields for one doc
    FieldsWriter localFieldsWriter;       
  // Fields for one doc

    long[] vectorFieldPointers;
    int[] vectorFieldNumbers;

    FieldData[] fieldDataArray;           
  // Fields touched by current doc
  
   final ByteBlockPool postingsPool = new ByteBlockPool();
   final ByteBlockPool vectorsPool = new ByteBlockPool();
   final CharBlockPool charPool = new CharBlockPool();
  
boolean updateDocument(Document doc, Analyzer analyzer, Term delTerm) {
    final ThreadState state = getThreadState(doc, delTerm);
    try {
      boolean success = false;
      try {
        try {
          state.processDocument(analyzer);
        } finally {
          finishDocument(state);
        }
        success = true;
      }  
    } 
```
ThreadState设计用来处理一个文档，它的主要流程：

TheadState#init(doc)用来完成对doc文档字段进行分析，并初始化FieldData[]数组
TheadState#processDocument(analyzer)完成对fieldData[]数组的迭代遍历。fieldData结构包含组成文档的字段Field和Field的值，而processDocument工作是将field-value的值进行切分成一串term分词，并通过invertField函数构建term的倒排索引结构。
TheadState#finishDocument用来完成tvx/tvd/tvf索引文件序列化进磁盘。
特别注意的是，TheadState#finishDocument函数执行结束后，term的<docID,docFrq> 以及 <Pos,Payload>倒排索引信息还保存在ThreadState#Posting与ByteBlockPool内存结构中，并没有序列化进行磁盘。

为什么TheadState#finishDocument函数中没有一并将frq/pos/tii/tis这些索引信息序列化进磁盘呢？

## DocumentWriter
```
final class DocumentsWriter {
  private IndexWriter writer;
  private Directory directory;

  private FieldInfos fieldInfos = new FieldInfos(); 
  // All fields we've seen
  private IndexOutput tvx, tvf, tvd;              
  // To write term vectors
  private FieldsWriter fieldsWriter;              
  // To write stored fields

  private String segment;                         
  // Current segment we are working on
  
  // Max # ThreadState instances; if there are more threads
  // than this they share ThreadStates
  private ThreadState[] threadStates = new ThreadState[0];
  private final HashMap threadBindings = new HashMap();
```
首先明确DocumentWriter与ThreadState之间的关系. DocumentWriter支持多线程上下文中进行文档的插入，其内部维护多个ThreadState实例对象。

前方分析了ThreadState作为一个线程上下文具备安全地处理一个文档，DocumentWriter接收多个处理文档，内部复用ThreadState机制实现多线程并行分析Doc并插入文件数据的功能。其基本流程是：

DocumentWriter#AddDocument(docs)
ThreadState1
ThreadState1#init(doc1)
ThreadState#processDocument
ThreadState#finishDocument
ThreadState2
ThreadState2#init(doc2)
ThreadState2#processDocument
ThreadState2#finishDocument
...
DocumentWriter#Close
DocumentWriter#flush
DocumentWriter#writeSegment:创建索引Segment
Get all threadStates from DocumentWriter
merged fieldData[] of all threadStates from DocumentWriter

那么将多个threadState上下文中的fieldData[] 进行合并，是什么意思？

为什么TheadState#finishDocument函数中没有一并将frq/pos/tii/tis这些索引信息序列化进磁盘呢？
这个问题与前一个问题可以合起来一起回答。

合并来自不同ThreadStates实例的Term倒排索引: [term-posting + byteBlockPool + charBlockPool]
首先，你需要充分明确一个ThreadState对象下管理的term倒排索引的结构。当然，它还是类似 <term> slice链表 + <docID, docFrq> slice链表 + <Pos, Payload> slice链表的信息。

考虑到需要在多个ThreadState管理的term进行合并，我们换一个Tree-view视角来解读上述数据结构。

DocumentWriter是写数据的源头。
它管理多个ThreadState实例对象，用来支持多线程并行将多个文档写入同一个索引。
ThreadState负责单个文档数据写入索引。
单个文档由多个Field组成。
每个Field-value由分词器（analyzer)将一个字符串转化为一系列的term。 term是最小分词单元。
一个Field由一个或者多个term组成。
一个term由一个或者多个<docID,feq>组成。
对于每个term的倒排索引，<docID, freq> slice链表信息是最核心的，分别表示term分词出现的文档号和对应的词频，它们是一一对应的。比如一个term1出现在二个doc1,doc2中，就会出现二个元组<doc1, freq1> 和<doc2,freq2>。
一个<docID,Freq>由一个或者多个<position, payload>组成
一个doc中可能出现多次相同的term，它对应的<docID, freq>元组只需要记录一次，但是<position, payload>需要根据term出现的frequency频度来记录term出现在doc中每一次具体的位置。正是这个原因， .pos索引文件是比较大的。

## 二棵树的Merge? -- 倒排索引结构的合并
Segment组织结构
我们知道Lucene中index索引由一个或者多个segment段组成。而segment是最小的独立索引单元，由多个Docs文档构成。

上文分析，一个ThreadState实例对象负责一个文档数据的写索引操作。当我们写Segment文档时，需要进一步将一个DocuemtWriters管理的多个文档数据进行合并。

"文档数据"进行合并？
本质将不同Doc上下文中相同term倒排索引结构执行合并。之所以出现这个需求，是ThreadState引入而导致的问题。

我们再一起回顾上图：为了支持Lucene多线程并行写文档数据。更准确说，Lucene支持多线程并行分析文档，在内存为term分词构建倒排索引结构，而引入ThreadState设计。

它会带来一个新的问题：不同Doc解析出的term分词，其构造的倒排索引结构是存储在ThreadState#ByteBlcokPool局部缓存中。

当我们调用DocumentWriter#flush函数，将内存倒排索引的数据刷新到目录并生成Segment段，之后写入segment的数据文档才能允许用户搜索。

换句话说，DocumentWriter管理一堆ThreadState对象，而不同ThreadState对象管理一堆Doc的倒排索引结构，具体表现是ThreadState使用局部的ByteBlockPool结构来缓存倒排索引信息。最后DocumentWriter#flush用来操作与合并众多局部的ByteBlockPool结构。

writeSegment逻辑
对于writeSegment函数，从多个线程上下文threadStates中取出不同doc的fieldData，从中找出相同field-name的fieldData写入数组fields中，并调用appendPostings函数将具有相同name的fieldData进行合并，并最终写入tii/tis/frq/prs索引文件中。
```
 private List writeSegment() throws IOException {

    
    IndexOutput freqOut = directory.createOutput(segmentName + ".frq");
    IndexOutput proxOut = directory.createOutput(segmentName + ".prx");

    // Gather all FieldData's that have postings, across all
    // ThreadStates
    ArrayList allFields = new ArrayList();
    assert allThreadsIdle();
    for(int i=0;i<threadStates.length;i++) {
      ThreadState state = threadStates[i];
      state.trimFields();
      final int numFields = state.numAllFieldData;
      for(int j=0;j<numFields;j++) {
        ThreadState.FieldData fp = state.allFieldDataArray[j];
        if (fp.numPostings > 0)
          allFields.add(fp);
      }
    }

    // Sort by field name
    Collections.sort(allFields);
    final int numAllFields = allFields.size();

    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,
    termsOut.maxSkipLevels,
    numDocsInRAM, freqOut, proxOut);

    int start = 0;
    while(start < numAllFields) {

      final String fieldName = ((ThreadState.FieldData) allFields.get(start)).fieldInfo.name;

      int end = start+1;
      while(end < numAllFields && ((ThreadState.FieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))
        end++;
      
      ThreadState.FieldData[] fields = new ThreadState.FieldData[end-start];
      for(int i=start;i<end;i++)
        fields[i-start] = (ThreadState.FieldData) allFields.get(i);

      // If this field has postings then add them to the
      // segment
      appendPostings(fields, termsOut, freqOut, proxOut);

      for(int i=0;i<fields.length;i++)
        fields[i].resetPostingArrays();

      start = end;
    }
```
appendPosting逻辑
appendPosting将相同名字的field域的倒排表posting进行合并后，添加到Lucene索引文件。

AppendPostings函数先取出DocumentWriter管理的多个ThreadState对象，然后遍历每个局部ThreadState#ByteBlockPool缓存结构，将相同term的倒排索引信息进行合并。

比如，下面二个相同term1出现在不同的文档doc1与doc2中，并且分别由不同的ThreadState1与ThreadState2解析管理与存储。
```
//ThreadState1#ByteBlockPool缓存
term1 -> <doc1, freq2>           //对应frq索引文件
term1 -> <pos1-1,pos1-2>         //对应pos索引文件

//ThreadState2#ByteBlockPool缓存
term1 -> <doc2, freq2>          //对应frq索引文件
term1 -> <pos2-1,pos2-2>        //对应pos索引文件
```
现在对tem1进行合并，其最终结构会变更成如下:
```
term1 -> <doc1, feq2> <doc2, freq2>        //对应frq索引文件
term1 -> <pos1-1, pos1-1> <pos2-1, pos2-2> //对应pos索引文件
```

既然是term的倒排索引合并，自然是从 field -> term -> <docID, freq> -> <position, payload>这4层结构来遍历：

while(numFields > 0) 代表对field的遍历
while(numToMerge > 0) 代表对term与<docID, freq>二层的遍历
for(int j=0;j<termDocFreq;j++) 代表对<position, payload>的遍历
```

  /* Walk through all unique text tokens (Posting
   * instances) found in this field and serialize them
   * into a single RAM segment. */
  void appendPostings(ThreadState.FieldData[] fields,
                      TermInfosWriter termsOut,
                      IndexOutput freqOut,
                      IndexOutput proxOut)
    throws CorruptIndexException, IOException {

    final int fieldNumber = fields[0].fieldInfo.number;
    int numFields = fields.length;

    final FieldMergeState[] mergeStates = new FieldMergeState[numFields];

    for(int i=0;i<numFields;i++) {
      FieldMergeState fms = mergeStates[i] = new FieldMergeState();
      fms.field = fields[i];
      fms.postings = fms.field.sortPostings();
      assert fms.field.fieldInfo == fields[0].fieldInfo;
      // Should always be true
      boolean result = fms.nextTerm();
      assert result;
    }

    final int skipInterval = termsOut.skipInterval;
    currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;

    FieldMergeState[] termStates = new FieldMergeState[numFields];

    while(numFields > 0) {

      // Get the next term to merge
      termStates[0] = mergeStates[0];
      int numToMerge = 1;

      for(int i=1;i<numFields;i++) {
        final char[] text = mergeStates[i].text;
        final int textOffset = mergeStates[i].textOffset;
        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);

        if (cmp < 0) {
          termStates[0] = mergeStates[i];
          numToMerge = 1;
        } else if (cmp == 0)
          termStates[numToMerge++] = mergeStates[i];
      }

      int df = 0;
      int lastPayloadLength = -1;

      int lastDoc = 0;

      final char[] text = termStates[0].text;
      final int start = termStates[0].textOffset;
      int pos = start;
      while(text[pos] != 0xffff)
        pos++;

      long freqPointer = freqOut.getFilePointer();
      long proxPointer = proxOut.getFilePointer();

      skipListWriter.resetSkip();

      // Now termStates has numToMerge FieldMergeStates
      // which all share the same term.  Now we must
      // interleave the docID streams.
      while(numToMerge > 0) {
        
        if ((++df % skipInterval) == 0) {
          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);
          skipListWriter.bufferSkip(df);
        }

        FieldMergeState minState = termStates[0];
        for(int i=1;i<numToMerge;i++)
          if (termStates[i].docID < minState.docID)
            minState = termStates[i];

        final int doc = minState.docID;
        final int termDocFreq = minState.termFreq;

        assert doc < numDocsInRAM;
        assert doc > lastDoc || df == 1;

        final int newDocCode = (doc-lastDoc)<<1;
        lastDoc = doc;

        final ByteSliceReader prox = minState.prox;

        // Carefully copy over the prox + payload info,
        // changing the format to match Lucene's segment
        // format.
        for(int j=0;j<termDocFreq;j++) {
          final int code = prox.readVInt();
          if (currentFieldStorePayloads) {
            final int payloadLength;
            if ((code & 1) != 0) {
              // This position has a payload
              payloadLength = prox.readVInt();
            } else
              payloadLength = 0;
            if (payloadLength != lastPayloadLength) {
              proxOut.writeVInt(code|1);
              proxOut.writeVInt(payloadLength);
              lastPayloadLength = payloadLength;
            } else
              proxOut.writeVInt(code & (~1));
            if (payloadLength > 0)
              copyBytes(prox, proxOut, payloadLength);
          } else {
            assert 0 == (code & 1);
            proxOut.writeVInt(code>>1);
          }
        }

        if (1 == termDocFreq) {
          freqOut.writeVInt(newDocCode|1);
        } else {
          freqOut.writeVInt(newDocCode);
          freqOut.writeVInt(termDocFreq);
        }

        if (!minState.nextDoc()) {
          // Remove from termStates
          int upto = 0;
          for(int i=0;i<numToMerge;i++)
            if (termStates[i] != minState)
              termStates[upto++] = termStates[i];
          numToMerge--;

          // Advance this state to the next term
          if (!minState.nextTerm()) {
            // OK, no more terms, so remove from mergeStates
            // as well
            upto = 0;
            for(int i=0;i<numFields;i++)
              if (mergeStates[i] != minState)
                mergeStates[upto++] = mergeStates[i];
            numFields--;
          }
        }
      }

      assert df > 0;

      // Done merging this term

      long skipPointer = skipListWriter.writeSkip(freqOut);

      // Write term
      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
      termsOut.add(fieldNumber, text, start, pos-start, termInfo);
    }
  }
```
Skipdata跳表存储管理 （未完待续）
上面AppendPostings函数逻辑其实比较复杂，其中有二个关键的步骤是：

写freq和pos索引文件。
```
if (currentFieldStorePayloads) {
            final int payloadLength;
            if ((code & 1) != 0) {
              // This position has a payload
              payloadLength = prox.readVInt();
            } else
              payloadLength = 0;
            if (payloadLength != lastPayloadLength) {
              proxOut.writeVInt(code|1);
              proxOut.writeVInt(payloadLength);
              lastPayloadLength = payloadLength;
            } else
              proxOut.writeVInt(code & (~1));
            if (payloadLength > 0)
              copyBytes(prox, proxOut, payloadLength);
          } 
  
  if (1 == termDocFreq) {
          freqOut.writeVInt(newDocCode|1);
        } else {
          freqOut.writeVInt(newDocCode);
          freqOut.writeVInt(termDocFreq);
        } 
```

为freq和pos索引文件建立跳表数据结构并序列化写入文件。
```
if ((++df % skipInterval) == 0) {
          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);
          skipListWriter.bufferSkip(df);
        }
```

























