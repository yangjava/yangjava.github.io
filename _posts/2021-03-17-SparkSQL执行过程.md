---
layout: post
categories: [Spark]
description: none
keywords: Spark
---
# SparkSQL执行过程
简单介绍Spark SQL内部机制中涉及的基本概念和数据结构。

## 从SQL到RDD
在典型的Spark SQL应用场景中，数据的读取、数据表的创建和分析都是必不可少的过程。通常来讲，SQL查询所面对的数据模型以关系表为主。

使用 Spark SQL 进行数据分析的一般步骤如下：
```
// 第一步 初始化 SparkSession
val spark = SparkSession.builder()
      .appName("example")
      .master("local")
      .getOrCreate()

// 第二步 创建数据表并读取
spark.read.json("student.json")
 .createOrReplaceTempView("student")

// 第三步 通过SQL查询
spark.sql("select name from student where age > 18").show()
```
这里的 student.json 如下：
```
{"id":1,"name":"Kate","age":29}
{"id":2,"name":"Andy","age":30}
{"id":3,"name":"Tony","age":10}
```
案例中涉及的操作分为3步。

- 创建SparkSession类。从2.0版本开始，SparkSession逐步取代SparkContext成为Spark应用程序的入口。
- 创建数据表并读取数据。这里假设数据存储在本地名为student的json文件中，包含3条记录且每条记录包含3个列（分别对应学生的id、name和age）。本案例创建了同样名为student的数据表（视图）。
- 通过SQL进行数据分析。在SparkSession类的sql方法中可以输入任意满足语法的语句，本案例所查询的数据是年龄在18岁以上的学生名字。

值得一提的是，上述案例第2步创建数据表时虽然没有显示调用SQL语句（如关系数据库中的“create table”等），但其本质上也是SQL中的一种（DDL操作），在内部转换执行时，所涉及的流程和第3步执行SQL查询的流程类似。因此，从一般性考虑，后续内容只对第3步背后的实现进行分析。

这里首先从通用的角度介绍SQL转换的过程。一般来讲，对于Spark SQL系统，从SQL到Spark中RDD的执行需要经过两个大的阶段，分别是逻辑计划（LogicalPlan）和物理计划（PhysicalPlan）。

逻辑计划阶段会将用户所写的SQL语句转换成树型数据结构（逻辑算子树），SQL语句中蕴含的逻辑映射到逻辑算子树的不同节点。顾名思义，逻辑计划阶段生成的逻辑算子树并不会直接提交执行，仅作为中间阶段。

最终逻辑算子树的生成过程经历3个子阶段，分别对应未解析的逻辑算子树（Unresolved LogicalPlan，仅仅是数据结构，不包含任何数据信息等）、解析后的逻辑算子树（Analyzed LogicalPlan，节点中绑定各种信息）和优化后的逻辑算子树（Optim ized LogicalPlan，应用各种优化规则对一些低效的逻辑计划进行转换）。

物理计划阶段将上一步逻辑计划阶段生成的逻辑算子树进行进一步转换，生成物理算子树。物理算子树的节点会直接生成RDD或对RDD进行transformation操作（注：每个物理计划节点中都实现了对RDD进行转换的execute方法）。同样地，物理计划阶段也包含3个子阶段：首先，根据逻辑算子树，生成物理算子树的列表Iterator[PhysicalPlan]（同样的逻辑算子树可能对应多个物理算子树）；然后，从列表中按照一定的策略选取最优的物理算子树（SparkPlan）；最后，对选取的物理算子树进行提交前的准备工作，例如，确保分区操作正确、物理算子树节点重用、执行代码生成等，得到“准备后”的物理算子树（Prepared SparkPlan）。经过上述步骤后，物理算子树生成的RDD执行action操作（如例子中的show），即可提交执行。

从SQL语句的解析一直到提交之前，上述整个转换过程都在Spark集群的Driver端进行，不涉及分布式环境。SparkSession类的sql方法调用SessionState中的各种对象，包括上述不同阶段对应的SparkSqlParser类、Analyzer类、Optim izer类和SparkPlanner类等，最后封装成一个QueryExecution对象。因此，在进行Spark SQL开发时，可以很方便地将每一步生成的计划单独剥离出来分析。

回到前面的案例，SQL语句较为简单（不包含Join和Aggregation等操作），因此其转换过程也相对简单。SQL语句，生成的逻辑算子树中有Relation、Filter和Project节点，分别对应数据表、过滤逻辑（age>18）和列剪裁逻辑（只涉及3列中的2列）。下一步的物理算子树从逻辑算子树一对一映射得到，Relation逻辑节点转换为FileSourceScanExec执行节点，Filter逻辑节点转换为FilterExec执行节点，Project逻辑节点转换为ProjectExec执行节点。

生成的物理算子树根节点是ProjectExec，每个物理节点中的execute函数都是执行调用接口，由根节点开始递归调用，从叶子节点开始执行。图3.3下方展示了物理算子树的执行逻辑，与直接采用RDD进行编程类似。需要注意的是，FileSourceScanExec叶子执行节点中需要构造数据源对应的RDD，FilterExec和ProjectExec中的execute函数对RDD执行相应的transformation操作。

总的来看，SQL转换为RDD在流程上比较清晰。虽然实际生产环境中的SQL语句非常复杂，涉及的映射操作也比较烦琐，但总体上仍然遵循上述步骤。在后续章节会详细剖析这一整套转换流程。第4章会介绍如何从SQL解析为逻辑计划，第5章对逻辑计划（LogicalPlan）中的各个子阶段进行详细分析，第6章介绍物理计划（PhysicalPlan）中的实现机制。

## 重要概念
Spark SQL内部实现上述流程中平台无关部分的基础框架称为Catalyst。在深入分析流程每个阶段的原理之前，本节先简要介绍Catalyst中涉及的重要概念和数据结构，主要包括InternalRow体系、TreeNode体系和Expression体系。

### InternalRow体系
数据处理首先需要考虑如何表示数据。对于关系表来讲，通常操作的数据都是以“行”为单位的。在Spark SQL内部实现中，InternalRow就是用来表示一行行数据的类，因此物理算子树节点产生和转换的RDD类型即为RDD[InternalRow]。此外，InternalRow中的每一列都是Catalyst内部定义的数据类型。

从类的定义来看，InternalRow作为一个抽象类，包含num Fields和update方法，以及各列数据对应的get与set方法，但具体的实现逻辑体现在不同的子类中。需要注意的是，InternalRow中都是根据下标来访问和操作列元素的。

InternalRow体系比较简单，其具体的实现不多，包括BaseGenericInternalRow、UnsafeRow和JoinedRow 3个直接子类。

- BaseGenericInternalRow：同样是一个抽象类，实现了InternalRow中定义的所有get类型方法，这些方法的实现都通过调用类中定义的genericGet虚函数进行，该函数的实现在下一级子类中。
- JoinedRow：顾名思义，该类主要用于Join操作，将两个InternalRow放在一起形成新的InternalRow。使用时需要注意构造参数的顺序。
- UnsafeRow：不采用Java对象存储的方式，避免了JVM中垃圾回收（GC）的代价。此外，UnsafeRow对行数据进行了特定的编码，使得存储更加高效。

从直接子类继续往下，BaseGenericInternalRow也衍生出3个子类，分别是GenericInternal-Row、SpecificInternalRow和MutableUnsafeRow类。其中，MutableUnsafeRow和UnsafeRow相关，用来支持对特定的列数据进行修改，这里暂时不作介绍。下面主要介绍GenericInternalRow和SpecificInternalRow。

阅读以上代码，可见GenericInternalRow构造参数是Array[Any]类型，采用对象数组进行底层存储，genericGet也是直接根据下标访问的。这里需要注意，数组是非拷贝的，因此一旦创建，就不允许通过set操作进行改变。而SpecificInternalRow则是以Array[MutableValue]为构造参数的，允许通过set操作进行修改。

### TreeNode体系 
从案例的转换过程可以看到，无论是逻辑计划还是物理计划，都离不开中间数据结构。在Catalyst中，对应的是TreeNode体系。TreeNode类是Spark SQL中所有树结构的基类，定义了一系列通用的集合操作和树遍历操作接口。

TreeNode内部包含一个Seq[BaseType]类型的变量children来表示孩子节点。TreeNode定义了foreach、map、collect等针对节点操作的方法，以及transformUp和transformDown等遍历节点并对匹配节点进行相应转换的方法。

TreeNode本身是scala.Product类型，因此可以通过productElement函数或productIterator迭代器对CaseClass参数信息进行索引和遍历。实际上，TreeNode一直在内存里维护，不会dum p到磁盘以文件形式存储，且无论在映射逻辑执行计划阶段，还是优化逻辑执行计划阶段，树的修改都是以替换已有节点的方式进行的。

TreeNode提供的仅仅是一种泛型，实际上包含了两个子类继承体系，即图3.5中的QueryPlan和Expression体系。Expression是Catalyst中的表达式体系，下一节会展开介绍。QueryPlan类下面又包含逻辑算子树（LogicalPlan）和物理执行算子树（SparkPlan）两个重要的子类，其中逻辑算子树在Catalyst中内置实现，可以剥离出来直接应用到其他系统中；而物理算子树SparkPlan和Spark执行层紧密相关，当Catalyst应用到其他计算模型时，可以进行相应的适配修改。

作为基础类，TreeNode本身仅提供了最简单和最基本的操作。图3.6列举了TreeNode中现有的一些方法，例如不同遍历方式的transform系列方法、用于替换新的子节点的w ithNewChildren方法等。此外，treeString函数能够将TreeNode以树型结构展示，在查看表达式、逻辑算子树和物理算子树时经常用到。

除上述操作外，Catalyst中还提供了节点位置功能，即能够根据TreeNode定位到对应的SQL字符串中的行数和起始位置。该功能在SQL解析发生异常时能够方便用户迅速找到出错的地方，具体参见如下代码。

可以看到，Origin提供了line和startPosition两个构造参数，分别代表行号和偏移量。在CurrentOrigin对象中，提供了各种set和get操作。其中，比较重要的是w ithOrigin方法，支持在TreeNode上执行操作的同时修改当前origin信息。

## Expression体系
表达式一般指的是不需要触发执行引擎而能够直接进行计算的单元，例如加减乘除四则运算、逻辑操作、转换操作、过滤操作等。如果说TreeNode是“框架”，那么Expression就是“灵魂”。在各种SQL引擎中，表达式（Expression）都起着重要的作用。

Catalyst实现了完善的表达式体系，与各种算子（QueryPlan）占据同样的地位。算子执行前通常都会进行“绑定”操作，将表达式与输入的属性对应起来，同时算子也能够调用各种表达式处理相应的逻辑。在Expression类中，主要定义了5个方面的操作，包括基本属性、核心操作、输入输出、字符串表示和等价性判断，如图3.7所示。

核心操作中的eval函数实现了表达式对应的处理逻辑，也是其他模块调用该表达式的主要接口，而genCode和doGenCode用于生成表达式对应的Java代码（这部分内容将在第9章中介绍）。字符串表示用于查看该Expression的具体内容，如表达式名和输入参数等。下面对Expression包含的基本属性和操作进行简单介绍。

- foldable：该属性用来标记表达式能否在查询执行之前直接静态计算。目前，foldable为true的情况有两种，第一种是该表达式为Literal类型（“字面值”，例如常量等），第二种是当且仅当其子表达式中foldable都为true时。当foldable为true时，在算子树中，表达式可以预先直接处理（“折叠”）。
- determ inistic：该属性用来标记表达式是否为确定性的，即每次执行eval函数的输出是否都相同。考虑到Spark分布式执行环境中数据的Shuffl e操作带来的不确定性，以及某些表达式（如Rand等）本身具有不确定性，该属性对于算子树优化中判断谓词能否下推等很有必要。
- nullable：该属性用来标记表达式是否可能输出Null值，一般在生成的Java代码中对相关条件进行判断。
- references：返回值为AttributeSet类型，表示该Expression中会涉及的属性值，默认情况为所有子节点中属性值的集合。
- canonicalized：返回经过规范化（Canonicalize）处理后的表达式。规范化处理会在确保输出结果相同的前提下通过一些规则对表达式进行重写，具体逻辑可以参见Canonicalize工具类。
- sem anticEquals：判断两个表达式在语义上是否等价。基本的判断条件是两个表达式都是确定性的（determ inistic为true）且两个表达式经过规范化处理后（Canonicalized）仍然相同。

在Spark SQL中，Expression本身也是TreeNode类的子类，因此能够调用所有TreeNode的方法，例如transform等，也可以通过多级的子Expression组合成复杂的Expression。Expression涉及范围广且数目庞大，相关的类或接口将近300个，这里列举一些比较常用的Expression来介绍。

- Nondeterm inistic接口：具有不确定性的Expression，其中determ inistic和foldable属性都默认返回false，典型的实现包括MonotonicallyIncreasingID表达式、Rand和Randn表达式等。
- Unevaluable接口：非可执行的表达式，即调用其eval函数会抛出异常。该接口主要用于生命周期不超过逻辑计划解析和优化阶段的表达式，例如Star(∗)表达式在解析阶段就会被展开成具体的列集合。
- CodegenFallback接口：不支持代码生成的表达式。某些表达式涉及第三方实现（例如Hive的UDF）等情况，无法生成Java代码，此时通过CodegenFallback直接调用，该接口中实现了具体的调用方法。
- LeafExpression：叶子节点类型的表达式，即不包含任何子节点，因此其children方法通常默认返回Nil值。该类型的Expression目前大约有30个，包括Star、CurrentDate、Pi表达式等。
- UnaryExpression：一元类型表达式，只含有一个子节点。这种类型的表达式总量110多种，较为庞大。其输入涉及一个子节点，例如，Abs操作、UpCast表达式等。
- BinaryExpression：二元类型表达式，包含两个子节点。这种类型的表达式数目也比较庞大，大约有80种。比较常用的是一些二元的算数表达式，例如加减乘除操作、RLike函数等。
- TernaryExpression：三元类型表达式，包含3个子节点。这种类型的表达式数目不多，大约有10种，大部分都是一些字符串操作的函数，非常典型的例子可以参考Substring函数，其子节点分别是字符串、下标和长度的表达式。

## 内部数据类型系统
数据类型系统是任何SQL引擎都必不可少的组成部分。数据类型主要用来表示数据表中存储的列信息，常见的数据类型包括简单的整数、浮点数、字符串，以及复杂的嵌套结构等。在Spark SQL中，Catalyst实现了完善的数据类型系统。

数据类型系统中类的相互继承关系如图3.9所示，所有的数据类型都继承自Abstract-DataType抽象类。比较常用的是各种NumericType类型，包括ByteType（表示一字节的整数，范围是-128～127）、ShortType（表示两字节的整数，范围是-32768～32767）、IntegerType（表示4字节的整数）、LongType（表示8字节的整数）、FloatType（表示4字节的单精度浮点数）和DoubleType（表示8字节的双精度浮点数）等。另外，DecimalType是需要特别注意的数据类型，可以用来表示不可变的任意精度的十进制数字，依托内部的java.math.BigDecimal，支持常规的四则运算和UDF（例如round和floor等）。使用DecimalType进行转换操作时需要注意precision和scale值的选取，其中scale表示小数部分位数，precision-scale表示整数部分的位数。

常用的复合数据类型有数组类型（ArrayType）、字典类型（MapType）和结构体类型（StructType）3种。其中，数组类型（ArrayType）中要求数组元素类型一致；字典类型（MapType）中既要求所有key的类型一致，也要求所有的value类型一致。

综上所述，Spark SQL中提供了丰富的数据类型。在实际应用中，可以根据具体的应用需求来选取合适的数据类型（基本类型、复合类型），特别需要注意各种数据类型表示的范围和数据溢出情况的处理。



















