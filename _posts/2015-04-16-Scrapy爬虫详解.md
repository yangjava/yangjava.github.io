---
layout: post
categories: [Python]
description: none
keywords: Python
---
# Scrapy爬虫详解

## Scrapy网络爬虫基础
首先介绍Scrapy中最重要的组件爬虫（Spider），它用于构建HTTP请求并从网页中提取数据；接着介绍使用Item封装数据；最后介绍使用Pipeline组件对数据进行处理，如数据清理、去重及持久化存储等。

## 使用Spider提取数据
Scrapy网络爬虫编程的核心就是爬虫（Spider）组件，它其实是一个继承于Spider的类，主要功能是封装一个发送给网站服务器的HTTP请求，解析网站返回的网页及提取数据。

下面从数据流的角度分析一下执行步骤：
- Spider生成初始页面请求（封装于Request对象中），提交给引擎。
- 引擎通知下载器按照Request的要求，下载网页文档，再将文档封装成Response对象作为参数传回给Spider。
- Spider解析Response中的网页内容，生成结构化数据（Item），或者产生新的请求（如爬取下一页），再次发送给引擎。
- 如果发送给引擎的是新的Request，就回到第（2）步继续往下执行。如果发送的是结构化数据（Item），则引擎通知其他组件处理该数据（保存到文件或数据库中）。

以实现起点中文网小说热销榜为例，打开Spiders目录下的qidian_hot_spider.py，实现代码如下：
```python
#-*-coding:utf-8-*-
from scrapy import Request
from scrapy.spiders import Spider                               #导入Spider类
class HotSalesSpider(Spider):
  #定义爬虫名称
  name = 'hot'
  #起始的URL列表
  start_urls = ["https://www.qidian.com/rank/hotsales?style=1"]
  # 解析函数
  def parse(self, response):
      #使用xpath定位到小说内容的div元素，保存到列表中
      list_selector = response.xpath("//div[@class='book-mid-info']")
      #依次读取每部小说的元素，从中获取小说名称、作者、类型和形式
      for one_selector in list_selector:
          #获取小说名称
          name = one_selector.xpath("h4/a/text()").extract()[0]
          #获取作者
          author =  one_selector.xpath("p[1]/a[1]/text()").extract()[0]
          #获取类型
          type = one_selector.xpath("p[1]/a[2]/text()").extract()[0]
          #获取形式（连载还是完本）
          form = one_selector.xpath("p[1]/span/text()").extract()[0]
          #将爬取到的一部小说保存到字典中
          hot_dict = {"name":name,                                      #小说名称
                      "author":author,                                  #作者
                      "type":type,                                      #类型
                      "form":form}                                      #形式
          #使用yield返回字典
          yield hot_dict
```
首先从scrapy模块导入了两个模块：Request和Spider；然后定义了一个继承于Spider的类HotSalesSpider。该类的结构很简单，有两个属性name和start_urls，两个方法parse()和start_requests()，它们都是基类Spider的属性和方法。

下面来看一下它们各自的功能。
- name：必填项。name是区分不同爬虫的唯一标识，因为一个Scrapy项目中允许有多个爬虫。不同的爬虫，name值不能相同。
- start_urls：存放要爬取的目标网页地址的列表。
- start_requests()：爬虫启动时，引擎自动调用该方法，并且只会被调用一次，用于生成初始的请求对象（Request）。start_requests()方法读取start_urls列表中的URL并生成Request对象，发送给引擎。引擎再指挥其他组件向网站服务器发送请求，下载网页。代码中之所以没看到start_requests()方法，是因为我们没有重写它，直接使用了基类的功能。
- parse()：Spider类的核心方法。引擎将下载好的页面作为参数传递给parse()方法，parse()方法执行从页面中解析数据的功能。

### 重写start_requests()方法
Spider的结构非常简单，不难理解，但是你一定有这样的疑问：
- 如何避免爬虫被网站识别出来导致被禁呢？
- 引擎是怎么知道要将下载好的页面发送给parse()方法而不是其他方法？能否自定义这个方法？
  第一个问题的答案是可以重写（override）start_requests()方法，手动生成一个功能更强大的Request对象。因为伪装浏览器、自动登录等功能都是在Request对象中设置的。
  第二个问题的答案是引擎之所以能自动定位，是因为在Request对象中，指定了解析数据的回调函数，而默认情况下，Request指定的解析函数就是parse()方法。

下面我们就来重写start_requests()方法，对起点中文网小说热销榜的功能做一些优化。优化内容有：
- 将爬虫伪装成浏览器。
- 设置新的解析数据的回调函数（不使用默认的parse()）。

实现代码如下：
```python
#-*-coding:utf-8-*-
from scrapy import Request
from scrapy.spiders import Spider#导入Spider类
class HotSalesSpider(Spider):
  #定义爬虫名称
  name = 'hot'
  #设置用户代理（浏览器类型）
  qidian_headers = {"User-Agent":"Mozilla/"
                  "5.0 (Windows NT 10.0; "
                  "Win64; x64) AppleWebKit/"
                  "537.36 (KHTML, like Gecko) Chrome/"
                  "68.0.3440.106 Safari/"
                  "537.36"}
  #获取初始Request
  def start_requests(self):
      url = "https://www.qidian.com/rank/hotsales?style=1"
      #生成请求对象，设置url，headers，callback
      yield Request(url,headers=self.qidian_headers,callback=self.qidian_
          parse)
  # 解析函数
  def qidian_parse(self, response):
  ……
```
在类HotSalesSpider中，新增一个字典型的属性qidian_headers，用于设置请求头信息。这里设置的User-Agent，就是用于伪装浏览器。
另外，代码中删除了属性start_urls，并重写了start_requests()方法，用于自定义Request对象。Request对象设置了3个参数：
- url：请求访问的网址。
- headers：请求头信息。
- callback：回调函数。这里确定解析数据的函数为qidian_parse()。引擎会将下载好的页面（Response对象）发送给该方法，执行数据解析功能。
  解析函数由parse()改为qidian_parse()，实现代码未变。

## Request对象
Request对象用来描述一个HTTP请求，它通常在Spider中生成并由下载器执行。

Request的定义形式为：
```python
class scrapy.http.Request(url [，callback，method ='GET'，headers，body，
cookies，meta，encoding ='utf-8'，priority = 0，dont_filter = False，errback ])
```
其中，参数url为必填项，其他为选填项，下面逐个介绍这些参数。
以下参数用于设置向网站发送的HTTP请求的内容，你一定不会感到陌生。
- url：HTTP请求的网址，如https://baidu.com。
- method：HTTP请求的方法，如GET、POST、PUT等，默认为GET，必须大写英文字母。
- body：HTTP的请求体，类型为str或unicode。
- headers：HTTP的请求头，类型为字典型。请求头包含的内容可以参考2.1.3节HTTP请求。
- cookies：请求的Cookie值，类型为字典型或列表型，可以实现自动登录的效果，后面章节会具体讲解。
- encoding：请求的编码方式，默认为UTF-8。
  以下参数设置Scrapy框架内部的事务。
- callback：指定回调函数，即确定页面解析函数，默认为parse()。页面下载完成后，回调函数将会被调用，如果在处理期间发生异常，则会调用errback()函数。
- meta：字典类型，用于数据的传递。它可以将数据传递给其他组件，也可以传递给Respose对象，本章的项目案例中会使用到该参数。
- priority：请求的优先级，默认为0，优先级高的请求会优先下载。
- dont_filter：如果对同一个url多次提交相同的请求，可以使用此项来忽略重复的请求，避免重复下载，其值默认为False。如果设置为True，即使是重复的请求，也会强制下载，例如爬取实时变化的股票信息数据。
- errback：在处理请求时引发任何异常时调用的函数，包括HTTP返回的404页面不存在的错误。

Request中的参数看上去有很多，但除了url外，其他参数都有默认值，大部分情况下不必设置。

### 使用选择器提取数据
Scrapy提取数据有自己的一套机制，被称做选择器（Selector类），它能够自由“选择”由XPath或CSS表达式指定的HTML文档的某些部分。Scrapy的选择器短小简洁、解析快、准确性高，使用其内置的方法可以快速地定位和提取数据。
下面就来了解一下选择器（Selector类）及选择器列表（SelectorList类，选择器对象的集合）内置的方法。

定位数据
- xpath(query)：查找与XPath表达式匹配的节点，并返回一个SelectorList对象。SelectorList对象类似于一个列表，包含了所有匹配到的节点。参数query是XPath表达式的字符串。
- css(query)：查找与CSS表达式匹配的节点，并返回一个SelectorList对象。参数query是CSS表达式的字符串。

提取数据
- extract()：提取文本数据，返回unicode字符串列表。使用xpath()或css()方法将匹配到的节点包装为SelectorList对象后，可以使用extract()方法提取SelectorList对象中的文本，并将其存放于列表中。
- extract_first()：SelectorList独有的方法，提取SelectorList对象中第一个文本数据，返回unicode字符串。
- re(regex)：使用正则表达式提取数据，返回所有匹配的unicode字符串列表。
- re_first()：SelectorList独有的方法，提取第一个与正则表达式匹配的字符串。

### Response对象与XPath
我们完全没有必要手动构造一个选择器对象来实现对网页信息的查找与提取。因为Scrapy将下载下来的网页信息封装为Response对象传递给解析函数时，会自动构造一个选择器作为Response对象的属性，这样就能通过Response对象非常方便地查找与提取网页数据。

下面再来分析一下解析函数，函数框架如下：
```python
# 解析函数
def qidian_parse(self, response):
……
```
参数response接收封装有网页信息的Response对象，这时就可以使用下面的方法实现对数据的定位。
- response.selector.xpath(query)；
- response.selector.css(query)。
  由于在Response中使用XPath和CSS查询十分普遍，因此Response对象提供了两个实用的快捷方式，它们能自动创建选择器并调用选择器的xpath()或css()方法来定位数据。简化后的方法如下：
- response.xpath(query)；
- response.css(query)。

### Response对象
Response用来描述一个HTTP响应，它只是一个基类。当下载器下载完网页后，下载器会根据HTTP响应头部的Content-Type自动创建Response的子类对象。子类主要有：
- TextResponse；
- HtmlResponse；
- XmlResponse。

其中，TextResponse是HtmlResponse和XmlResponse的子类。我们通常爬取的是网页，即HTML文档，下载器创建的便是HtmlResponse。

下面以HtmlResponse为例，介绍它的属性。
- url：响应的url，只读，如https://www.baidu.com。
- status：HTTP响应的状态码，如200、403、404。状态码可以参考2.1.4节HTTP响应。
- headers：HTTP的响应头，类型为字典型。具体内容可以参考2.1.4节HTTP响应。
- body：HTTP响应体。具体内容可以参考2.1.4节HTTP响应。
- meta：用于接收传递的数据。使用request.meta将数据传递出去后，可以使用response.meta获取数据。

### 多页数据的爬取
在解析函数中，提取完本页数据并提交给引擎后，设法提取到下一页的URL地址，使用这个URL地址生成一个新的Request对象，再提交给引擎。也就是说，解析本页的同时抛出一个下一页的请求，解析下一页时抛出下下页的请求，如此递进，直到最后一页。
```python
#-*-coding:utf-8-*-
from scrapy import Request
from scrapy.spiders import Spider                       #导入Spider类
class HotSalesSpider(Spider):
  #定义爬虫名称
  name = 'hot'
  current_page = 1                                                      #设置当前页，起始为1
  #获取初始Request
  def start_requests(self):
      url = "https://www.qidian.com/rank/hotsales?style=1"
      #生成请求对象，设置url、headers和callback
      yield Request(url,callback=self.qidian_parse)
  #解析函数
  def qidian_parse(self, response):
      #使用xpath定位到小说内容的div元素，并保存到列表中
      list_selector = response.xpath("//div[@class='book-mid-info']")
      #依次读取每部小说的元素，从中获取小说名称、作者、类型和形式
      for one_selector in list_selector:
          #获取小说名称
          name = one_selector.xpath("h4/a/text()").extract_first()
          #获取作者
          author = one_selector.xpath("p[1]/a[1]/text()").extract()[0]
          #获取类型
          type = one_selector.xpath("p[1]/a[2]/text()").extract()[0]
          #获取形式（连载还是完本）
          form = one_selector.xpath("p[1]/span/text()").extract()[0]
          #将爬取到的一部小说保存到字典中
          hot_dict = {"name":name,                      #小说名称
                      "author":author,                  #作者
                      "type":type,                      #类型
                      "form":form}                      #形式
          #使用yield返回字典
          yield hot_dict
      #获取下一页URL，并生成Request请求，提交给引擎
      #1.获取下一页URL
      self.current_page+=1
      if self.current_page<=25:
          next_url = "https://www.qidian.com/rank/hotsales?style=1&page=
          %d"%(self.current_page)
          #2.根据URL生成Request，使用yield返回给引擎
          yield Request(next_url,callback=self.qidian_parse)
```
以上代码看着多，其实仅增加了加粗代码部分，下面来分析一下这些代码。
- 属性current_page，用于记录当前的页码，初始值为1。
- 通过分析得知，第N页的URL地址为https://www.qidian.com/rank/hotsales?style=1&page=N，即只有page的值是变化（递增）的。获取下一页的URL就变得简单了。
- 根据下一页的URL，构建一个Request对象，构建方法和start_requests()中Request对象构建方法一样，仅仅是URL不同。

### 使用Item封装数据
Item对象是一个简单的容器，用于收集抓取到的数据，其提供了类似于字典（dictionary-like）的API，并具有用于声明可用字段的简单语法。

以起点中文网小说热销榜项目qidian_hot为例，在新建项目时，自动生成的items.py文件，就是用于封装数据的。之所以叫items，是因为源文件中可以定义多种Item，其原始代码为：
```python
import scrapy
class QidianHotItem(scrapy.Item):
  # define the fields for your item here like:
  # name = scrapy.Field()
  pass
```
已知需要爬取的小说的字段有小说名称、作者、类型和形式。在类QidianHotItem中声明这几个字段，代码如下：
```python
import scrapy
#保存小说热销榜字段数据
class QidianHotItem(scrapy.Item):
  # define the fields for your item here like:
  name = scrapy.Field()                                 #小说名称
  author = scrapy.Field()                               #作者
  type = scrapy.Field()                                 #类型
  form = scrapy.Field()                                 #形式
```
下面分析一下代码：
- 类QidianHotItem继承于Scrapy的Item类。
- name、author、type、form为小说的各个字段名。
- scrapy.Field()生成一个Field对象，赋值给各自的字段。
- Field对象用于指定每个字段的元数据，并且Field对象对接受的数据没有任何限制。因此，在定义属性字段时，无须考虑它的数据类型，使用起来非常方便。

下面修改HotSalesSpider类中的代码，使用QidianHotItem替代Python字典存储数据，实现代码如下：
```python
#-*-coding:utf-8-*-
from scrapy import Request
from scrapy.spiders import Spider                                       #导入Spider类
from qidian_hot.items import QidianHotItem                      #导入模块
class HotSalesSpider(Spider):
  ……
  # 解析函数
  def qidian_parse(self, response):
      #使用xpath定位到小说内容的div元素，并保存到列表中
      list_selector = response.xpath("//div[@class='book-mid-info']")
      #依次读取每部小说的元素，从中获取小说名称、作者、类型和形式
      for one_selector in list_selector:
          #获取小说名称
          name = one_selector.xpath("h4/a/text()").extract_first()
          #获取作者
          author = one_selector.xpath("p[1]/a[1]/text()").extract()[0]
          #获取类型
          type = one_selector.xpath("p[1]/a[2]/text()").extract()[0]
          #获取形式（连载还是完本）
          form = one_selector.xpath("p[1]/span/text()").extract()[0]
          #将爬取到的一部小说保存到item中
          item = QidianHotItem()                        #定义QidianHotItem对象
          item["name"] = name                           #小说名称
          item["author"] = author                       #作者
          item["type"] = type                           #类型
          item["form"] = form                           #形式
          #使用yield返回item
          yield item
      #获取下一页URL，并生成Request请求提交给引擎
      ……
```
以上代码分析如下：
- 首先导入qidian_hot.items下的QidianHotItem模块。
- 生成QidianHotItem的对象item，用于保存一部小说信息。
- 将从页面中提取到的各个字段赋给item。赋值方法跟Pyton的字典一样，使用key-value的形式。key要与在QidianHotItem中定义的名称一致，否则会报错，value为各个字段值。Item复制了标准的字典API，因此可以按照字典的形式赋值。

### 使用ItemLoader填充容器
目前为止我们爬取的数据的字段较少，但是当项目很大、提取的字段数以百计时，数据的提取规则也会越来越多，再加上还要对提取到的数据做转换处理，代码就会变得庞大，维护起来十分困难。

为了解决这个问题，Scrapy提供了项目加载器（ItemLoader）这样一个填充容器。通过填充容器，可以配置Item中各个字段的提取规则，并通过函数分析原始数据，最后对Item字段赋值，使用起来非常便捷。

Item和ItemLoader的区别在于：

- Item提供了保存抓取到的数据的容器，需要手动将数据保存于容器中。
- Itemloader提供的是填充容器的机制。

下面使用ItemLoader来改写起点中文网小说热销榜的项目。打开爬虫（Spider）源文件qidian_hot.py。

导入ItemLoader类

导入ItemLoader类，代码如下：
```
from scrapy.loader import ItemLoader                    #导入ItemLoader类
```

实例化ItemLoader对象

在使用ItemLoader之前，必须先将其实例化。来看一下数据解析函数qidian_parse()的实现代码：
```
#解析函数
def qidian_parse(self, response):
  #使用xpath定位到小说内容的div元素，并保存到列表中
  list_selector = response.xpath("//div[@class='book-mid-info']")
  #依次读取每部小说的元素，从中获取小说名称、作者、类型和形式
  for one_selector in list_selector:
      #生成ItemLoader的实例
      #参数item接收QidianHotItem实例，selector接收一个选择器
      novel = ItemLoader(item=QidianHotItem(),selector=one_selector)
      #使用XPath选择器获取小说名称
      novel.add_xpath("name","h4/a/text()")
      #使用XPath选择器获取作者
      novel.add_xpath("author","p[1]/a[1]/text()")
      #使用XPath选择器获取类型
      novel.add_xpath("type","p[1]/a[2]/text()")
      #使用CSS选择器获取小说形式（连载还是完本）
      novel.add_css("form",".author span::text")
      #将提取好的数据load出来，并使用yield返回
      yield novel.load_item()
```
很明显，使用ItemLoader实现的Spider功能，代码量更少、更加清晰。因为ItemLoader将数据提取与数据封装的功能全实现了。

在实例化ItemLoader时，ItemLoader接收一个Item实例来指定要加载的Item（参数item）；指定response或者selector来确定要解析的内容（参数response或selector）。

使用ItemLoader填充数据

实例化ItemLoader对象后，就要开始提取数据到ItemLoader中了。ItemLoader提供了3种重要的方法将数据填充进来。

- add_xpath()：使用XPath选择器提取数据。
- add_css()：使用CSS选择器提取数据。
- add_value()：直接传值。

以上3个方法中都有两个参数，第一个参数指定字段名，第二个参数指定对应的提取规则或者传值。

给Item对象赋值

当提取到的数据被填充到ItemLoader后，还需要调用load_item()方法给Item对象赋值。

## 使用Pipeline处理数据
Scrapy的Item Pipeline（项目管道）是用于处理数据的组件。

## Item Pipeline介绍
当Spider将收集到的数据封装为Item后，将会被传递到Item Pipeline（项目管道）组件中等待进一步处理。Scrapy犹如一个爬虫流水线，Item Pipeline是流水线的最后一道工序，但它是可选的，默认关闭，使用时需要将它激活。如果需要，可以定义多个Item Pipeline组件，数据会依次访问每个组件，执行相应的数据处理功能。

以下为Item Pipeline的典型应用：
- 清理数据。
- 验证数据的有效性。
- 查重并丢弃。
- 将数据按照自定义的格式存储到文件中。
- 将数据保存到数据库中。

编写自己的Item Pipeline

编写自己的Item Pipeline组件其实很简单，它只是一个实现了几个简单方法的Python类。当建立一个项目后，这个类就已经自动创建了。打开项目qidian_hot下的pipelines.py，发现自动生成了如下代码：
```
class QidianHotPipeline(object):
  def process_item(self, item, spider):
      return item
```

下面在方法process_item()中，实现数据处理的功能。
```
class QidianHotPipeline(object):
  def process_item(self, item, spider):
      #判断小说形式是连载还是完结
      if item["form"] == "连载":                  #连载的情况
          item["form"] = "LZ"                           #替换为简称
      else:#其他情况
          item["form"] = "WJ"
      return item
```
QidianHotPipeline是自动生成的Item Pipeline类，它无须继承特定的基类，只需要实现某些特定的方法，如process_item()、open_spider()和close_spider()。注意，方法名不可改变。

process_item()方法是Item Pipeline类的核心方法，必须要实现，用于处理Spider爬取到的每一条数据（Item）。它有两个参数：

item：待处理的Item对象。

spider：爬取此数据的Spider对象。

方法的返回值是处理后的Item对象，返回的数据会传递给下一级的Item Pipeline（如果有）继续处理。

## 启用Item Pipeline
在Scrapy中，Item Pipeline是可选组件，默认是关闭的。要想激活它，只需在配置文件settings.py中启用被注释掉的代码即可。
```
# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
 'qidian_hot.pipelines.QidianHotPipeline': 300,
}
```
ITEM_PIPELINES是一个字典，将想要启用的Item Pipeline添加到这个字典中。其中，键是Item Pipeline类的导入路径，值是一个整数值。如果启用多个Item Pipeline，这些值就决定了它们运行的顺序，数值越小，优先级越高。

下面来看一下运行多个Item Pipeline的例子。

多个Item Pipeline
如果有这样一个需求，同一个作者只能上榜一部作品，而爬取到的数据中可能有多部同一作者的作品。因此可以实现一个去重处理的Item Pipeline，将重复数据过滤掉。在pipelines.py中，定义一个去重的Item Pipeline类DuplicatesPipeline，实现代码如下：
```
from scrapy.exceptions import DropItem
#去除重复作者的Item Pipeline
class DuplicatesPipeline(object):
  def __init__(self):
      #定义一个保存作者姓名的集合
      self.author_set = set()
  def process_item(self, item, spider):
      if item['author'] in self.author_set:
          #抛弃重复的Item项
          raise DropItem("查找到重复姓名的项目: %s"%item)
      else:
          self.author_set.add(item['author'])
      return item

```

在构造函数__init__()中定义一个保存作者姓名的集合author_set。

在process_item()方法中，判断item中的author字段是否已经存在于集合author_set中，如果不存在，则将item中的author字段存入author_set集合中；如果存在，就是重复数据，使用raise抛出一个DropItem异常，该Item就会被抛弃，不会传递给后面的Item Pipeline继续处理，更不会导出到文件中。

在配置文件settings.py中启用DuplicatesPipeline：
```
# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
 'qidian_hot.pipelines.DuplicatesPipeline': 100,
 'qidian_hot.pipelines.QidianHotPipeline': 300,
}
```
DuplicatesPipeline设置的值比QidianHotPipeline小，因此优先执行DuplicatesPipeline，过滤掉重复项，再将非重复项传递给QidianHotPipeline继续处理。

## 保存为其他类型文件
之前我们都是通过命令将数据保存为CSV文件。但是，如果要将数据保存为TXT文件，并且字段之间使用其他间隔符（例如分号），使用命令就无法实现了。Scrapy中自带的支持导出的数据类型有CSV、JSON和XML等，如果有特殊要求，需要自己实现。

下面通过Item Pipeline实现将数据保存为文本文档（txt），并且字段之间使用分号（；）间隔的功能。在pipelines.py中，定义一个保存数据的Item Pipeline类SaveToTxtPipeline，实现代码如下：
```
#将数据保存于文本文档中的Item Pipeline
class SaveToTxtPipeline(object):
  file_name = "hot.txt"                         #文件名称
  file = None                                           #文件对象
  #Spider开启时，执行打开文件操作
  def open_spider(self,spider):
      #以追加形式打开文件
      self.file = open(self.file_name,"a",encoding="utf-8")
  #数据处理
  def process_item(self, item, spider):
      #获取item中的各个字段，将其连接成一个字符串
      # 字段之间用分号隔开
      # 字符串末尾要有换行符\n
      novel_str = item['name']+";"+\
                  item["author"]+";"+\
                  item["type"]+";"+\
                  item["form"]+"\n"
      #将字符串写入文件中
      self.file.write(novel_str)
      return item
  #Spider关闭时，执行关闭文件操作
  def close_spider(self,spider):
      #关闭文件
      self.file.close()
```
类SaveToTxtPipeline中多了几个方法，下面一起来了解一下。

Item Pipeline中，除了必须实现的process_item()方法外，还有3个比较常用的方法，可根据需求选择实现。

- open_spider(self,spider)方法：当Spider开启时（爬取数据之前），该方法被调用，参数spider为被开启的Spider。该方法通常用于在数据处理之前完成某些初始化工作，如打开文件和连接数据库等。
- close_spider(self,spider)方法：当Spider被关闭时（所有数据被爬取完毕），该方法被调用，参数spider为被关闭的Spider。该方法通常用于在数据处理完后，完成某些清理工作，如关闭文件和关闭数据库等。
- from_crawler(cls,crawler)方法：该方法被调用时，会创建一个新的Item Pipeline对象，参数crawler为使用当前管道的项目。该方法通常用于提供对Scrapy核心组件的访问，如访问项目设置文件settings.py。

下面再来看一下SaveToTxtPipeline实现的思路。
- 属性file_name定义文件名称，属性file定义文件对象，便于操作文件。
- open_spider()方法实现文件的打开操作，在数据处理前执行一次。 
- close_spider()方法实现文件的关闭操作。在数据处理后执行一次。
- process_item()方法实现数据的写入操作。首先，获取item中的所有字段，将它们连成字符串，字段之间用分号间隔，而且字符串末尾要加上换行符（\n），实现一行显示一条数据；然后，使用Python的write()函数将数据写入文件中。

在配置文件settings.py中启用SaveToTxtPipeline：
```
# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
 'qidian_hot.pipelines.DuplicatesPipeline': 100,
 'qidian_hot.pipelines.QidianHotPipeline': 300,
 'qidian_hot.pipelines.SaveToTxtPipeline': 400,
}
```

为了便于管理，Scrapy中将各种配置信息放在了配置文件settings.py中。上面的文件名也可以转移到settings.py中配置，下面来看一下修改方法。

在settings.py中设置文件名称。
```
FILE_NAME = "hot.txt"
```

在SaveToTxtPipeline中获取配置信息。
```
#将数据保存于文本文档中的Item Pipeline中
class SaveToTxtPipeline(object):
  file = None              #文件对象
  @classmethod
  def from_crawler(cls,crawler):
      #获取配置文件中的FILE_NAME的值
      #如果获取失败，就使用默认值"hot2.txt"
      cls.file_name = crawler.settings.get("FILE_NAME","hot2.txt")
      return cls()
……
```
SaveToTxtPipeline增加了方法from_crawler()，用于获取配置文件中的文件名。注意，该方法是一个类方法（@classmethod），Scrapy会调用该方法来创建SaveToTxtPipeline对象，它有两个参数：

- cls：SaveToTxtPipeline对象。
- crawler：Scrapy中的核心对象，通过它可以访问配置文件（settings.py）。

from_crawler()方法必须返回一个新生成的SaveToTxtPipeline对象（return cls()）。







































