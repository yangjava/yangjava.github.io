---
layout: post
categories: [Spark]
description: none
keywords: Spark
---
# Spark源码执行环境

## 执行环境
Spark对任务的计算都依托于Executor的能力，所有的Executor都有自己的Spark执行环境SparkEnv。有了SparkEnv，就可以将数据存储在存储体系中；就能利用计算引擎对计算任务进行处理，就可以在节点间进行通信等。SparkEnv还提供了多种多样的内部组件，实现不同的功能。SparkEnv是一个很重要的组件，虽然在创建SparkContext的时候也涉及它（只是因为local模式的需要），但是它与Executor的关系则更为紧密。

## 启动集群
常见的Spark运行方式有本地模式、集群模式。本地模式所有的处理都运行在同一个JVM中，而后者，可以运行在不同节点上。

Spark支持Scala或Python的REPL（Read-Eval-Print-Loop，交互式shell）来进行交互式程序编写，交互式编程在输入的代码执行后立即能看到结果，非常友好和方便。

在2.0之前的Spark版本中，Spark shell会自动创建一个SparkContext对象sc。SparkContext起中介的作用，通过它来使用Spark其他的功能。每一个JVM都有一个对应的SparkContext，Driver Program通过SparkContext连接到集群管理器来实现对集群中任务的控制。Spark配置参数的设置以及对SQLContext、HiveContext和StreamingContext的控制也要通过SparkContext。

不过在Spark 2.0中引入了SparkSession对象（spark），运行Spark shell则会自动创建一个SparkSession对象，在输入spark时就会发现它已经存在了（见图2-3）、SparkConf、SparkContext和SQLContext都已经被封装在SparkSession当中，它为用户提供了一个统一的切入点，同时也提供了各种DataFrame和Dataset的API，大大降低了学习Spark的难度。

启动Spark集群的界面，编程语言是Scala，如果希望使用Python为编辑语句，该如何启动呢？运行Pyspark即可。

## SparkEnv概述
SparkEnv的私有方法create用于创建SparkEnv，由于create方法涉及很多SparkEnv内部组件的实例化过程，所以首先了解SparkEnv的组成。

### 安全管理器SecurityManager
SecurityManager主要对账号、权限及身份认证进行设置和管理。如果Spark的部署模式为YARN，则需要生成secret key（密钥）并存入Hadoop UGI。而在其他模式下，则需要设置环境变量_SPARK_AUTH_SECRET（优先级更高）或spark.authenticate.secret属性指定secret key（密钥）。SecurityManager还会给当前系统设置默认的口令认证实例。在SparkEnv中创建SecurityManager的代码如下。
```
val securityManager = new SecurityManager(conf, ioEncryptionKey)
```
SecurityManager内部有很多属性，我们先来对这些属性进行介绍。
- authOn：是否开启认证。可以通过spark.authenticate属性配置，默认为false。
- aclsOn：是否对账号进行授权检查。可通过spark.acls.enable（优先级较高）或spark.ui.acls.enable（此属性是为了向前兼容）属性进行配置。aclsOn的默认值为false。
- adminAcls：管理员账号集合。可以通过spark.admin.acls属性配置，默认为空。
- adminAclsGroups：管理员账号所在组的集合。可以通过spark.admin.acls.groups属性配置，默认为空。
- viewAcls：有查看权限的账号的集合。包括adminAcls、defaultAclUsers及spark.ui.view.acls属性配置的用户。
- viewAclsGroups：拥有查看权限的账号，所在组的集合。包括adminAclsGroups和spark.ui.view.acls.groups属性配置的用户。
- modifyAcls：有修改权限的账号的集合。包括adminAcls、defaultAclUsers及spark.modify.acls属性配置的用户。
- modifyAclsGroups：拥有修改权限的账号所在组的集合。包括adminAclsGroups和spark.modify.acls.groups属性配置的用户。
- defaultAclUsers：默认用户。包括系统属性user.name指定的用户或系统登录用户或者通过系统环境变量SPARK_USER进行设置的用户。
- secretKey：密钥。在YARN模式下，首先使用sparkCookie从Hadoop UGI中获取密钥。如果Hadoop UGI没有保存密钥，则生成新的密钥（密钥长度可以通过spark.authenticate.secretBitLength属性指定）并存入Hadoop UGI。其他模式下，则需要设置环境变量_SPARK_AUTH_SECRET（优先级更高）或spark.authenticate.secret属性指定。

SecurityManager中设置了默认的口令认证实例Authenticator，此实例采用匿名内部类实现，用于每次使用HTTP client从HTTP服务器获取用户的用户名和密码。这是由于Spark的节点间通信往往需要动态协商用户名、密码，这种方式灵活地支持了这种需求。设置默认身份认证器的代码如下。

```
if (authOn) { 
  // 设置默认的口令认证实例Authenticator，它的getPasswordAuthentication方法用于获取用户名、密码
  Authenticator.setDefault(
    new Authenticator() {
      override def getPasswordAuthentication(): PasswordAuthentication = {
        var passAuth: PasswordAuthentication = null
        val userInfo = getRequestingURL().getUserInfo()
        if (userInfo != null) {
          val  parts = userInfo.split(":", 2)
          passAuth = new PasswordAuthentication(parts(0), parts(1).toCharArray())
        }
        return passAuth
      }
    }
  )
}

```

### RPC环境
RpcEnv是Spark 2.x.x版本中新出现的组件，那么它是不是给Spark带来了新的能力呢？对于有些组件是这样的，但是RpcEnv组件肩负着另一项历史使命——替代Spark 2.x.x以前版本中采用的Akka。Spark移除了对Akka的使用，这令我感到困惑，“Akka是一款很不错的分布式消息系统，为什么要替代？正如3.2节所说的——可以让用户使用任何版本的Akka来编程”。Akka具有分布式集群下的消息发送、远端同步调用、远端异步调用、路由、持久化、监管、At Least Once Delivery（至少投递一次）等能力，一旦Akka被替代，那就意味着RpcEnv必须也能支持这些机制。

现在我们来看看SparkEnv中的RpcEnv是如何被创建的，SparkEnv中创建RpcEnv的代码如下。
```
val systemName = if (isDriver) driverSystemName else executorSystemName
val rpcEnv = RpcEnv.create(systemName, bindAddress, advertiseAddress, port, conf,
  securityManager, clientMode = !isDriver)
```
这段代码首先生成系统名称systemName，如果当前应用为Driver（即SparkEnv位于Driver中），那么systemName为sparkDriver，否则（即SparkEnv位于Executor中），systemName为sparkExecutor。然后调用RpcEnv的create方法创建RpcEnv。create方法中只有两条语句。

```
 val config = RpcEnvConfig(conf, name, bindAddress, advertiseAddress, port, securityManager,
    clientMode) // RpcEnvConfig中保存了RpcEnv的配置信息
  new NettyRpcEnvFactory().create(config) // 创建RpcEnv
```
这里的RpcEnvConfig实际是一个样例类，用于保存RpcEnv的配置信息。实际创建RpcEnv的动作发生在NettyRpcEnvFactory的create方法中。

创建NettyRpcEnv
```
def create(config: RpcEnvConfig): RpcEnv = {
  val sparkConf = config.conf
  val javaSerializerInstance =
    new JavaSerializer(sparkConf).newInstance().asInstanceOf[JavaSerializerInstance]
  val nettyEnv =
    new NettyRpcEnv(sparkConf, javaSerializerInstance, config.advertiseAddress,
      config.securityManager)
  if (!config.clientMode) {
    val startNettyRpcEnv: Int => (NettyRpcEnv, Int) = { actualPort =>
      nettyEnv.startServer(config.bindAddress, actualPort)
      (nettyEnv, nettyEnv.address.port)
    }
    try {
      Utils.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1
    } catch {
      case NonFatal(e) =>
        nettyEnv.shutdown()
        throw e
    }
  }
  nettyEnv
}
}
```
创建NettyRpcEnv的步骤如下。
- 创建javaSerializerInstance。此实例将用于RPC传输对象的序列化。
- 创建NettyRpcEnv。创建NettyRpcEnv其实就是对内部各个子组件TransportConf、Dispatcher、TransportContext、TransportClientFactory、TransportServer的实例化过程，这些内容都将在后面一一介绍。
- 启动NettyRpcEnv。这一步首先定义了一个偏函数startNettyRpcEnv，其函数实际为执行NettyRpcEnv的startServer方法（startServer方法的具体内容将在5.3.7节进行介绍），最后在启动NettyRpcEnv之后返回NettyRpcEnv及服务最终使用的端口。这里使用了Utils的startServiceOnPort方法，startServiceOnPort实际上是调用了作为参数的偏函数startNettyRpcEnv，有关startServiceOnPort的具体介绍可以参阅附录A中的内容。

由于抽象类RpcEnv只有一个实现子类NettyRpcEnv，所以本章不打算展示RpcEnv的接口定义，而是直接从介绍NettyRpcEnv入手。在正式介绍NettyRpcEnv的构造过程之前，我们需要先做一些准备。RpcEndpoint和RpcEndpointRef都是NettyRpcEnv中的重要概念，我们需要首先理解它们，再来看NettyRpcEnv的构造过程。

### RPC端点RpcEndpoint
RPC端点是对Spark的RPC通信实体的统一抽象，所有运行于RPC框架之上的实体都应该继承RpcEndpoint。Spark早期版本节点间的消息通信主要采用Akka的Actor，从Spark 2.0.0版本开始移除了对Akka的依赖，这就意味着Spark需要Actor的替代品，RPC端点RpcEndpoint由此而生。RpcEndpoint是对能够处理RPC请求，给某一特定服务提供本地调用及跨节点调用的RPC组件的抽象。




















