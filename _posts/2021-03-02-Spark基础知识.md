---
layout: post
categories: [Spark]
description: none
keywords: Spark
---
# Spark基础知识
讲解Spark系统的基础知识，包括RDD编程模型、DataFrame和Dataset用户接口。

## RDD编程模型
编程模型的灵活程度决定了分布式系统的适用范围。实际上在Spark被提出之前，业界已经出现了大量的分布式计算框架，并提供了各种编程接口抽象（Abstraction）。

例如，在典型的MapReduce编程模型中，用户只需要编写Map和Reduce接口即可实现业务逻辑；又如Apache的Hama系统，直接提供了BSP模型的编程接口。

虽然在编程接口的种类和丰富程度上已经比较完善了，但这些系统普遍都缺乏操作分布式内存的接口抽象，导致很多应用在性能上非常低效。 这些应用的共同特点是需要在多个并行操作之间重用工作数据集，典型的场景就是机器学习和图应用中常用的迭代算法（每一步对数据执行相似的函数）。

例如，许多机器学习算法需要将当前迭代权值调优后的结果数据集作为下次迭代的输入，在使用MapReduce计算框架经过一次Reduce操作后输出数据会写回磁盘，然后从磁盘读取作为下次迭代的输入，这种密集的磁盘IO操作方式极大地降低了性能。

针对这类数据重用的需求，研发人员也尝试过各种解决方案。例如，Google的迭代式图计算系统Pregel会将中间数据存储在内存中，HaLoop在基本的MapReduce接口上又扩展了一个专门用于迭代的接口。然而，这些方案都存在很大的局限性，针对的也只是特定的计算需求，数据重用隐藏在系统实现背后，没有将重用逻辑显式地抽象出来形成通用接口。

RDD则是直接在编程接口层面提供了一种高度受限的共享内存模型。RDD是Spark的核心数据结构，全称是弹性分布式数据集（ResilientDistributed Dataset），其本质是一种分布式的内存抽象，表示一个只读的数据分区（Partition）集合。一个RDD通常只能通过其他的RDD转换而创建。

RDD定义了各种丰富的转换操作（如map、join和fi lter等），通过这些转换操作，新的RDD包含了如何从其他RDD衍生所必需的信息，这些信息构成了RDD之间的依赖关系（Dependency）。依赖具体分为两种，一种是窄依赖，RDD之间分区是一一对应的；另一种是宽依赖，下游RDD的每个分区与上游RDD（也称之为父RDD）的每个分区都有关，是多对多的关系。窄依赖中的所有转换操作可以通过类似管道（Pipeline）的方式全部执行，宽依赖意味着数据需要在不同节点之间Shuffle传输。

RDD计算的时候会通过一个compute函数得到每个分区的数据。若RDD是通过已有的文件系统构建的，则com pute函数读取指定文件系统中的数据；如果RDD是通过其他RDD转换而来的，则compute函数执行转换逻辑，将其他RDD的数据进行转换。RDD的操作算子包括两类，一类是transformation，用来将RDD进行转换，构建RDD的依赖关系；另一类称为action，用来触发RDD的计算，得到RDD的相关计算结果或将RDD保存到文件系统中。

在Spark中，RDD可以创建为对象，通过对象上的各种方法调用来对RDD进行转换。经过一系列的transformation逻辑之后，就可以调用action来触发RDD的最终计算。通常来讲，action包括多种方式，可以是向应用程序返回结果（show、count和collect等），也可以是向存储系统保存数据（saveAsTextFile等）。在Spark中，只有遇到action，才会真正地执行RDD的计算（注：这被称为惰性计算，英文为Lazy Evqluation），这样在运行时可以通过管道的方式传输多个转换。

总结而言，基于RDD的计算任务可描述为：从稳定的物理存储（如分布式文件系统HDFS）中加载记录，记录被传入由一组确定性操作构成的DAG（有向无环图），然后写回稳定存储。RDD还可以将数据集缓存到内存中，使得在多个操作之间可以很方便地重用数据集。总的来讲，RDD能够很方便地支持MapReduce应用、关系型数据处理、流式数据处理（Stream Processing）和迭代型应用（图计算、机器学习等）。

在容错性方面，基于RDD之间的依赖，一个任务流可以描述为DAG。在实际执行的时候，RDD通过Lineage信息（血缘关系）来完成容错，即使出现数据分区丢失，也可以通过Lineage信息重建分区。如果在应用程序中多次使用同一个RDD，则可以将这个RDD缓存起来，该RDD只有在第一次计算的时候会根据Lineage信息得到分区的数据，在后续其他地方用到这个RDD的时候，会直接从缓存处读取而不用再根据Lineage信息计算，通过重用达到提升性能的目的。

虽然RDD的Lineage信息可以天然地实现容错（当RDD的某个分区数据计算失败或丢失时，可以通过Lineage信息重建），但是对于长时间迭代型应用来说，随着迭代的进行，RDD与RDD之间的Lineage信息会越来越长，一旦在后续迭代过程中出错，就需要通过非常长的Lineage信息去重建，对性能产生很大的影响。为此，RDD支持用checkpoint机制将数据保存到持久化的存储中，这样就可以切断之前的Lineage信息，因为checkpoint后的RDD不再需要知道它的父RDD，可以从checkpoint处获取数据。

从用户角度来讲，如果要使用Spark进行数据处理和分析，需要编写一个Driver程序，并将其提交到集群执行。RDD在API层次上是Spark系统中底层，且最为灵活，自从DataFrame和Dataset接口出现后，直接使用RDD的场景越来越少。但是，作为底层支持，熟悉RDD的原理对于理解Spark SQL物理执行阶段的实现逻辑非常重要。

## DataFrame与Dataset
对于数据分析开发人员来说，一套直观易用且富有表达力的API能够极大地提高生产力。Spark在RDD基础上，提供了DataFrame和Dataset用户编程接口，并且在跨语言（Scala、Java、Python和R）方面具有很好的支持。为了追求简化，降低开发人员的学习成本，从Spark 2.0开始，DataFrame和Dataset进行了统一。

DataFrame与RDD一样，都是不可变分布式弹性数据集。不同之处在于，RDD中的数据不包含任何结构信息，数据的内部结构可以被看作“黑盒”，因此，直接使用RDD时需要开发人员实现特定的函数来完成数据结构的解析；而DataFrame中的数据集类似于关系数据库中的表，按列名存储，具有Schema信息，开发人员可以直接将结构化数据集导入DataFrame。

DataFrame的数据抽象是命名元组（对应Row类型），相比RDD多了数据特性，因此可以进行更多的优化。例如，使用DataFrame来实现WordCount，构造RDD之后，直接用toDF得到只有一列（列名为“line”）的DataFrame，然后调用explode方法将一行数据展开，最后使用groupBy方法完成聚合逻辑。

Dataset是比DataFrame更为强大的API，两者整合之后DataFrame本质上是一种特殊的Dataset（Dataset[Row]类型）。Dataset具有两个完全不同的API特征：强类型（Strongly-Typed）API和弱类型（Untyped）API。强类型一般通过Scala中定义的CaseClass或Java中的Class来指定。

作为DataFrame的扩展，Dataset结合了RDD和DataFrame的优点，提供类型安全和面向对象的编程接口，并引入了编码器（Encoder）的概念。

典型的Dataset创建与使用案例如下，其中定义的Person类就起到了Encoder的作用。在映射的过程中，Encoder首先检查定义的Person类的类型是否与数据相符，如果不相符（例如age字段大于Long的最大值等），则能够及时提供有用的错误信息，防止以不正确的方式处理数据。类似于DataFrame，Dataset创建之后也能够很容易地使用lambda表达式进行各种转换操作。

Encoder不仅能够在编译阶段完成类型安全检查，还能够生成字节码与堆外数据进行交互，提供对各个属性的按需访问，而不必对整个对象进行反序列化操作，极大地减少了网络数据传输的代价。此外，DataFrame和Dataset这些高级的API，能够利用Spark SQL中的Optim izer和Tungsten技术自动完成存储和计算的优化，降低内存使用，并极大地提升性能。



