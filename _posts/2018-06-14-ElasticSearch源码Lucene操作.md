---
layout: post
categories: [ElasticSearch]
description: none
keywords: ElasticSearch
---
# ElasticSearch源码Lucene操作
InternalEngine是对Lucene操作的封装，涉及数据读写、删除、flush等操作；FollowingEngine是follow shard操作，主要涉及CCR跨集群复制的特性。

ReadOnlyEngine只读操作，可以和InternalEngine并行执行；FrozenEngine是ReadOnlyEngine的子类，用于索引延迟加载功能；NoOpEngine在索引关闭时使用。

## InternalEngine index 写入
数据写入的核心在InternalEngine 类的index方法，逻辑上有两次获取锁的过程，一次是shard级别的锁，另一个是document级别的版本锁。基本流程是先写Lucene，然后写Translog。
```
public IndexResult index(Index index) throws IOException {
    assert Objects.equals(index.uid().field(), IdFieldMapper.NAME) : index.uid().field();
    final boolean doThrottle = index.origin().isRecovery() == false;
    //1. 读锁:此处获取了一个分片级别的读锁，意味着ElasticSearch在执行分片Bulk请求处理任务时，该分片状态为不可读。
    try (ReleasableLock releasableLock = readLock.acquire()) {
        // 确保引擎没有关闭
        ensureOpen();
        // 如果是写主分片则index的index.seqNo一定为空，如果是副分片或处于recovery过程则seq no一定不能为空
        assert assertIncomingSequenceNumber(index.origin(), index.seqNo());
        //2. 版本锁:在开始执行索引策略之前，会用去versionMap获取一把版本锁
        //该操作也是ElasticSearch对索引文档的进行乐观并发控制的机制之一，确保针不同版本索引文档之间的CRUD互相独立
        // 对文档uid加锁,写Lucene开始时对文档uid加锁，如果同id的doc已经存在，则调用lucene的updateDocument接口，如果是新文档则调用lucene的addDoucument
        try (Releasable ignored = versionMap.acquireLock(index.uid().bytes());
            Releasable indexThrottle = doThrottle ? () -> {} : throttle.acquireThrottle()) {
            lastWriteNanos = index.startTime();
            // 获取是主分片的策略还是副本分片的策略
            final IndexingStrategy plan = indexingStrategyForOperation(index);

            final IndexResult indexResult;
            if (plan.earlyResultOnPreFlightError.isPresent()) {
                //earlyResultOnPreFlightError存在值标识写入有异常
                indexResult = plan.earlyResultOnPreFlightError.get();
                assert indexResult.getResultType() == Result.Type.FAILURE : indexResult.getResultType();
            } else {
                // generate or register sequence number
                //在写入Lucene之前，先生成Sequence Number和Version
                if (index.origin() == Operation.Origin.PRIMARY) {
                    //3.为文档索引生成_seg_no,seq_no是一个ElasticSearch-shard级别的全局序列号，严格递增，由localCheckpointTracker类来负责赋值和状态控制。
                    //
                    //在ElasticSearch分片上所发生的的CRUD操作，都会被分配一个_seg_no赋值到索引文档中，在写入Translog时也会跟随文档进入，
                    // 当且仅当请求操作成功时才会返回。序列号_seg_no的作用是确保确保针不同版本索引文档之间的CRUD互相独立且有序，在实际流程控制当中，
                    // ElasticSearch对写入文档的控制使用了乐观锁进行并发控制。
                    // generateSeqNoForOperationOnPrimary：Sequence Number每次递增1，Version根据当前doc的最大版本加1。
                    // getAutoGeneratedIdTimestamp： 发起request请求时的System.nanoTime()
                    index = new Index(index.uid(), index.parsedDoc(), generateSeqNoForOperationOnPrimary(index), index.primaryTerm(),
                        index.version(), index.versionType(), index.origin(), index.startTime(), index.getAutoGeneratedIdTimestamp(),
                        index.isRetry(), index.getIfSeqNo(), index.getIfPrimaryTerm());

                    final boolean toAppend = plan.indexIntoLucene && plan.useLuceneUpdateDocument == false;
                    if (toAppend == false) {
                        advanceMaxSeqNoOfUpdatesOrDeletesOnPrimary(index.seqNo());
                    }
                } else {
                    markSeqNoAsSeen(index.seqNo());
                }

                assert index.seqNo() >= 0 : "ops should have an assigned seq no.; origin: " + index.origin();
                 //addStaleOpToLucene属性表示当前的文档索引操作是否要添加到Lucene中，即使它已经被标记为过期或已被删除。
                 //如果该变量为true，则当前的文档索引操作将被添加到Lucene中，即使它已被标记为过期或已被删除。
                 //如果该变量为false，则会执行其他的文档索引操作，例如添加新文档或更新现有文档
                    
                 //useLuceneUpdateDocument属性表示是否使用 Lucene 的更新文档功能来更新文档
                 //更新文档的常见方式有两种：
                 //1.删除旧文档，然后添加新文档；
                 //2.使用Lucene的更新文档功能直接修改文档。
                 //  使用更新文档功能可以减少磁盘IO和内存占用，提高索引性能，但需要满足一定的条件，
                 //  例如文档必须存储在单独的Lucene索引段中，并且不能包含已被删除的字段等。
                if (plan.indexIntoLucene || plan.addStaleOpToLucene) {
                    //使用某个策略将索引写入lucene，最终会调用lucene的文档写入接口
                    indexResult = indexIntoLucene(index, plan);
                } else {
                    indexResult = new IndexResult(
                        plan.versionForIndexing, index.primaryTerm(), index.seqNo(), plan.currentNotFoundOrDeleted);
                }
            }
```

## indexIntoLucene
indexIntoLucene是把数据通过IndexWriter（Lucene类）写入lucene buffer。
```
private IndexResult indexIntoLucene(Index index, IndexingStrategy plan)
    //更新文档的seqNo和primaryTerm;
    //此处的seqNo派生自序列号服务（如果该服务位于主文档上）或现有文档的序列号（如果位于副本上）。
    //此处的primaryTerm已经设置好，参考IndexShard#prepareIndex。
    index.parsedDoc().updateSeqID(index.seqNo(), index.primaryTerm());
    index.parsedDoc().version().setLongValue(plan.versionForIndexing);
    try {
        if (plan.addStaleOpToLucene) {
            // 如果该document已经是过时的了，则首先在document中增加soft-deleted标志字段，然后再IndexWriter.addDocument进行写入
            addStaleDocs(index.docs(), indexWriter);
        } else if (plan.useLuceneUpdateDocument) {
            assert assertMaxSeqNoOfUpdatesIsAdvanced(index.uid(), index.seqNo(), true, true);
            // 如果该document已经存在则调用IndexWriter.updateDocument进行更新
            updateDocs(index.uid(), index.docs(), indexWriter);
        } else {
            // document does not exists, we can optimize for create, but double check if assertions are running
            assert assertDocDoesNotExist(index, canOptimizeAddDocument(index) == false);
            // document不存在，则调用IndexWriter.addDocument创建新document
            addDocs(index.docs(), indexWriter);
        }
        return new IndexResult(plan.versionForIndexing, index.primaryTerm(), index.seqNo(), plan.currentNotFoundOrDeleted);
    }
```
Lucene文件写入的示例如下图，根据不同的数据类型及FieldType生成不同的文件类型，最后形成Segment。

### IndexWriter写入
（1）addStaleDocs
```
private void addStaleDocs(final List<ParseContext.Document> docs, final IndexWriter indexWriter) throws IOException {
    assert softDeleteEnabled : "Add history documents but soft-deletes is disabled";
    for (ParseContext.Document doc : docs) {
        doc.add(softDeletesField); // soft-deleted every document before adding to Lucene
    }
    if (docs.size() > 1) {
        indexWriter.addDocuments(docs);
    } else {
        indexWriter.addDocument(docs.get(0));
    }
}
```
（2）updateDocs
```
// 如果该document已经存在则调用IndexWriter.updateDocument进行更新
private void updateDocs(final Term uid, final List<ParseContext.Document> docs, final IndexWriter indexWriter) throws IOException {
    if (softDeleteEnabled) {
        //软连接
        if (docs.size() > 1) {
            indexWriter.softUpdateDocuments(uid, docs, softDeletesField);
        } else {
            indexWriter.softUpdateDocument(uid, docs.get(0), softDeletesField);
        }
    } else {
        if (docs.size() > 1) {
            indexWriter.updateDocuments(uid, docs);
        } else {
            indexWriter.updateDocument(uid, docs.get(0));
        }
    }
    numDocUpdates.inc(docs.size());
}
```
（3）addDocs
```
private void addDocs(final List<ParseContext.Document> docs, final IndexWriter indexWriter) throws IOException {
    if (docs.size() > 1) {
        indexWriter.addDocuments(docs);
    } else {
        indexWriter.addDocument(docs.get(0));
    }
    numDocAppends.inc(docs.size());
}
```
## InternalEngine delete 删除
数据删除的核心在InternalEngine 类的delete方法，逻辑上有两次获取锁的过程。数据删除的基本流程是先删除Lucene的数据，然后再删除Translog数据。
```
public DeleteResult delete(Delete delete) throws IOException {
    versionMap.enforceSafeAccess();

    final DeleteResult deleteResult;
    // NOTE: we don't throttle this when merges fall behind because delete-by-id does not create new segments:
    // 1.获取读锁；2.获取版本锁
    try (ReleasableLock ignored = readLock.acquire(); Releasable ignored2 = versionMap.acquireLock(delete.uid().bytes())) {
        ensureOpen();
        lastWriteNanos = delete.startTime();
        // 获取是主分片的删除策略还是副本分片的删除策略
        final DeletionStrategy plan = deletionStrategyForOperation(delete);

        if (plan.earlyResultOnPreflightError.isPresent()) {
            //earlyResultOnPreFlightError存在值标识，删除有异常
            deleteResult = plan.earlyResultOnPreflightError.get();
        } else {
            // generate or register sequence number
            if (delete.origin() == Operation.Origin.PRIMARY) {
                //主分片删除
                delete = new Delete(delete.type(), delete.id(), delete.uid(), generateSeqNoForOperationOnPrimary(delete),
                    delete.primaryTerm(), delete.version(), delete.versionType(), delete.origin(), delete.startTime(),
                    delete.getIfSeqNo(), delete.getIfPrimaryTerm());

                advanceMaxSeqNoOfUpdatesOrDeletesOnPrimary(delete.seqNo());
            } else {
                markSeqNoAsSeen(delete.seqNo());
            }

            if (plan.deleteFromLucene || plan.addStaleOpToLucene) {
                //从Lucene中删除
                deleteResult = deleteInLucene(delete, plan);
            } else {
                deleteResult = new DeleteResult(
                    plan.versionOfDeletion, delete.primaryTerm(), delete.seqNo(), plan.currentlyDeleted == false);
            }
        }
        if (delete.origin().isFromTranslog() == false && deleteResult.getResultType() == Result.Type.SUCCESS) {
            //先删除Lucen，再删除Translog。add包含Delete Index 和 NoOp（忽略）三个操作
            final Translog.Location location = translog.add(new Translog.Delete(delete, deleteResult));
            deleteResult.setTranslogLocation(location);
        }
```
（1） deleteInLucene

deleteInLucene从Lucene中删除，translog.add(newTranslog.Delete(delete, deleteResult))和write操作一样， add方法支持Delete Index 和 NoOp（忽略）三个操作。
```
private DeleteResult deleteInLucene(Delete delete, DeletionStrategy plan) throws IOException {
    assert assertMaxSeqNoOfUpdatesIsAdvanced(delete.uid(), delete.seqNo(), false, false);
    try {
        if (softDeleteEnabled) {
            //soft-deletes 本质上就是加了一个额外的字段表示文档被删除了，然后在通过一些其他 api 将soft-deletes的 doc 读出来
            final ParsedDocument tombstone = engineConfig.getTombstoneDocSupplier().newDeleteTombstoneDoc(delete.type(), delete.id());
            assert tombstone.docs().size() == 1 : "Tombstone doc should have single doc [" + tombstone + "]";
            tombstone.updateSeqID(delete.seqNo(), delete.primaryTerm());
            tombstone.version().setLongValue(plan.versionOfDeletion);
            final ParseContext.Document doc = tombstone.docs().get(0);
            assert doc.getField(SeqNoFieldMapper.TOMBSTONE_NAME) != null :
                "Delete tombstone document but _tombstone field is not set [" + doc + " ]";
            //用软删除的方式删除，实际就是创建一个新的 doc，将docId设置为要删除的 docId，并将 softDelete 字段值设置为1
            //softDeletesField: NumericDocValuesField(SOFT_DELETES_FIELD, 1)
            doc.add(softDeletesField);
            if (plan.addStaleOpToLucene || plan.currentlyDeleted) {
                indexWriter.addDocument(doc);
            } else {
                //softUpdateDocument完成了文档的软删除过程，接下来用不同的 reader 就可以读取到，或者过滤掉被软删除的文档。
                indexWriter.softUpdateDocument(delete.uid(), doc, softDeletesField);
            }
        } else if (plan.currentlyDeleted == false) {
            // any exception that comes from this is a either an ACE or a fatal exception there
            // can't be any document failures  coming from this
            //硬删除使用Lucene deleteDocuments删除文档
            indexWriter.deleteDocuments(delete.uid());
        }
        if (plan.deleteFromLucene) {
            numDocDeletes.inc();
            versionMap.putDeleteUnderLock(delete.uid().bytes(),
                new DeleteVersionValue(plan.versionOfDeletion, delete.seqNo(), delete.primaryTerm(),
                    engineConfig.getThreadPool().relativeTimeInMillis()));
        }
        return new DeleteResult(
            plan.versionOfDeletion, delete.primaryTerm(), delete.seqNo(), plan.currentlyDeleted == false);
```
softDeleteEnabled本质上就是加了一个额外的字段表示文档被删除了，然后在通过一些其他 api 将soft-deletes的 doc 读出来。

（2） IndexWriter操作

软删除：indexWriter.softUpdateDocument(delete.uid(), doc, softDeletesField);调用Lucene的 IndexWriter类softUpdateDocument方法实现。

硬删除：indexWriter.deleteDocuments()是Lucene最原始的删除。

软删除和硬删除的区别就是 merge 之后，被软删除的 doc会被保留。而后通过一些其他的接口可以读到被软删除的文档。

## InternalEngine refresh 刷新
refresh操作是把In-memory buffer中的数据写入到OS Cache形成一个新的Segment同时清空In-memory buffer，这样数据就处于可被搜索状态。通过代码，也能够看到refresh操作也是依赖Lucene完成。
```
final boolean refresh(String source, SearcherScope scope, boolean block) throws EngineException {
    final long localCheckpointBeforeRefresh = localCheckpointTracker.getProcessedCheckpoint();
    boolean refreshed;
    try {
        // refresh does not need to hold readLock as ReferenceManager can handle correctly if the engine is closed in mid-way.
        if (store.tryIncRef()) {
            // increment the ref just to ensure nobody closes the store during a refresh
            try {
                // even though we maintain 2 managers we really do the heavy-lifting only once.
                // the second refresh will only do the extra work we have to do for warming caches etc.
                ReferenceManager<ElasticsearchDirectoryReader> referenceManager = getReferenceManager(scope);
                // it is intentional that we never refresh both internal / external together
                //maybeRefreshBlocking和maybeRefresh的区别是：
                // 当有另外一个线程在执行时，maybeRefreshBlocking会阻塞直到另外的线程执行结束，maybeRefresh直接返回
                if (block) {
                    referenceManager.maybeRefreshBlocking();
                    refreshed = true;
                } else {
                    //refresh操作调用lucene的ReferenceManager，真正的refresh操作由lucene执行
                    refreshed = referenceManager.maybeRefresh();
                }
            } finally {
                store.decRef();
            }
            if (refreshed) {
                lastRefreshedCheckpointListener.updateRefreshedCheckpoint(localCheckpointBeforeRefresh);
            }
        } else {
            refreshed = false;
        }
```

## InternalEngine flush
flush操作：先执行一次refresh操作，之后将系统缓存区中的一个一个Segment拷贝到磁盘中，之后清空translog

调用Translog.rollGeneration方法。生成一个新的Translog。
调用commitIndexWriter方法。向lucene写入commitData信息。将lucene中所有的修改都提交到索引中，然后同步（sync）索引数据到磁盘文件中。这样索引数据就彻底保存到磁盘中了。
调用refresh方法将缓冲区中数据写入一个新的Segment
调用Translog.trimUnreferencedReaders删除掉未被引用的日志reader。
调用refreshLastCommittedSegmentInfos更新最新的Segment信息
```
public CommitId flush(boolean force, boolean waitIfOngoing) throws EngineException {

    final byte[] newCommitId;
    /*
     * Unfortunately the lock order is important here. We have to acquire the readlock first otherwise
     * if we are flushing at the end of the recovery while holding the write lock we can deadlock if:
     *  Thread 1: flushes via API and gets the flush lock but blocks on the readlock since Thread 2 has the writeLock
     *  Thread 2: flushes at the end of the recovery holding the writeLock and blocks on the flushLock owned by Thread 1
     */
    try (ReleasableLock lock = readLock.acquire()) {
        ensureOpen();
        if (flushLock.tryLock() == false) {
            // if we can't get the lock right away we block if needed otherwise barf
            if (waitIfOngoing) {
                logger.trace("waiting for in-flight flush to finish");
                flushLock.lock();
                logger.trace("acquired flush lock after blocking");
            } else {
                return new CommitId(lastCommittedSegmentInfos.getId());
            }
        } else {
            logger.trace("acquired flush lock immediately");
        }
        try {
            // Only flush if (1) Lucene has uncommitted docs, or (2) forced by caller, or (3) the
            // newly created commit points to a different translog generation (can free translog)
            boolean hasUncommittedChanges = indexWriter.hasUncommittedChanges();
            boolean shouldPeriodicallyFlush = shouldPeriodicallyFlush();
            //执行Flush的条件：（1）Lucene有未提交的数据；（2）调用方强制刷新；（3）translog文件大小超过阈值
            if (hasUncommittedChanges || force || shouldPeriodicallyFlush) {
                ensureCanFlush();
                try {
                    //1.生成一个新的Translog
                    translog.rollGeneration();
                    logger.trace("starting commit for flush; commitTranslog=true");
                    //2.向lucene写入commitData信息。将lucene中所有的修改都提交到索引中，然后同步（sync）索引数据到磁盘文件中。
                    //将系统缓存区里的segment写入到磁盘中
                    commitIndexWriter(indexWriter, translog, null);
                    
                    // we need to refresh in order to clear older version values
                    //3.执行一次refresh操作,refresh方法将缓冲区中数据写入一个新的Segment
                    refresh("version_table_flush", SearcherScope.INTERNAL, true);
                    //4.删除掉未被引用的日志reader,清空translog日志
                    translog.trimUnreferencedReaders();
                } catch (AlreadyClosedException e) {
                
                //5.更新最新的Segment信息
                refreshLastCommittedSegmentInfos();
            }
            newCommitId = lastCommittedSegmentInfos.getId();
        }
```
flush方法需要获取readlock和flushlock，首先获取readLock，否则会造成死锁：

线程1通过API执行flush时获得了flushlock，但因为线程2正在使用writeLock，所以阻塞等待readlock

线程2在recovery执行flush的时候占用了writeLock，然后阻塞等待正在被线程1占用的flushlock

## Segment合并
由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。

Elasticsearch通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。

段合并仍旧是以来Lucene。在ES的InternalEngine类初始化方法中对应writer = createWriter(); writer 默认Lucene的IndexWriter，createWriter()方法对应逻辑是：
```
private IndexWriter createWriter() throws IOException {
    try {
        final IndexWriterConfig iwc = getIndexWriterConfig();
        return createWriter(store.directory(), iwc);
    } catch (LockObtainFailedException ex) {
}
```

### getIndexWriterConfig
IndexWriter的配置类，与merger相关属性是iwc.setMergeScheduler(mergeScheduler); mergeScheduler对应的是new EngineMergeScheduler(engineConfig.getShardId(), engineConfig.getIndexSettings());

### createWriter
createWriter对应的代码如下，默认请情况下创建IndexWriter类，IndexWritre对应的是Lucene类。
```
IndexWriter createWriter(Directory directory, IndexWriterConfig iwc) throws IOException {
    //Assertions.ENABLED默认值未false
    if (Assertions.ENABLED) {
        return new AssertingIndexWriter(directory, iwc);
    } else {
        return new IndexWriter(directory, iwc);
    }
}
```
在Lucene9.0.0版本中，IndexWritrer类初始化方法中包含如下逻辑，完成MergeScheduler类的初始化。initDynamicDefaults方法核心是初始化Merge的线程个数及最大线程个数。
```
/**这里是mergeScheduler的初始化类，即完成Segment的初始化方法*/
mergeScheduler.initialize(infoStream, directoryOrig);
--------------------------------------------------------
void initialize(InfoStream infoStream, Directory directory) throws IOException {
  super.initialize(infoStream, directory);
  initDynamicDefaults(directory);
}
```

## Segment Merge
Lucene支持ConcurrentMergeScheduler和SerialMergeScheduler两种类型，从名字也能够看出一个支持并发一个是顺序执行，ES内部的实现只有ConcurrentMergeScheduler。

Merge触发的原因对应的是Lucene的MergeTrigger类：
```
public enum MergeTrigger {
  /** Merge was triggered by a segment flush. */
  SEGMENT_FLUSH,

  /**
   * Merge was triggered by a full flush. Full flushes can be caused by a commit, NRT reader reopen
   * or a close call on the index writer.
   */
  FULL_FLUSH,

  /** Merge has been triggered explicitly by the user. */
  EXPLICIT,

  /** Merge was triggered by a successfully finished merge. */
  MERGE_FINISHED,

  /** Merge was triggered by a closing IndexWriter. */
  CLOSING,

  /** Merge was triggered on commit. */
  COMMIT,
  /** Merge was triggered on opening NRT readers. */
  GET_READER,
}
```
merge最后调用的方法入口是mergeScheduler.merge(this, trigger, newMergesFound)
```
//通过ConcurrentMergeScheduler类中的merge方法创建用户合并的线程MergeThread并启动
@Override
public synchronized void merge(MergeSource mergeSource, MergeTrigger trigger) throws IOException {

  if (trigger == MergeTrigger.CLOSING) {
    // Disable throttling on close:
    targetMBPerSec = MAX_MERGE_MB_PER_SEC;
    updateMergeThreads();
  }
  // Iterate, pulling from the IndexWriter's queue of
  // pending merges, until it's empty:
  while (true) {

    if (maybeStall(mergeSource) == false) {
      break;
    }

    // 取出注册的后选段
    OneMerge merge = mergeSource.getNextMerge();
    if (merge == null) {
      return;
    }

    boolean success = false;
    try {
      // OK to spawn a new merge thread to handle this
      // merge: 构建用于合并的线程MergeThread
      final MergeThread newMergeThread = getMergeThread(mergeSource, merge);
     //新增的mergeThreads，加入mergeThreads
      mergeThreads.add(newMergeThread);

      updateIOThrottle(newMergeThread.merge, newMergeThread.rateLimiter);

      // 启用线程
      newMergeThread.start();
      updateMergeThreads();

      success = true;
    } 
---------------------------------------------
MergeThread类对应的run方法
public void run() {
  try {
    if (verbose()) {
      message(String.format(Locale.ROOT, "merge thread %s start", this.getName()));
    }

    doMerge(mergeSource, merge);
```
doMerge方法调用，回到ElasticSearch的ElasticsearchConcurrentMergeScheduler类中实现的doMerge方法。
```
protected void doMerge(IndexWriter writer, MergePolicy.OneMerge merge) throws IOException {
    int totalNumDocs = merge.totalNumDocs();
    long totalSizeInBytes = merge.totalBytesSize();
    long timeNS = System.nanoTime();
    currentMerges.inc();
    currentMergesNumDocs.inc(totalNumDocs);
    currentMergesSizeInBytes.inc(totalSizeInBytes);

    OnGoingMerge onGoingMerge = new OnGoingMerge(merge);
    onGoingMerges.add(onGoingMerge);
    ..........................
    }
    try {
        beforeMerge(onGoingMerge);
        //调用父类ConcurrentMergeScheduler(Lucene类)的doMerge方法
        super.doMerge(writer, merge);
    }
```
super.doMerge(writer, merge);调用父类ConcurrentMergeScheduler的doMerge方法：
```
  protected void doMerge(IndexWriter writer, OneMerge merge) throws IOException {
    writer.merge(merge);
  }
```
然后调用Lucene内IndexWriter的merge方法public void merge(MergePolicy.OneMerge merge) ，执行Segment的合并，这个过程的示意图如下图。

从Segment合并的过程中看，ElasticSearch自身并没有执行合并操作，而是依赖Lucene的IndexWriter完成。

下图可以看出Lucene的Segment的合并是分批合并，而不是一次性合并出一个大的Segment。

## InternalEngine 读取
IndexShard.getEngine方法，一个索引分片对应一个Engine实例，Engine中的方法均为分片级别的操作。get方法对应数据读取：
```
public GetResult get(Get get, BiFunction<String, SearcherScope, Engine.Searcher> searcherFactory) throws EngineException {
    assert Objects.equals(get.uid().field(), IdFieldMapper.NAME) : get.uid().field();
    //开启分片读锁。当分片在执行Flush操作时，此处将被阻塞。
    //因为Get操作，有可能需要从Translog中读取最新数据。而Flush操作会将内核中Segment刷写到磁盘，同时截断Translog。
    try (ReleasableLock ignored = readLock.acquire()) {
        ensureOpen();
        SearcherScope scope;
        //默认情况为实时读取数据
        if (get.realtime()) {
            VersionValue versionValue = null;
            //开启文档版本锁。
            try (Releasable ignore = versionMap.acquireLock(get.uid().bytes())) {
                // we need to lock here to access the version map to do this truly in RT
                // 锁定版本以实时获取更新过的最新版本
                versionValue = getVersionFromMap(get.uid().bytes());
            }
            if (versionValue != null) {
                if (versionValue.isDelete()) {
                    //最新版本数据被删除
                    return GetResult.NOT_EXISTS;
                }
                //并发场景下可能存在版本冲突
                //根据当前版本类型检查当前版本是否与预期版本冲突
                if (get.versionType().isVersionConflictForReads(versionValue.version, get.version())) {
                    throw new VersionConflictEngineException(shardId, get.id(),
                        get.versionType().explainConflictForReads(versionValue.version, get.version()));
                }
                if (get.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO && (
                    get.getIfSeqNo() != versionValue.seqNo || get.getIfPrimaryTerm() != versionValue.term
                    )) {
                    throw new VersionConflictEngineException(shardId, get.id(),
                        get.getIfSeqNo(), get.getIfPrimaryTerm(), versionValue.seqNo, versionValue.term);
                }
                //默认从Translog中读取最新数据。isReadFromTranslog和readtime属性值一致。
                if (get.isReadFromTranslog()) {
                    // this is only used for updates - API _GET calls will always read form a reader for consistency
                    // the update call doesn't need the consistency since it's source only + _parent but parent can go away in 7.0
                    //文档有更新操作时，首先被写入Translog.一秒后才会执行refresh操作，刷至内核缓冲区。
                    // 所以这1秒内文档数据是没有更新，处于不一致状态。只能先从Translog中读取最新数据。
                    if (versionValue.getLocation() != null) {
                        try {
                            //返回Translog中的最新数据
                            Translog.Operation operation = translog.readOperation(versionValue.getLocation());
                            if (operation != null) {
                                // in the case of a already pruned translog generation we might get null here - yet very unlikely
                                final Translog.Index index = (Translog.Index) operation;
                                TranslogLeafReader reader = new TranslogLeafReader(index);
                                //返回Translog中的最新数据
                                return new GetResult(new Engine.Searcher("realtime_get", reader,
                                    IndexSearcher.getDefaultSimilarity(), null, IndexSearcher.getDefaultQueryCachingPolicy(), reader),
                                    new VersionsAndSeqNoResolver.DocIdAndVersion(0, index.version(), index.seqNo(), index.primaryTerm(),
                                        reader, 0));
                            }
                     
                    } else {
                        trackTranslogLocation.set(true);
                    }
                }
                assert versionValue.seqNo >= 0 : versionValue;
                //不从Translog中读取，则调用refresh，将数据刷新至内核，从而对读操作可见。
                refreshIfNeeded("realtime_get", versionValue.seqNo);
            }
            scope = SearcherScope.INTERNAL;
        } else {
            // we expose what has been externally expose in a point in time snapshot via an explicit refresh
            scope = SearcherScope.EXTERNAL;
        }

        // no version, get the version from the index, we know that we refresh on flush
        //Searcher.Reader读取分片数据
        return getFromSearcher(get, searcherFactory, scope);
    }
}
```
校验并发版本冲突时，源码中作了两处校验：

version
ifSeqNo和ifPrimaryTerm
version针对单个文档，文档的每次修改(包括删除)操作版本号都会自增 version=version+1。
ifSeqNo针对索引单个分片，对分片内文档的每次修改(包括删除)操作ifSeqNo都会累加，ifSeqNo=ifSeqNo+1。
ifPrimaryTerm在每次主分片切换时累加，如下线重启，故障切换重新选举新的主分片等
```
protected final GetResult getFromSearcher(Get get, BiFunction<String, SearcherScope, Engine.Searcher> searcherFactory,
                                            SearcherScope scope) throws EngineException {
    //获取Searcher，searcherFactory对应IndexShard的acquireSearcher方法
    final Engine.Searcher searcher = searcherFactory.apply("get", scope);
    final DocIdAndVersion docIdAndVersion;
    try {
        //不缓存PerThreadIDVersionAndSeqNoLookup
        docIdAndVersion = VersionsAndSeqNoResolver.loadDocIdAndVersion(searcher.getIndexReader(), get.uid(), true);
    } catch (Exception e) {
        Releasables.closeWhileHandlingException(searcher);
        //TODO: A better exception goes here
        throw new EngineException(shardId, "Couldn't resolve version", e);
    }

    //并发版本冲突检测
    if (docIdAndVersion != null) {
        if (get.versionType().isVersionConflictForReads(docIdAndVersion.version, get.version())) {
            Releasables.close(searcher);
            throw new VersionConflictEngineException(shardId, get.id(),
                    get.versionType().explainConflictForReads(docIdAndVersion.version, get.version()));
        }
        if (get.getIfSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO && (
            get.getIfSeqNo() != docIdAndVersion.seqNo || get.getIfPrimaryTerm() != docIdAndVersion.primaryTerm
        )) {
            Releasables.close(searcher);
            throw new VersionConflictEngineException(shardId, get.id(),
                get.getIfSeqNo(), get.getIfPrimaryTerm(), docIdAndVersion.seqNo, docIdAndVersion.primaryTerm);
        }
    }

    if (docIdAndVersion != null) {
        // don't release the searcher on this path, it is the
        // responsibility of the caller to call GetResult.release
        return new GetResult(searcher, docIdAndVersion);
    }
```
IndexReader是一个抽象类，提供用于访问索引数据的接口。在打开新IndexReader之前，对索引所做的任何更改都是不可见的。
IndexReader通常通过调用DirectoryReader.open(Directory)方法来构造实例.
有两种类型的IndexReader：
- LeafReader：叶子读取器。LeafReader是一个抽象类，提供用于访问索引的接口。索引的搜索完全通过这个抽象接口完成，因此实现它的任何子类都是可搜索的。支持检索存储（stored）字段、文档值、词条(terms)和词条倒排索引记录(postings)。
- CompositeReader：复合读取器。CompositeReader也是抽像类，它由多个子读取器LeafReader组成，CompositeReader本身是不具备直接检索功能，只能从LeafReader子读取器进行检索。CompositeReader通过LeafReaderContext.leaves()获取所有子读取器。

注意： IndexReader 实例是完全线程安全的，这意味着多个线程可以同时调用其任何方法。读操作线程安全，个人理解更多是依赖Segment的不变性实现，读操作引起的并发冲突问题，在ES层面解决。