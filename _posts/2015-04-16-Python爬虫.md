---
layout: post
categories: [Python]
description: none
keywords: Python
---
# Python爬虫
爬虫就是通过编写程序来获取互联网上的优秀资源

## 什么是爬虫
爬虫其实是一门计算机中的技术，它被广泛应用于搜索引擎。常见的搜索引擎有百度、Google、Bing等。搜索引擎的工作原理大致分为爬取信息、存储、建立索引、排序、检索等环节，其中第一阶段就是使用专用程序收集网页数据，这个程序通常称为蜘蛛（Spider）或爬虫（Crawler）。搜索引擎从已知的数据库出发，访问这些网页并抓取文件。搜索引擎通过这些爬虫从一个网站爬到另一个网站，跟踪网页中的链接，访问更多的网页，这个过程称为爬行，这些新的网址会被存入数据库等待搜索。简而言之，爬虫就是通过不间断地访问互联网，然后从中获取你指定的信息并返回给你，获取你所需要的数据。而我们的互联网上，随时都有无数的爬虫在爬取数据，并返回给使用者。除了搜索引擎，爬虫还可以完成很多别的事情。

## 爬虫合法么？
首先，爬虫在法律上是不被禁止的，也就是说法律上是允许爬虫存在的，但是爬虫也具有违法风险的。

robots.txt是一个协议,我们可以把它理解为一个网站的"管家",它会告诉搜索引擎哪些页面可以访问,哪些页面不能访问。也可以规定哪些搜索引擎可以访问我们的网站而哪些搜索引擎不能爬取我们网站的信息等等,是网站管理者指定的"君子协议"。
当一个搜索机器人（有的叫搜索蜘蛛）访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，那么搜索机器人就沿着链接抓取。
另外，robots.txt必须放置在一个站点的根目录下，而且文件名必须全部小写。

在浏览器的网址搜索栏中，输入网站的根域名，然后再输入/robot.txt即可查看。比如，百度的robots.txt网址为 https://www.baidu.com/robots.txt

## 爬虫的原理
如果要获取网络上数据，我们要给爬虫一个网址（程序中通常叫URL），爬虫发送一个HTTP请求给目标网页的服务器，服务器返回数据给客户端（也就是我们的爬虫），爬虫再进行数据解析、保存等一系列操作。

## 爬虫原理
网络连接需要计算机一次Request请求和服务器端的Response回应。爬虫也是需要做两件事：
- 模拟计算机对服务器发起Request请求。
- 接收服务器端的Response内容并解析、提取所需的信息。

## 我的第一个爬虫程序

### Requests库
Requests库的官方文档指出：让HTTP服务人类。细心的读者会发现，Requests库的作用就是请求网站获取网页数据的。让我们从简单的实例开始，讲解Requests库的使用方法。
```
import requests
#网站为百度网站
res = requests.get('https://www.baidu.com/')
print(res)
#返回结果为<Response [200]>，说明请求网址成功，若为404,400则请求网址失败
print(res.text)
```
有时爬虫需要加入请求头来伪装成浏览器，以便更好地抓取数据。在Chrome浏览器中按F12键打开Chrome开发者工具，刷新网页后找到User-Agent进行复制
```
import requests
headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36"
}
#网站为百度网站
res = requests.get('https://www.baidu.com/',headers=headers)
print(res)
#返回结果为<Response [200]>，说明请求网址成功，若为404,400则请求网址失败
print(res.text)

```










