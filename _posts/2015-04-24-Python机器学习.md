---
layout: post
categories: [Python]
description: none
keywords: Python
---
# Python机器学习

## 什么是机器学习
机器学习（Machine Learning，ML）是一门多领域的交叉学科，涉及概率论、统计学、线性代数、算法等多门学科。它专门研究计算机如何模拟和学习人的行为，以获取新的知识或技能，重新组织已有的知识结构使之不断完善自身的性能。

机器学习已经有了十分广泛的应用，例如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。

机器学习的算法分为两大类：监督学习和无监督学习。
- 监督学习即在机器学习过程中提供对错指示。一般是在数据组中包含最终结果（0，1），通过算法让机器自己减少误差。这一类学习主要应用于分类和预测（Regression&Classify）。监督学习从给定的训练数据集中学习出一个目标函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入和输出，也可以说包括特征和目标，训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。
- 非监督学习又称归纳性学习（Clustering），利用 K 方式（KMean）建立中心（Centriole），通过循环和递减运算（Iteration&Descent）来减小误差，达到分类的目的。

## 线性模型
线性模型原本是一个统计学中的术语，近年来越来越多地应用在机器学习领域。实际上线性模型并不是特指某一个模型，而是一类模型。在机器学习领域，常用的线性模型包括线性回归、岭回归、套索回归、逻辑回归和线性SVC等。下面我们先来研究一下线性模型的公式及特点。

## 线性模型的一般公式
在回归分析当中，线性模型的一般预测公式如下：
$$
ŷ = w[0] * x[0] + w[1] * x[1] + … + w[p] * x[p] + b
$$

式中：x[0]，x[1]，…，x[p]为数据集中特征变量的数量（这个公式表示数据集中的数据点一共有p个特征）；w和b为模型的参数；ŷ为模型对于数据结果的预测值。对于只有一个特征变量的数据集，公式可以简化为
$$
ŷ = w[0] * x[0] + b
$$
是不是觉得这个公式看上去像是一条直线的方程的解析式？没错，w[0]是直线的斜率，b是y轴偏移量，也就是截距。如果数据的特征值增加的话，每个w值就会对应每个特征直线的斜率。如果换种方式来理解的话，那么模型给出的预测可以看作输入特征的加权和，而w参数就代表了每个特征的权重，当然，w也可以是负数。

注意　ŷ读作“y hat”，代表y的估计值。

## 最基本的线性模型——线性回归
线性回归，也称为普通最小二乘法（OLS），是在回归分析中最简单也是最经典的线性模型。本节中我们将介绍线性回归的原理和在实践中的表现。

## 线性回归的基本原理
线性回归的原理是，找到当训练数据集中y的预测值和其真实值的平方差最小的时候，所对应的w值和b值。线性回归没有可供用户调节的参数，这是它的优势，但也代表我们无法控制模型的复杂性。接下来我们继续使用make_regression函数，生成一个样本数量为100，特征数量为2的数据集，并且用train_test_split函数将数据集分割成训练数据集和测试数据集，再用线性回归模型计算出w值和b值。
```shell
    #导入数据集拆分工具
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    X, y = make_regression(n_samples=100,n_features=2,n_informative=2,random_
    state=38)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)
    lr = LinearRegression().fit(X_train, y_train)
    print('\n\n\n代码运行结果：')
    print('==========\n')
    print("lr.coef_: {}".format(lr.coef_[:]))
    print("lr.intercept_: {}".format(lr.intercept_))
    print('\n==========')
    print('\n\n\n')
```
【结果分析】intercept_属性一直是一个浮点数，而coef_属性则是一个NumPy数组，其中每个特征对应数据中的一个数值，由于我们这次使用make_regression生成的数据集中数据点有2个特征，所以lr.coef_是一个二维数组。也就是说，在本例中线性回归模型的方程可以表示为
```shell
y = 70.385 9×X1 + 7.4321×X2 -1.42e-14
```

## 使用L2正则化的线性模型——岭回归
岭回归也是回归分析中常用的线性模型，它实际上是一种改良的最小二乘法。本节中我们将介绍岭回归的原理及在实践中的性能表现。

## 岭回归的原理
从实用的角度来说，岭回归实际上是一种能够避免过拟合的线性模型。在岭回归中，模型会保留所有的特征变量，但是会减小特征变量的系数值，让特征变量对预测结果的影响变小，在岭回归中是通过改变其alpha参数来控制减小特征变量系数的程度。而这种通过保留全部特征变量，只是降低特征变量的系数值来避免过拟合的方法，我们称之为L2正则化。

岭回归在scikit-learn中是通过linear_model.Ridge函数来调用的

## 使用L1正则化的线性模型——套索回归
除了岭回归之外，还有一个对线性回归进行正则化的模型，即套索回归（lasso）。本节我们重点探讨套索回归的原理及其在实际应用中的表现。

## 套索回归的原理
和岭回归一样，套索回归也会将系数限制在非常接近0的范围内，但它进行限制的方式稍微有一点不同，我们称之为L1正则化。与L2正则化不同的是，L1正则化会导致在使用套索回归的时候，有一部分特征的系数会正好等于0。也就是说，有一些特征会彻底被模型忽略掉，这也可以看成是模型对于特征进行自动选择的一种方式。把一部分系数变成0有助于让模型更容易理解，而且可以突出体现模型中最重要的那些特征。

## K最近邻算法——近朱者赤，近墨者黑

## 朴素贝叶斯
贝叶斯（Thomas Bayes）是一位英国数学家，1701年生于伦敦，曾是一位神父。后于1742年成为英国皇家学会会员。贝叶斯在数学方面主要研究概率论，他创立了贝叶斯统计理论，该理论对现代概率论和数理统计又有很重要的作用，在数学和工程领域都得到了广泛的应用，本节将介绍贝叶斯定理和朴素贝叶斯分类器的基本概念。

## 朴素贝叶斯算法的不同方法
朴素贝叶斯算法包含多种方法，在scikit-learn中，朴素贝叶斯有三种方法，分别是贝努利朴素贝叶斯（Bernoulli Naïve Bayes）、高斯贝叶斯（Gaussian Naïve Bayes）和多项式朴素贝叶斯（Multinomial Naïve Bayes），本节将对这几种方法进行介绍。

## 贝努利朴素贝叶斯
在上面的例子当中，我们使用了朴素贝叶斯算法中的一种方法，称为贝努利朴素贝叶斯（Bernoulli Naïve Bayes），这种方法比较适合于符合贝努利分布的数据集，贝努利分布也被称为“二项分布”或者是“0-1分布”，比如我们进行抛硬币的游戏，硬币落下来只有两种可能的结果：正面或者反面，这种情况下，我们就称抛硬币的结果是贝努利分布的。

## 高斯朴素贝叶斯
高斯朴素贝叶斯，顾名思义，是假设样本的特征符合高斯分布，或者说符合正态分布时所用的算法。

## 多项式朴素贝叶斯
多项式朴素贝叶斯，从名字也可以推断出它主要是用于拟合多项式分布的数据集。可能多项式分布相对于二项式分布和高斯分布来说，我们会接触得少一些。但如果我们可以理解二项式分布，那么理解多项式分布也会非常简单。二项式分布可以通过抛硬币的例子来进行理解，那么多项式分布都可以用掷骰子来理解。

我们知道硬币只有两个面，正面和反面，而骰子有6个面，因此每掷一次骰子，结果都可能是从1～6这6个数字，如果我们掷n次骰子，而每个面朝上的次数的分布情况，就是一个多项式分布。

## 决策树与随机森林
决策树是一种在分类与回归中都有非常广泛应用的算法，它的原理是通过对一系列问题进行if/else的推导，最终实现决策。

## 随机森林
常言道，不要为了一棵树放弃一片森林。这句话在机器学习算法方面也是非常正确的。虽然决策树算法简单易理解，而且不需要对数据进行转换，但是它的缺点也很明显——决策树往往容易出现过拟合的问题。不过这难不倒我们，因为我们可以让很多树组成团队来工作，也就是——随机森林。

## 随机森林的基本概念
先来一段比较官方的解释：随机森林有的时候也被称为是随机决策森林，是一种集合学习方法，既可以用于分类，也可以用于回归。而所谓集合学习算法，其实就是把多个机器学习算法综合在一起，制造出一个更加大模型的意思。这也就很好地解释了为什么这种算法称为随机森林了

## 支持向量机SVM

## 神经网络

## 数据预处理、降维、特征提取及聚类

### 使用StandardScaler进行数据预处理
首先我们还是先手工生成一些数据，用它们来说明数据预处理的一些原理和方法。这次我们依旧使用scikit-learn的make_blobs函数，在Jupyter Notebook中新建一个notebook，输入代码如下：
```shell
    #导入numpy
import numpy as np
    #导入画图工具
import matplotlib.pyplot as plt
    #导入数据集生成工具
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=40, centers=2, random_state=50, cluster_std=2)
    #用散点图绘制数据点
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.cool)
    #显示图像
plt.show()
```

## 数据表达与特征工程


























