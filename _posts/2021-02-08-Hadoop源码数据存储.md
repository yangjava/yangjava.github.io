---
layout: post
categories: [Hadoop]
description: none
keywords: Hadoop
---
# Hadoop源码数据存储
HDFS最重要的功能就是存储数据，即如何写读数据是HDFS最核心的功能点

## HDFS写数据流程
- 客户端发起写文件请求，调用DistributedFileSystem（FileSystem的实现类）的create()方法。
- 通过NameNodeRPC与NameNode建立通信，首先会判断文件的用户权限以及文件是否已经存在，还有文件是否存在父目录，返回是否可以上传的标识和要上传的数据块。
- 客户端开始向FSDataOutputStream发送写数据请求，FSDataOutputStream会将数据切分成packet放入数据队列中。
- 客户端将NameNode返回的可用于写入的DataNode信息列表和block数据块按照就近的策略写入第一个DataNode，DataNode会根据HDFS的副本策略在其他DataNode直接进行数据复制。
- DataNode写完数据之后会返回一个确认响应，FSDataOutputStream收集完DataNode的响应之后会清空用于接收响应的数据队列，用于下一次接收信息。
- 写完数据之后调用客户端的close（）方法，关闭数据流。
- 然后客户端将完成数据写入的信息通知NameNode。

以上就是HDFS写数据的整体流程，下面开始从源码层面进行剖析。

可以看到FileSystem是HDFS文件交互系统的抽象类，底层有很多不同的实现类，包括WebHdfsFileSystem、S3FileSystem、HttpFSFileSystem、AzureFileSystem等等。

从HDFS写数据的流程图可以知道第一步就是先定位到create（）方法。

找到FileSystem这个类的create（）方法，可以看到只有一行代码，而且返回的对象就是FSDataOutputStream输出流，所以下一步找实现类，这也是我们阅读源码的一种好习惯。

```
/**
 * Create an FSDataOutputStream at the indicated Path.
 * Files are overwritten by default.
 * @param f the file to create
 */
public FSDataOutputStream create(Path f) throws IOException {
  //定位就是这个方法
  return create(f, true);
}
 
 
/**
 * Create an FSDataOutputStream at the indicated Path.
 * @param f the file to create
 * @param overwrite if a file with this name already exists, then if true,
 *   the file will be overwritten, and if false an exception will be thrown.
 */
public FSDataOutputStream create(Path f, boolean overwrite)
    throws IOException {
  //指定要创建的文件，如果已存在则覆盖掉，设置文件缓存大小为4M，获取副本策略以及默认数据块的大小
  return create(f, overwrite, 
                getConf().getInt("io.file.buffer.size", 4096),
                getDefaultReplication(f),
                getDefaultBlockSize(f));
}
 
 
/**
 * Create an FSDataOutputStream at the indicated Path.
 * @param f the file name to open
 * @param overwrite if a file with this name already exists, then if true,
 *   the file will be overwritten, and if false an error will be thrown.
 * @param bufferSize the size of the buffer to be used.
 * @param replication required block replication for the file. 
 */
public FSDataOutputStream create(Path f, 
                                 boolean overwrite,
                                 int bufferSize,
                                 short replication,
                                 long blockSize
                                 ) throws IOException {
  //只有一行，那就是它了
  return create(f, overwrite, bufferSize, replication, blockSize, null);
}
```
在这里提一下HDFS代码的方法其实大部分写的很简洁，不像一些方法写了几百行，实际上阅读起来是非常费劲的，但是HDFS则会将不同的逻辑层层抽取，有点像套娃，使得代码的可读性比较好。

找到FileSystem的实现类DistributedFileSystem的create（）方法。
```
@Override
public FSDataOutputStream create(Path f, FsPermission permission,
    boolean overwrite, int bufferSize, short replication, long blockSize,
    Progressable progress) throws IOException {
  //巧了，又是一行，是不是很像套娃，一层套一层
  //写数据的时候会判断文件是否存在，是否覆盖文件，如果overwrite为false则执行CreateFlag.CREATE，反之为true则执行CreateFlag.OVERWRITE
  return this.create(f, permission,
      overwrite ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)
          : EnumSet.of(CreateFlag.CREATE), bufferSize, replication,
      blockSize, progress, null);
}
 
 
@Override
public FSDataOutputStream create(final Path f, final FsPermission permission,
  final EnumSet<CreateFlag> cflags, final int bufferSize,
  final short replication, final long blockSize, final Progressable progress,
  final ChecksumOpt checksumOpt) throws IOException {
  statistics.incrementWriteOps(1);
  Path absF = fixRelativePart(f);
  return new FileSystemLinkResolver<FSDataOutputStream>() {
    @Override
    public FSDataOutputStream doCall(final Path p)
        throws IOException, UnresolvedLinkException {
      //创建了一个DFSOutputStream，进行初始化操作
      /**
       *    往文件目录树里面添加了INodeFile
       *    添加了契约管理
       *    启动了DataStreamer，写数据核心服务
       */
      final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,
              cflags, replication, blockSize, progress, bufferSize,
              checksumOpt);
      
      return dfs.createWrappedOutputStream(dfsos, statistics);
    }
    @Override
    public FSDataOutputStream next(final FileSystem fs, final Path p)
        throws IOException {
      return fs.create(p, permission, cflags, bufferSize,
          replication, blockSize, progress, checksumOpt);
    }
  }.resolve(this, absF);
}
```
分析dfs.create(.....）方法里面执行了什么操作，在这里先说一个概念“文件契约（lease）”，HDFS在写数据文件的时候，会为每个数据文件创建契约，

主要涉及三个重要的步骤：添加契约、开启契约、延续契约（续约）对应LeaseManager中的addLease（）、startLease（）、renewLease（）。

```
/**
 * Adds (or re-adds) the lease for the specified file.
 */
synchronized Lease addLease(String holder, String src) {
 
 
  Lease lease = getLease(holder);
  if (lease == null) {
   // 如果契约为空则创建一个契约
    lease = new Lease(holder);
   //存储到支持排序序的数据结构里面
    leases.put(holder, lease);
    sortedLeases.add(lease);
  } else {
    //续约
    renewLease(lease);
  }
  sortedLeasesByPath.put(src, lease);
  lease.paths.add(src);
  return lease;
}
```
何为契约？顾名思义就是一种约束，在一个动作中参与其中需要遵守的规则。在HDFS中有个文件契约机制，同一个时间只允许一个客户端获取NameNode上面一个文件的契约，然后才可以向获取契约的文件写入数据。 

此时如果其他客户端尝试获取文件契约的时候，就获取不到，只能干等着。 通过这个机制，可以保证同一时间只有一个客户端在写一个文件。 在获取到了文件契约之后，在写文件的过程期间，那个客户端需要开启一个线程，不停的发送请求给 NameNode进行文件续约，告诉NameNode。

NameNode大哥，我还在写文件啊，你给我一直保留那个契约好吗？ 同时NameNode内部有一个专门的后台线程，负责监控各个契约的续约时间。 如果某个契约很长时间没续约了，即某个客户端没有继续写入文件了，此时就自动过期掉这个契约，让别的客户端来写。

文件契约机制在这里不展开说，有兴趣的可以自己查资料了解一下，还是聚焦到HDFS写数据的流程的主题上来。
```

/**
 * Call
 * {@link #create(String, FsPermission, EnumSet, boolean, short, long, Progressable, int, ChecksumOpt)}
 * with <code>createParent</code> set to true.
 */
public DFSOutputStream create(String src, FsPermission permission, EnumSet<CreateFlag> flag, short replication,
      long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt) throws IOException {
   //似曾相识create
   return create(src, permission, flag, true, replication, blockSize, progress, buffersize, checksumOpt, null);
}
 
 
public DFSOutputStream create(String src, FsPermission permission, EnumSet<CreateFlag> flag, boolean createParent,
      short replication, long blockSize, Progressable progress, int buffersize, ChecksumOpt checksumOpt,
      InetSocketAddress[] favoredNodes) throws IOException {
   checkOpen();
   if (permission == null) {
      permission = FsPermission.getFileDefault();
   }
   FsPermission masked = permission.applyUMask(dfsClientConf.uMask);
   if (LOG.isDebugEnabled()) {
      LOG.debug(src + ": masked=" + masked);
   }
   
   
    //这里主要做了三个动作，newStreamForCreate这个方法
    // 1、往文件目录树INodeDirectory添加了INodeFile
    // 2、添加文件契约lease
    // 3、启动DataStreamer,负责与NameNodeRPCServer通信
   final DFSOutputStream result = DFSOutputStream.newStreamForCreate(this, src, masked, flag, createParent,
         replication, blockSize, progress, buffersize, dfsClientConf.createChecksum(checksumOpt),
         getFavoredNodesStr(favoredNodes));
 
 
   //开启契约
   beginFileLease(result.getFileId(), result);
   return result;
}
 
 
/** Get a lease and start automatic renewal */
//获取一个文件契约并启动自动续约
private void beginFileLease(final long inodeId, final DFSOutputStream out) throws IOException {
   getLeaseRenewer().put(inodeId, out, this);
}
 
 
//这里使用了synchronized关键字，保证同一个时间只有一个客户端能写数据，其他的阻塞
synchronized void put(final long inodeId, final DFSOutputStream out,
    final DFSClient dfsc) {
  if (dfsc.isClientRunning()) {
    if (!isRunning() || isRenewerExpired()) {
      //start a new deamon with a new id.
      final int id = ++currentId;
      //创建了一个后台线程
      daemon = new Daemon(new Runnable() {
        @Override
        public void run() {
          try {
            if (LOG.isDebugEnabled()) {
              LOG.debug("Lease renewer daemon for " + clientsString()
                  + " with renew id " + id + " started");
            }
            //LeaseRenewer----负责管理续约
            LeaseRenewer.this.run(id);
          } catch(InterruptedException e) {
            if (LOG.isDebugEnabled()) {
              LOG.debug(LeaseRenewer.this.getClass().getSimpleName()
                  + " is interrupted.", e);
            }
          } finally {
            synchronized(LeaseRenewer.this) {
              Factory.INSTANCE.remove(LeaseRenewer.this);
            }
            if (LOG.isDebugEnabled()) {
              LOG.debug("Lease renewer daemon for " + clientsString()
                  + " with renew id " + id + " exited");
            }
          }
        }
        
        @Override
        public String toString() {
          return String.valueOf(LeaseRenewer.this);
        }
      });
      //启动后台守护线程
      daemon.start();
    }
    dfsc.putFileBeingWritten(inodeId, out);
    emptyTime = Long.MAX_VALUE;
  }
}
```
以上完成了写数据的准备动作，契约管理。

## 启动DataStreamer
```
static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src, FsPermission masked,
      EnumSet<CreateFlag> flag, boolean createParent, short replication, long blockSize, Progressable progress,
      int buffersize, DataChecksum checksum, String[] favoredNodes) throws IOException {
   TraceScope scope = dfsClient.getPathTraceScope("newStreamForCreate", src);
   try {
      HdfsFileStatus stat = null;
 
 
      // Retry the create if we get a RetryStartFileException up to a maximum
      // number of times
      boolean shouldRetry = true;
      int retryCount = CREATE_RETRY_COUNT;
      
      //不断重试，确保文件目录创建成功
      while (shouldRetry) {
         shouldRetry = false;
         try {
            //hdfs创建文件，可以详细看NameNodeRPC端的create()方法
            stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,
                  new EnumSetWritable<CreateFlag>(flag), createParent, replication, blockSize,
                  SUPPORTED_CRYPTO_VERSIONS);
            break;//成功则跳出循环，失败则抛出异常并重试
         } catch (RemoteException re) {
            IOException e = re.unwrapRemoteException(AccessControlException.class,
                  DSQuotaExceededException.class, FileAlreadyExistsException.class,
                  FileNotFoundException.class, ParentNotDirectoryException.class,
                  NSQuotaExceededException.class, RetryStartFileException.class, SafeModeException.class,
                  UnresolvedPathException.class, SnapshotAccessControlException.class,
                  UnknownCryptoProtocolVersionException.class);
            if (e instanceof RetryStartFileException) {
               //重试
               if (retryCount > 0) {
                  shouldRetry = true;
                  retryCount--;
               } else {
                  throw new IOException("Too many retries because of encryption" + " zone operations", e);
               }
            } else {
               throw e;
            }
         }
      }
      Preconditions.checkNotNull(stat, "HdfsFileStatus should not be null!");
 
 
      //初始化了DataStreamer，DataStreamer是写数据流程的核心类，实质上DataStreamer是一个线程
      final DFSOutputStream out = new DFSOutputStream(dfsClient, src, stat, flag, progress, checksum,
            favoredNodes);
      //启动了DataStreamer
      out.start();
      return out;
   } finally {
      scope.close();
   }
}
```
下面分析一下写数据的核心类DFSOutputStream和DataStreamer。
```

/** Construct a new output stream for creating a file. */
private DFSOutputStream(DFSClient dfsClient, String src, HdfsFileStatus stat, EnumSet<CreateFlag> flag,
      Progressable progress, DataChecksum checksum, String[] favoredNodes) throws IOException {
   this(dfsClient, src, progress, stat, checksum);
   this.shouldSyncBlock = flag.contains(CreateFlag.SYNC_BLOCK);
       /**
        * Directory -> File -> Block(128M)  -> packet（64k）-> chunk（516byte）
        */
        //计算数据单元的值，比较简单，不展开，点进去看就知道了
   computePacketChunkSize(dfsClient.getConf().writePacketSize, bytesPerChecksum);
        
   //创建了DataStreamer，这才是核心所在
   streamer = new DataStreamer(stat, null);
   if (favoredNodes != null && favoredNodes.length != 0) {
      streamer.setFavoredNodes(favoredNodes);
   }
}
```
点进去new DataStreamer（）发现这个方法没什么内容，看完是不是有点无从下手？
```
private DataStreamer(HdfsFileStatus stat, ExtendedBlock block) {
   isAppend = false;
   isLazyPersistFile = isLazyPersist(stat);
   this.block = block;
   stage = BlockConstructionStage.PIPELINE_SETUP_CREATE;
}
```
所以说阅读源码不是一件很简单的事情，还是没有那么容易的，其实在上面的newStreamForCreate（.....）这个方法中就悄咪咪地说了DataStreamer是一个线程，有start（）方法，自然就有run（）方法，所以不言而喻，揪出run（）就是我们要的答案，后面写数据的时候还要结合这一部分关键代码。
```

/*
 * streamer thread is the only thread that opens streams to datanode, and closes
 * them. Any error recovery is also done by this thread.
 */
@Override
public void run() {
   long lastPacket = Time.monotonicNow();
   TraceScope scope = NullScope.INSTANCE;
   while (!streamerClosed && dfsClient.clientRunning) {
      // if the Responder encountered an error, shutdown Responder
      if (hasError && response != null) {
         try {
            response.close();
            response.join();
            response = null;
         } catch (InterruptedException e) {
            DFSClient.LOG.warn("Caught exception ", e);
         }
      }
      DFSPacket one;
      try {
         // process datanode IO errors if any
         boolean doSleep = false;
         
         if (hasError && (errorIndex >= 0 || restartingNodeIndex.get() >= 0)) {
 
 
            doSleep = processDatanodeError();
         }
         synchronized (dataQueue) {
            // wait for a packet to be sent.
            long now = Time.monotonicNow();
            
            // 刚开始创建文件的时候，dataQueue.size() == 0
            while ((!streamerClosed && !hasError && dfsClient.clientRunning && dataQueue.size() == 0
                  && (stage != BlockConstructionStage.DATA_STREAMING
                        || stage == BlockConstructionStage.DATA_STREAMING
                              && now - lastPacket < dfsClient.getConf().socketTimeout / 2))
                  || doSleep) {
               long timeout = dfsClient.getConf().socketTimeout / 2 - (now - lastPacket);
               timeout = timeout <= 0 ? 1000 : timeout;
               timeout = (stage == BlockConstructionStage.DATA_STREAMING) ? timeout : 1000;
               try {
                  //如果dataQueue队列里面没有数据，代码就会阻塞在这里，等待被唤醒。
                  dataQueue.wait(timeout);
               } catch (InterruptedException e) {
                  DFSClient.LOG.warn("Caught exception ", e);
               }
               doSleep = false;
               now = Time.monotonicNow();
            }
            if (streamerClosed || hasError || !dfsClient.clientRunning) {
               continue;
            }
            // get packet to be sent.
            if (dataQueue.isEmpty()) {
               one = createHeartbeatPacket();
               assert one != null;
            } else {
               //如果队列不为空，从往队列里面取出packet
               one = dataQueue.getFirst(); // regular data packet
               long parents[] = one.getTraceParents();
               if (parents.length > 0) {
                  scope = Trace.startSpan("dataStreamer", new TraceInfo(0, parents[0]));
                  // TODO: use setParents API once it's available from HTrace 3.2
                  // scope = Trace.startSpan("dataStreamer", Sampler.ALWAYS);
                  // scope.getSpan().setParents(parents);
               }
            }
         }
         // get new block from namenode.       
         // 建立数据管道pipeline，向NameNode申请block
         if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {
            if (DFSClient.LOG.isDebugEnabled()) {
               DFSClient.LOG.debug("Allocating new block");
            }
            //建立数据管道，保存管道中block的存储信息，包括位置，类型和ID 
            //nextBlockOutputStream()这个方法很重要，向namenode申请block用于写入数据，选择存放block的DataNode策略也是在这个方法里面，这里由于篇幅所限不展开说明。
            setPipeline(nextBlockOutputStream());
            //初始化DataStreaming服务，启动了ResponseProcessor，用来监听packet发送的状态。
            initDataStreaming();
         } else if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {
            if (DFSClient.LOG.isDebugEnabled()) {
               DFSClient.LOG.debug("Append to block " + block);
            }
            setupPipelineForAppendOrRecovery();
            initDataStreaming();
         }
         long lastByteOffsetInBlock = one.getLastByteOffsetBlock();
         if (lastByteOffsetInBlock > blockSize) {
            throw new IOException("BlockSize " + blockSize + " is smaller than data size. "
                  + " Offset of packet in block " + lastByteOffsetInBlock + " Aborting file " + src);
         }
         if (one.isLastPacketInBlock()) {
            // wait for all data packets have been successfully acked
            synchronized (dataQueue) {
               while (!streamerClosed && !hasError && ackQueue.size() != 0 && dfsClient.clientRunning) {
                  try {
                     // wait for acks to arrive from datanodes
                     dataQueue.wait(1000);
                  } catch (InterruptedException e) {
                     DFSClient.LOG.warn("Caught exception ", e);
                  }
               }
            }
            if (streamerClosed || hasError || !dfsClient.clientRunning) {
               continue;
            }
            stage = BlockConstructionStage.PIPELINE_CLOSE;
         }
         // send the packet
         Span span = null;
         synchronized (dataQueue) {
            // move packet from dataQueue to ackQueue
            if (!one.isHeartbeatPacket()) {
               span = scope.detach();
               one.setTraceSpan(span);
               //从dataQueue把要发送的这个packet移除出去
               dataQueue.removeFirst();
               //ackQueue里面添加这个packet
               ackQueue.addLast(one);
               //唤醒wait线程
               dataQueue.notifyAll();
            }
         }
         if (DFSClient.LOG.isDebugEnabled()) {
            DFSClient.LOG.debug("DataStreamer block " + block + " sending packet " + one);
         }
         // write out data to remote datanode
         TraceScope writeScope = Trace.startSpan("writeTo", span);
         try {
            //这个就是我们写数据代码
            one.writeTo(blockStream);
            blockStream.flush();
         } catch (IOException e) {
            // HDFS-3398 treat primary DN is down since client is unable to
            // write to primary DN. If a failed or restarting node has already
            // been recorded by the responder, the following call will have no
            // effect. Pipeline recovery can handle only one node error at a
            // time. If the primary node fails again during the recovery, it
            // will be taken out then.
            //PrimaryDatanode 指的是数据管道第一个datanode
            //errorIndex = 0;
            tryMarkPrimaryDatanodeFailed();
            //抛异常
            throw e;
         } finally {
            writeScope.close();
         }
         lastPacket = Time.monotonicNow();
         // update bytesSent
         long tmpBytesSent = one.getLastByteOffsetBlock();
         if (bytesSent < tmpBytesSent) {
            bytesSent = tmpBytesSent;
         }
         if (streamerClosed || hasError || !dfsClient.clientRunning) {
            continue;
         }
         // Is this block full?
         if (one.isLastPacketInBlock()) {
            // wait for the close packet has been acked
            synchronized (dataQueue) {
               while (!streamerClosed && !hasError && ackQueue.size() != 0 && dfsClient.clientRunning) {
                  dataQueue.wait(1000);// wait for acks to arrive from datanodes
               }
            }
            if (streamerClosed || hasError || !dfsClient.clientRunning) {
               continue;
            }
            endBlock();
         }
         if (progress != null) {
            progress.progress();
         }
         // This is used by unit test to trigger race conditions.
         if (artificialSlowdown != 0 && dfsClient.clientRunning) {
            Thread.sleep(artificialSlowdown);
         }
      } catch (Throwable e) {
         // Log warning if there was a real error.
         if (restartingNodeIndex.get() == -1) {
            DFSClient.LOG.warn("DataStreamer Exception", e);
         }
         if (e instanceof IOException) {
            setLastException((IOException) e);
         } else {
            setLastException(new IOException("DataStreamer Exception: ", e));
         }
         //捕获到了异常
         //把标识改为true
         hasError = true;
         if (errorIndex == -1 && restartingNodeIndex.get() == -1) {
            // Not a datanode issue
            streamerClosed = true;
         }
      } finally {
         scope.close();
      }
   }
   closeInternal();
```

## 初始化DataStreaming
```
/**
 * Initialize for data streaming
 */
private void initDataStreaming() {
   this.setName("DataStreamer for file " + src + " block " + block);
   
   response = new ResponseProcessor(nodes);
   //启动线程，看到这里是不是跟DataStreamer很相似
   response.start();
   stage = BlockConstructionStage.DATA_STREAMING;
}
 
 
   ResponseProcessor(DatanodeInfo[] targets) {
      this.targets = targets;
   }
   @Override
   public void run() {
      setName("ResponseProcessor for block " + block);
      //ack响应队列，返回DataNode写数据的结果
      PipelineAck ack = new PipelineAck();
      TraceScope scope = NullScope.INSTANCE;
      while (!responderClosed && dfsClient.clientRunning && !isLastPacketInBlock) {
         // process responses from datanodes.
         try {
            // read an ack from the pipeline
            long begin = Time.monotonicNow();
            //读取下游的处理结果
            ack.readFields(blockReplyStream);
            long duration = Time.monotonicNow() - begin;
            if (duration > dfsclientSlowLogThresholdMs && ack.getSeqno() != DFSPacket.HEART_BEAT_SEQNO) {
               DFSClient.LOG.warn("Slow ReadProcessor read fields took " + duration + "ms (threshold="
                     + dfsclientSlowLogThresholdMs + "ms); ack: " + ack + ", targets: "
                     + Arrays.asList(targets));
            } else if (DFSClient.LOG.isDebugEnabled()) {
               DFSClient.LOG.debug("DFSClient " + ack);
            }
            long seqno = ack.getSeqno();
            // processes response status from datanodes.
            for (int i = ack.getNumOfReplies() - 1; i >= 0 && dfsClient.clientRunning; i--) {
               final Status reply = PipelineAck.getStatusFromHeader(ack.getHeaderFlag(i));
               // Restart will not be treated differently unless it is
               // the local node or the only one in the pipeline.
               if (PipelineAck.isRestartOOBStatus(reply) && shouldWaitForRestart(i)) {
                  restartDeadline = dfsClient.getConf().datanodeRestartTimeout + Time.monotonicNow();
                  setRestartingNodeIndex(i);
                  String message = "A datanode is restarting: " + targets[i];
                  DFSClient.LOG.info(message);
                  throw new IOException(message);
               }
               // node error
               if (reply != SUCCESS) {
                  setErrorIndex(i); // first bad datanode
                  throw new IOException("Bad response " + reply + " for block " + block
                        + " from datanode " + targets[i]);
               }
            }
            assert seqno != PipelineAck.UNKOWN_SEQNO : "Ack for unknown seqno should be a failed ack: "
                  + ack;
            if (seqno == DFSPacket.HEART_BEAT_SEQNO) { // a heartbeat ack
               continue;
            }
            // a success ack for a data packet
            DFSPacket one;
            synchronized (dataQueue) {
               one = ackQueue.getFirst();
            }
            if (one.getSeqno() != seqno) {
               throw new IOException("ResponseProcessor: Expecting seqno " + " for block " + block
                     + one.getSeqno() + " but received " + seqno);
            }
            isLastPacketInBlock = one.isLastPacketInBlock();
            // Fail the packet write for testing in order to force a
            // pipeline recovery.
            if (DFSClientFaultInjector.get().failPacket() && isLastPacketInBlock) {
               failPacket = true;
               throw new IOException("Failing the last packet for testing.");
            }
            // update bytesAcked
            block.setNumBytes(one.getLastByteOffsetBlock());
            synchronized (dataQueue) {
               scope = Trace.continueSpan(one.getTraceSpan());
               one.setTraceSpan(null);
               lastAckedSeqno = seqno;
               //如果ack发送成功那么就会把ackQueue里面packet移除来
               ackQueue.removeFirst();
               dataQueue.notifyAll();
               one.releaseBuffer(byteArrayManager);
            }
         } catch (Exception e) {
            if (!responderClosed) {
               if (e instanceof IOException) {
                  setLastException((IOException) e);
               }
               hasError = true;
               // If no explicit error report was received, mark the primary
               // node as failed.
               tryMarkPrimaryDatanodeFailed();
               synchronized (dataQueue) {
                  dataQueue.notifyAll();
               }
               if (restartingNodeIndex.get() == -1) {
                  DFSClient.LOG.warn(
                        "DFSOutputStream ResponseProcessor exception " + " for block " + block, e);
               }
               responderClosed = true;
            }
         } finally {
            scope.close();
         }
      }
   }
          
   void close() {
      responderClosed = true;
      //打断当前的线程，让线程快速退出
      this.interrupt();
   }
}
```
执行完DistributedFileSystem的create（..）方法后，将返回的DFSOutputStream对象传递给createWrappedOutputStream（...）方法中进行再次封装，这里引用了设计模式中的“装饰者”模式（Decorator Pattern）。

```
/**
 * Wraps the stream in a CryptoOutputStream if the underlying file is encrypted.
 */
 //返回的是HDFSOutputStream
public HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos, FileSystem.Statistics statistics)
      throws IOException {
   return createWrappedOutputStream(dfsos, statistics, 0);
}

```
上面我们完成了create（）方法的分析，主要完成了文件目录树添加INodeFIle、文件契约管理并且启动了DataStreamer和初始化DataStreamming，接下来才是真正进行数据写入。

## 开始写数据
根据写数据的流程图我们找到FSDataOutputStream的write（）方法。
```
public void write(int b) throws IOException {
   //out就是DFSOutputStream
  out.write(b);
  position++;
  if (statistics != null) {
    statistics.incrementBytesWritten(1);
  }
}
```
通过DFSOutputStream的父类FSOutputSummer定位到write（）方法
```

/** Write one byte */
@Override
public synchronized void write(int b) throws IOException {
  buf[count++] = (byte)b;
  if(count == buf.length) {
    //写文件
    flushBuffer();
  }
}
 
 
/* Forces any buffered output bytes to be checksumed and written out to
 * the underlying output stream. 
 */
protected synchronized void flushBuffer() throws IOException {
  //
  flushBuffer(false, true);
}
 
 
protected synchronized int flushBuffer(boolean keep,
    boolean flushPartial) throws IOException {
  int bufLen = count;
  int partialLen = bufLen % sum.getBytesPerChecksum();
  int lenToFlush = flushPartial ? bufLen : bufLen - partialLen;
  if (lenToFlush != 0) {
    //核心的代码
    //HDFS File -> Block(128M) -> packet(64K) -> chunk 512 + chunksum 4  
    writeChecksumChunks(buf, 0, lenToFlush);
    if (!flushPartial || keep) {
      count = partialLen;
      System.arraycopy(buf, bufLen - count, buf, 0, count);
    } else {
      count = 0;
    }
  }
  // total bytes left minus unflushed bytes left
  return count - (bufLen - lenToFlush);
}
```
计算每个数据单元的校验和
```
/** Generate checksums for the given data chunks and output chunks & checksums
 * to the underlying output stream.
 */
private void writeChecksumChunks(byte b[], int off, int len)
throws IOException {
 //计算出来chunk的校验和
  sum.calculateChunkedSums(b, off, len, checksum, 0);
  //按照chunk的大小遍历数据
  for (int i = 0; i < len; i += sum.getBytesPerChecksum()) {
    int chunkLen = Math.min(sum.getBytesPerChecksum(), len - i);
    int ckOffset = i / sum.getBytesPerChecksum() * getChecksumSize();
    //往chunk写数据
    writeChunk(b, off + i, chunkLen, checksum, ckOffset, getChecksumSize());
  }
}
```
继续找到FSOutputSummer的实现类DFSOutputStream，定位到writeChunk（....）方法
```
// @see FSOutputSummer#writeChunk()
@Override
protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum, int ckoff, int cklen)
      throws IOException {
   TraceScope scope = dfsClient.getPathTraceScope("DFSOutputStream#writeChunk", src);
   try {
      //写chunk
      writeChunkImpl(b, offset, len, checksum, ckoff, cklen);
   } finally {
      scope.close();
   }
}
 
 
private synchronized void writeChunkImpl(byte[] b, int offset, int len, byte[] checksum, int ckoff, int cklen)
      throws IOException {
   dfsClient.checkOpen();
   checkClosed();
   if (len > bytesPerChecksum) {
      throw new IOException("writeChunk() buffer size is " + len + " is larger than supported  bytesPerChecksum "
            + bytesPerChecksum);
   }
   if (cklen != 0 && cklen != getChecksumSize()) {
      throw new IOException(
            "writeChunk() checksum size is supposed to be " + getChecksumSize() + " but found to be " + cklen);
   }
   if (currentPacket == null) {
      //创建packet
      currentPacket = createPacket(packetSize, chunksPerPacket, bytesCurBlock, currentSeqno++, false);
      if (DFSClient.LOG.isDebugEnabled()) {
         DFSClient.LOG.debug("DFSClient writeChunk allocating new packet seqno=" + currentPacket.getSeqno()
               + ", src=" + src + ", packetSize=" + packetSize + ", chunksPerPacket=" + chunksPerPacket
               + ", bytesCurBlock=" + bytesCurBlock);
      }
   }
   //往packet里面写chunk校验和
   currentPacket.writeChecksum(checksum, ckoff, cklen);
   //往packet里面写一个chunk
   currentPacket.writeData(b, offset, len);
   //累计计算一共写了多少个chunk，如果packet写满了127chunk，那就是一个完整的packet
   currentPacket.incNumChunks();
   //同理，如果packect写满了，会形成blcok
   bytesCurBlock += len;//128M
   // If packet is full, enqueue it for transmission
   
   //当packet写满了或者block写满了
   if (currentPacket.getNumChunks() == currentPacket.getMaxChunks() || bytesCurBlock == blockSize) {
      if (DFSClient.LOG.isDebugEnabled()) {
         DFSClient.LOG.debug("DFSClient writeChunk packet full seqno=" + currentPacket.getSeqno() + ", src="
               + src + ", bytesCurBlock=" + bytesCurBlock + ", blockSize=" + blockSize + ", appendChunk="
               + appendChunk);
      }
      //写满了一个packet，将packet加入队列，packet队列满了则阻塞住
      waitAndQueueCurrentPacket();
      // If the reopened file did not end at chunk boundary and the above
      // write filled up its partial chunk. Tell the summer to generate full
      // crc chunks from now on.
      if (appendChunk && bytesCurBlock % bytesPerChecksum == 0) {
         appendChunk = false;
         resetChecksumBufSize();
      }
      if (!appendChunk) {
         int psize = Math.min((int) (blockSize - bytesCurBlock), dfsClient.getConf().writePacketSize);
         computePacketChunkSize(psize, bytesPerChecksum);
      }
      //
      // if encountering a block boundary, send an empty packet to
      // indicate the end of block and reset bytesCurBlock.
      //  当前累计blcok的大小等于128M，说明我经写完了一个block了。
      if (bytesCurBlock == blockSize) {
         //一个block写完的时候，最后一个packet是一个空的packet。
         currentPacket = createPacket(0, 0, bytesCurBlock, currentSeqno++, true);
         currentPacket.setSyncBlock(shouldSyncBlock);
         //把当前的currentPacket 加入了dataQueue
         waitAndQueueCurrentPacket();
         bytesCurBlock = 0;
         lastFlushOffset = 0;
      }
   }
}
```
本文剖析了HDFS写数据的流程，结合写数据的流程图，主要分七步走，涉及了文件目录和文件的创建，文件契约机制管理，核心写数据服务DataStreamer启动和DataStreaming初始化，数据单元chunk和block写入DataQueue队列，Block的申请以及管道的建立，数据写入容错等等。说实话我个人感觉HDFS写数据的流程是非常复杂的，内容很多，源码阅读起来非常绕，容易把自己绕晕了，这篇文章也只是写了个大概，避免文章篇幅过长，很多细节的代码是没办法体现出来，有兴趣的可以拉取源码细看，希望对大家有所启发。

## 读数据
相对于写数据流程来说，读数据的流程会简单不少，写完这一篇之后，对HDFS的核心代码剖析算是告一段落了，这一系列包含了NameNode的初始化、DataNode的初始、元数据管理、HDFS写数据流程、HDFS读数据流程五个核心部分，毕竟HDFS是一个百万行级别代码的技术架构，内容非常多，所以本系列只选取HDFS关键且核心的功能点来剖析。

## HDFS读数据流程
- HDFS 客户端调用DistributedFileSystem 类（FileSystem的实现类）的open（）方法。
- DistributedFileSystem 通过NameNodeRPC与NameNode建立通信，调用getBlockLocations（）方法，请求block数据块的存储位置。
- DistributedFileSystem 返回一个FSDataInputStream对象给客户端，FSDataInputStream携带有block数据库的元数据信息，客户端调用FSDataInputStream的read（）方法，请求存放目标block最近的DataNode节点。
- DataNode和NameNode之间以数据流的方式进行通信，保证客户端可以重复调用read（）方法进行读取，选择就近的DataNode读取需要的数据块信息，如果发现读取的DataNode有异常，则尝试读取下一个DataNode的数据，直至读完最后一个数据块。
- 当读取完文件之后，调用close（）方法关闭DataNode和NameNode连接。

实际上，从流程上可以看出HDFS读数据数据和写入数据总体的流程是差不多的，但是读取数据会简单一些，下面我们开始进行源码的分析。

## 读取数据

我们还是根据流程图的过程进行分析，这样整个过程更加清晰。

找到FSDataInputStream这个类的open（）方法，传入数据路径。
```

/**
 * Opens an FSDataInputStream at the indicated Path.
 * @param f the file to open
 */
public FSDataInputStream open(Path f) throws IOException {
  return open(f, getConf().getInt("io.file.buffer.size", 4096));
}
/**
 * Opens an FSDataInputStream at the indicated Path.
 * @param f the file name to open
 * @param bufferSize the size of the buffer to be used.
 */
public abstract FSDataInputStream open(Path f, int bufferSize)
  throws IOException;
```
显而易见，open（...）方法中传入了文件路径以及配置文件中设置的文件缓冲区大小为4M。继续点进去open方法发现是一个抽象方法，那下一步就应该找到实现类的open（...）方法，根据流程图可以知道FileSystem的实现类就是DistributedFileSystem，不言而喻，直接定位到DistributedFileSystem这个类的open方法准没错。

```
@Override
public FSDataInputStream open(Path f, final int bufferSize)
    throws IOException {
  //计算读取操作的次数进行累加并记录
  statistics.incrementReadOps(1);
  //判断数据路径是绝对路径还是相对路径，不重要
  Path absF = fixRelativePart(f);
  return new FileSystemLinkResolver<FSDataInputStream>() {
    @Override
    public FSDataInputStream doCall(final Path p)
        throws IOException, UnresolvedLinkException {
      //重要代码，重点关注,跟写数据的套路差不多
      final DFSInputStream dfsis =
        dfs.open(getPathName(p), bufferSize, verifyChecksum);
      return dfs.createWrappedInputStream(dfsis);
    }
    @Override
    public FSDataInputStream next(final FileSystem fs, final Path p)
        throws IOException {
      return fs.open(p, bufferSize);
    }
  }.resolve(this, absF);
}
```
重点关注以上dfs.open（xxx）方法，调用之前会通过文件路径判断文件是否属于当前的文件系统。

```
/**
 * Create an input stream that obtains a nodelist from the namenode, and then
 * reads from all the right places. Creates inner subclass of InputStream that
 * does the right out-of-band work.
 */
public DFSInputStream open(String src, int buffersize, boolean verifyChecksum)
      throws IOException, UnresolvedLinkException {
    //检查文件是否处于打开状态，无关紧要的方法
   checkOpen();
   // Get block info from namenode，从namenode获取block信息
   TraceScope scope = getPathTraceScope("newDFSInputStream", src);
   try {
      return new DFSInputStream(this, src, verifyChecksum);
   } finally {
      scope.close();
   }
}
```
方法的最后返回了DFSInputStream（xxx）这个构造函数，并且在构造函数中调用了openInfo（）方法。

```
DFSInputStream(DFSClient dfsClient, String src, boolean verifyChecksum
               ) throws IOException, UnresolvedLinkException {
  this.dfsClient = dfsClient;
  this.verifyChecksum = verifyChecksum;
  this.src = src;
  synchronized (infoLock) {
    this.cachingStrategy = dfsClient.getDefaultReadCachingStrategy();
  }
  openInfo();
}
```

```

/**
 * Grab the open-file info from namenode
 * 从namenode获取要打开的文件对应的blcok信息
 */
void openInfo() throws IOException, UnresolvedLinkException {
  synchronized(infoLock) {
  //划重点，对应流程图的步骤二，从namenode获取block信息
    lastBlockBeingWrittenLength = fetchLocatedBlocksAndGetLastBlockLength();
    int retriesForLastBlockLength = dfsClient.getConf().retryTimesForGetLastBlockLength;
    //为了保证读取成功，特意用了while循环增强，循环调用fetchLocatedBlocksAndGetLastBlockLength（）
    while (retriesForLastBlockLength > 0) {
      // Getting last block length as -1 is a special case. When cluster
      // restarts, DNs may not report immediately. At this time partial block
      // locations will not be available with NN for getting the length. Lets
      // retry for 3 times to get the length.
      if (lastBlockBeingWrittenLength == -1) {
        DFSClient.LOG.warn("Last block locations not available. "
            + "Datanodes might not have reported blocks completely."
            + " Will retry for " + retriesForLastBlockLength + " times");
        waitFor(dfsClient.getConf().retryIntervalForGetLastBlockLength);
        lastBlockBeingWrittenLength = fetchLocatedBlocksAndGetLastBlockLength();
      } else {
        break;
      }
      retriesForLastBlockLength--;
    }
    if (retriesForLastBlockLength == 0) {
      throw new IOException("Could not obtain the last block locations.");
    }
  }
}
```
对应流程图步骤二的getBlockLocations方法，详情请看fetchLocatedBlocksAndGetLastBlockLength（）方法。

```

private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {
//调用DFSClient的getLocatedBlocks方法，通过文件路径获取blcok存储位置信息
  final LocatedBlocks newInfo = dfsClient.getLocatedBlocks(src, 0);
  if (DFSClient.LOG.isDebugEnabled()) {
    DFSClient.LOG.debug("newInfo = " + newInfo);
  }
  if (newInfo == null) {
    throw new IOException("Cannot open filename " + src);
  }
  if (locatedBlocks != null) {
    Iterator<LocatedBlock> oldIter = locatedBlocks.getLocatedBlocks().iterator();
    Iterator<LocatedBlock> newIter = newInfo.getLocatedBlocks().iterator();
    while (oldIter.hasNext() && newIter.hasNext()) {
      if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {
        throw new IOException("Blocklist for " + src + " has changed!");
      }
    }
  }
  locatedBlocks = newInfo;
  long lastBlockBeingWrittenLength = 0;
  if (!locatedBlocks.isLastBlockComplete()) {
    final LocatedBlock last = locatedBlocks.getLastLocatedBlock();
    if (last != null) {
      if (last.getLocations().length == 0) {
        if (last.getBlockSize() == 0) {
          // if the length is zero, then no data has been written to
          // datanode. So no need to wait for the locations.
          return 0;
        }
        return -1;
      }
      final long len = readBlockLength(last);
      last.getBlock().setNumBytes(len);
      lastBlockBeingWrittenLength = len; 
    }
  }
 
 
  fileEncryptionInfo = locatedBlocks.getFileEncryptionInfo();
 
 
  return lastBlockBeingWrittenLength;
}
```

重点关注DFSClient的getBlockLocations（）方法，从namenode获取block位置信息。
```

public LocatedBlocks getLocatedBlocks(String src, long start) throws IOException {
   return getLocatedBlocks(src, start, dfsClientConf.prefetchSize);
}
 
 
/*
 * This is just a wrapper around callGetBlockLocations, but non-static so that
 * we can stub it out for tests.
 */
@VisibleForTesting
public LocatedBlocks getLocatedBlocks(String src, long start, long length) throws IOException {
   TraceScope scope = getPathTraceScope("getBlockLocations", src);
   try {
      return callGetBlockLocations(namenode, src, start, length);
   } finally {
      scope.close();
   }
}
 
 
/**
 * @see ClientProtocol#getBlockLocations(String, long, long)
 */
static LocatedBlocks callGetBlockLocations(ClientProtocol namenode, String src, long start, long length)
      throws IOException {
   try {
   //通过RPC远程调用NameNodeRPCServer
      return namenode.getBlockLocations(src, start, length);
   } catch (RemoteException re) {
      throw re.unwrapRemoteException(AccessControlException.class, FileNotFoundException.class,
            UnresolvedPathException.class);
   }
}
 
 
@Idempotent
public LocatedBlocks getBlockLocations(String src,
                                       long offset,
                                       long length) 
    throws AccessControlException, FileNotFoundException,
    UnresolvedLinkException, IOException;
```

可以看到返回的是LocatedBlocks对象，包含了List<LocatedBlock> blocks，封装了block的信息，以及block在文件中的偏移量，还有block对应DataNode的位置信息。原理上是RPC调用了NameNodeRPCServer的getBlockLocations（）方法。
```
@Override // ClientProtocol
public LocatedBlocks getBlockLocations(String src, 
                                        long offset, 
                                        long length) 
    throws IOException {
  //检查NameNode是否已经启动
  checkNNStartup();
  //计算获取到block信息并记录变化
  metrics.incrGetBlockLocations();
  return namesystem.getBlockLocations(getClientMachine(), 
                                      src, offset, length);
}
 
 
/**
 * Get block locations within the specified range.
 * @see ClientProtocol#getBlockLocations(String, long, long)
 */
LocatedBlocks getBlockLocations(String clientMachine, String src,
    long offset, long length) throws IOException {
  checkOperation(OperationCategory.READ);
  //创建Block信息结果对象
  GetBlockLocationsResult res = null;
  readLock();
  try {
    checkOperation(OperationCategory.READ);
    //获取block位置信息
    res = getBlockLocations(src, offset, length, true, true);
  } catch (AccessControlException e) {
    logAuditEvent(false, "open", src);
    throw e;
  } finally {
    readUnlock();
  }
  logAuditEvent(true, "open", src);
  if (res.updateAccessTime()) {
    writeLock();
    final long now = now();
    try {
      checkOperation(OperationCategory.WRITE);
      INode inode = res.iip.getLastINode();
      boolean updateAccessTime = now > inode.getAccessTime() +
          getAccessTimePrecision();
      if (!isInSafeMode() && updateAccessTime) {
        boolean changed = FSDirAttrOp.setTimes(dir,
            inode, -1, now, false, res.iip.getLatestSnapshotId());
        if (changed) {
          getEditLog().logTimes(src, -1, now);
        }
      }
    } catch (Throwable e) {
      LOG.warn("Failed to update the access time of " + src, e);
    } finally {
      writeUnlock();
    }
  }
  //将获取到的block信息赋值给LocatedBlocks 
  LocatedBlocks blocks = res.blocks;
  if (blocks != null) {
    blockManager.getDatanodeManager().sortLocatedBlocks(
        clientMachine, blocks.getLocatedBlocks());
    // lastBlock is not part of getLocatedBlocks(), might need to sort it too
    //获取到最后一个Block的位置信息
    LocatedBlock lastBlock = blocks.getLastLocatedBlock();
    if (lastBlock != null) {
      ArrayList<LocatedBlock> lastBlockList = Lists.newArrayList(lastBlock);
      blockManager.getDatanodeManager().sortLocatedBlocks(
          clientMachine, lastBlockList);
    }
  }
  //返回LocatedBlocks对象，封装了目标文件包含的所有block的位置信息
  return blocks;
}
```
以上就完成了步骤二获取block位置信息的分析，同样的将返回的DFSInputStream对象传递给createWrappedInputStream（...）方法中进行再次封装。接下来根据NameNode返回的LocatedBlocks对象信息，请求FSDataInputStream的 read（）方法。

```

/**
 * Read bytes from the given position in the stream to the given buffer.
 *
 * @param position  position in the input stream to seek
 * @param buffer    buffer into which data is read
 * @param offset    offset into the buffer in which data is written
 * @param length    maximum number of bytes to read
 * @return total number of bytes read into the buffer, or <code>-1</code>
 *         if there is no more data because the end of the stream has been
 *         reached
 */
@Override
public int read(long position, byte[] buffer, int offset, int length)
  throws IOException {
  return ((PositionedReadable)in).read(position, buffer, offset, length);
}
```

FSDataInputStream会调用其封装的DFSInputStream的read（xxx）方法。
```

/**
 * Read bytes starting from the specified position.
 * 
 * @param position start read from this position
 * @param buffer read buffer
 * @param offset offset into buffer
 * @param length number of bytes to read
 * 
 * @return actual number of bytes read
 */
@Override
public int read(long position, byte[] buffer, int offset, int length)
    throws IOException {
  TraceScope scope =
      dfsClient.getPathTraceScope("DFSInputStream#byteArrayPread", src);
  try {
    return pread(position, buffer, offset, length);
  } finally {
    scope.close();
  }
}
 
 
 
 
private int pread(long position, byte[] buffer, int offset, int length)
    throws IOException {
  // sanity checks，检查文件系统是否运行中
  dfsClient.checkOpen();
  if (closed.get()) {
    throw new IOException("Stream closed");
  }
  failures = 0;
  //获取LocatedBlocks的长度
  long filelen = getFileLength();
  if ((position < 0) || (position >= filelen)) {
    return -1;
  }
  int realLen = length;
  if ((position + length) > filelen) {
    realLen = (int)(filelen - position);
  }
  
  // determine the block and byte range within the block
  // corresponding to position and realLen
  //得到从offset到offset + length范围内的block列表
  List<LocatedBlock> blockRange = getBlockRange(position, realLen);
  int remaining = realLen;
  Map<ExtendedBlock,Set<DatanodeInfo>> corruptedBlockMap 
    = new HashMap<ExtendedBlock, Set<DatanodeInfo>>();
   //对block列表进行遍历，读取需要的block数据，因为需要的数据不一定是存在一个block列表中，通常分布在多个block
  for (LocatedBlock blk : blockRange) {
    long targetStart = position - blk.getStartOffset();
    long bytesToRead = Math.min(remaining, blk.getBlockSize() - targetStart);
    try {
      if (dfsClient.isHedgedReadsEnabled()) {
        hedgedFetchBlockByteRange(blk, targetStart, targetStart + bytesToRead
            - 1, buffer, offset, corruptedBlockMap);
      } else {
        fetchBlockByteRange(blk, targetStart, targetStart + bytesToRead - 1,
            buffer, offset, corruptedBlockMap);
      }
    } finally {
      // Check and report if any block replicas are corrupted.
      // BlockMissingException may be caught if all block replicas are
      // corrupted.
      reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);
    }
    remaining -= bytesToRead;
    position += bytesToRead;
    offset += bytesToRead;
  }
  assert remaining == 0 : "Wrong number of bytes read.";
  if (dfsClient.stats != null) {
    dfsClient.stats.incrementBytesRead(realLen);
  }
  return realLen;
}
```
分析getBlockRange（xxx）方法，通过指定的范围从namenode获取数据，优先从缓存中获取。

```

/**
 * Get blocks in the specified range.
 * Fetch them from the namenode if not cached. This function
 * will not get a read request beyond the EOF.
 * @param offset starting offset in file
 * @param length length of data
 * @return consequent segment of located blocks
 * @throws IOException
 */
private List<LocatedBlock> getBlockRange(long offset,
    long length)  throws IOException {
  // getFileLength(): returns total file length
  // locatedBlocks.getFileLength(): returns length of completed blocks
  //通常offset是要小于文件长度的
  if (offset >= getFileLength()) {
    throw new IOException("Offset: " + offset +
      " exceeds file length: " + getFileLength());
  }
  //之前有说到，block的状态有两种，一种是complete写入完成的，另一种是uncomplete构建中的状态
  synchronized(infoLock) {
    final List<LocatedBlock> blocks;
    //得到locatedBlocks的长度
    final long lengthOfCompleteBlk = locatedBlocks.getFileLength();
    final boolean readOffsetWithinCompleteBlk = offset < lengthOfCompleteBlk;
    final boolean readLengthPastCompleteBlk = offset + length > lengthOfCompleteBlk;
 
 
    if (readOffsetWithinCompleteBlk) {
      //get the blocks of finalized (completed) block range，
      blocks = getFinalizedBlockRange(offset,
        Math.min(length, lengthOfCompleteBlk - offset));
    } else {
      blocks = new ArrayList<LocatedBlock>(1);
    }
 
 
    // get the blocks from incomplete block range
    if (readLengthPastCompleteBlk) {
       blocks.add(locatedBlocks.getLastLocatedBlock());
    }
 
 
    return blocks;
  }
}
 
 
/**
 * Get blocks in the specified range.
 * Includes only the complete blocks.
 * Fetch them from the namenode if not cached.
 */
private List<LocatedBlock> getFinalizedBlockRange(
    long offset, long length) throws IOException {
  synchronized(infoLock) {
    assert (locatedBlocks != null) : "locatedBlocks is null";
    List<LocatedBlock> blockRange = new ArrayList<LocatedBlock>();
    // search cached blocks first
    //首先会先从缓存的locatedBlocks中查找offset所在的block在缓存链表中的位置
    int blockIdx = locatedBlocks.findBlock(offset);
    if (blockIdx < 0) { // block is not cached，无缓存
      blockIdx = LocatedBlocks.getInsertIndex(blockIdx);
    }
    long remaining = length;
    long curOff = offset;
    while(remaining > 0) {
      LocatedBlock blk = null;
      if(blockIdx < locatedBlocks.locatedBlockCount())
        //根据blcokIdx找到block
        blk = locatedBlocks.get(blockIdx);
        //说明没有缓存，从NameNode查找block并添加到缓存
      if (blk == null || curOff < blk.getStartOffset()) {
        LocatedBlocks newBlocks;
        newBlocks = dfsClient.getLocatedBlocks(src, curOff, remaining);
        locatedBlocks.insertRange(blockIdx, newBlocks.getLocatedBlocks());
        continue;
      }
      assert curOff >= blk.getStartOffset() : "Block not found";
      blockRange.add(blk);
      long bytesRead = blk.getStartOffset() + blk.getBlockSize() - curOff;
      remaining -= bytesRead;
      curOff += bytesRead;
      //继续读取下一个block
      blockIdx++;
    }
    return blockRange;
  }
}
```
其中我们看一下pread（xxx）方法下引用的fetchBlockByteRange（...）方法。
```

private void fetchBlockByteRange(LocatedBlock block, long start, long end,
    byte[] buf, int offset,
    Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap)
    throws IOException {
    //通过偏移量获取LocatedBlock 对象
  block = getBlockAt(block.getStartOffset());
  //熟悉的while循环，为了最大程度上保证成功获取数据
  while (true) {
  //选择就近的一个DataNode进行读取
    DNAddrPair addressPair = chooseDataNode(block, null);
    try {
   //通过选择的DataNode，根据block的起始偏移量开始获取数据,获取完成后return；结束循环
      actualGetFromOneDataNode(addressPair, block, start, end, buf, offset,
          corruptedBlockMap);
      return;
    } catch (IOException e) {
      // Ignore. Already processed inside the function.
      // Loop through to try the next node.
    }
  }
}
```
执行完actualGetFromOneDataNode（...）方法获取完数据之后会执行close（）方法结束连接。
```

private void actualGetFromOneDataNode(final DNAddrPair datanode,
    LocatedBlock block, final long start, final long end, byte[] buf,
    int offset, Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap)
    throws IOException {
  DFSClientFaultInjector.get().startFetchFromDatanode();
  int refetchToken = 1; // only need to get a new access token once
  int refetchEncryptionKey = 1; // only need to get a new encryption key once
 
 
  while (true) {
    // cached block locations may have been updated by chooseDataNode()
    // or fetchBlockAt(). Always get the latest list of locations at the
    // start of the loop.
    CachingStrategy curCachingStrategy;
    boolean allowShortCircuitLocalReads;
    block = getBlockAt(block.getStartOffset());
    synchronized(infoLock) {
      curCachingStrategy = cachingStrategy;
      allowShortCircuitLocalReads = !shortCircuitForbidden();
    }
    DatanodeInfo chosenNode = datanode.info;
    InetSocketAddress targetAddr = datanode.addr;
    StorageType storageType = datanode.storageType;
    //初始化BlockReader
    BlockReader reader = null;
 
 
    try {
      DFSClientFaultInjector.get().fetchFromDatanodeException();
      Token<BlockTokenIdentifier> blockToken = block.getBlockToken();
      int len = (int) (end - start + 1);
      //reader负责从DataNode读取数据，构建socket连接到DataNode
      reader = new BlockReaderFactory(dfsClient.getConf()).
          setInetSocketAddress(targetAddr).
          setRemotePeerFactory(dfsClient).
          setDatanodeInfo(chosenNode).
          setStorageType(storageType).
          setFileName(src).
          setBlock(block.getBlock()).
          setBlockToken(blockToken).
          setStartOffset(start).
          setVerifyChecksum(verifyChecksum).
          setClientName(dfsClient.clientName).
          setLength(len).
          setCachingStrategy(curCachingStrategy).
          setAllowShortCircuitLocalReads(allowShortCircuitLocalReads).
          setClientCacheContext(dfsClient.getClientContext()).
          setUserGroupInformation(dfsClient.ugi).
          setConfiguration(dfsClient.getConfiguration()).
          build();
       //读取数据
      int nread = reader.readAll(buf, offset, len);
      updateReadStatistics(readStatistics, nread, reader);
 
 
      if (nread != len) {
        throw new IOException("truncated return from reader.read(): " +
                              "excpected " + len + ", got " + nread);
      }
      DFSClientFaultInjector.get().readFromDatanodeDelay();
      return;
    } catch (ChecksumException e) {
      String msg = "fetchBlockByteRange(). Got a checksum exception for "
          + src + " at " + block.getBlock() + ":" + e.getPos() + " from "
          + chosenNode;
      DFSClient.LOG.warn(msg);
      // we want to remember what we have tried
      
      addIntoCorruptedBlockMap(block.getBlock(), chosenNode, corruptedBlockMap);
      //如果读取失败，则将该DataNode标记位异常节点
      addToDeadNodes(chosenNode);
      throw new IOException(msg);
    } catch (IOException e) {
      if (e instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {
        DFSClient.LOG.info("Will fetch a new encryption key and retry, " 
            + "encryption key was invalid when connecting to " + targetAddr
            + " : " + e);
        // The encryption key used is invalid.
        refetchEncryptionKey--;
        dfsClient.clearDataEncryptionKey();
        continue;
      } else if (refetchToken > 0 && tokenRefetchNeeded(e, targetAddr)) {
        refetchToken--;
        try {
          fetchBlockAt(block.getStartOffset());
        } catch (IOException fbae) {
          // ignore IOE, since we can retry it later in a loop
        }
        continue;
      } else {
        String msg = "Failed to connect to " + targetAddr + " for file "
            + src + " for block " + block.getBlock() + ":" + e;
        DFSClient.LOG.warn("Connection failure: " + msg, e);
        addToDeadNodes(chosenNode);
        throw new IOException(msg);
      }
    } finally {
      if (reader != null) {
        reader.close();
      }
    }
  }
}
```
本文分析了HDFS写数据流程，从HDFS客户端调用DistributedFileSystem 和FSDataInputStream这两个核心类的方法，通过NameNodeRPC调用NameNode对应的实现方法，获取目标block数据块的元数据信息，通过得到的元数据信息从对应的DataNode节点读取数据，直到读完最后一个block，关闭DataNode和NameNode之间的数据流，完成数据读取。














