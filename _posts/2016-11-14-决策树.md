---
layout: post
categories: [Python,scikitlearn]
description: none
keywords: Python
---
# 决策树
决策树是最经典的机器学习模型之一。它的预测结果容易理解，易于向业务部门解释，预测速度快，可以处理类别型数据和连续型数据。

- 信息熵及信息增益的概念，以及决策树的分裂的原则；
- 决策树的创建及剪枝算法；
- scikit-learn中决策树算法的相关参数；
- 使用决策树预测泰坦尼克号幸存者实例；
- scikit-learn中模型参数选择的工具及使用方法；
- 聚合算法及随机森林算法的原理。

## 算法原理
决策树是一个类似于流程图的树结构，分支节点表示对一个特征进行测试，根据测试结果进行分类，树叶节点代表一个类别。

每选择一个特征进行测试，数据集就被划分成多个子数据集。接着继续在子数据集上选择特征，并进行数据集划分，直到创建出一个完整的决策树。创建好决策树模型后，从根节点一路往下即可预测行为。

问题来了，在创建决策树的过程中，要先对哪个特征进行分裂？

## 信息增益
我们天天在谈论信息，那么信息要怎么样来量化呢？1948年，香农在他著名的《通信的数学原理》中提出了信息熵（Entropy）的概念，从而解决了信息的量化问题。香农认为，一条信息的信息量和它的不确定性有直接关系。一个问题不确定性越大，要搞清楚这个问题，需要了解的信息就越多，其信息熵就越大。信息熵的计算公式为：
```
H(x) = E[I(xi)] = E[ log(2,1/P(xi)) ] = -∑P(xi)log(2,P(xi)) (i=1,2,..n)
```
其中，P（x）表示事件x出现的概率。例如，一个盒子里分别有5个白球和5个红球，随机取出一个球。问：这个球是红色的还是白色的？这个问题的信息量多大呢？由于红球和白球出现的概率都是1/2，代入信息熵公式，可以得到其信息熵为：
即，这个问题的信息量是1 bit。对，你没有看错，信息量的单位就是比特。我们要确定这个球是红色的还是白色的，只需要1比特的信息就够了。再举一个极端的例子，一个盒子里有10个白球，随机取出一个球，这个球是什么颜色的？这个问题的信息量是多少呢？答案是0，因为这是一个确定的事件，其概率P（x）=1，我们代入香农的信息熵公式，即可得到其信息熵为0。即，我们不需要再获取任何新的信息，即可知道这个球一定是白色的。

回到决策树的构建问题上，当我们要构建一个决策树时，应该优先选择哪个特征来划分数据集呢？答案是：遍历所有的特征，分别计算，使用这个特征划分数据集前后信息熵的变化值，然后选择信息熵变化幅度最大的那个特征，来优先作为数据集划分依据。即选择信息增益最大的特征作为分裂节点。

比如，一个盒子里共有红、白、黑、蓝4种颜色的球共16个，其中红球2个，白球2个，黑球4个，蓝球8个。红球和黑球的体积一样，都为1个单位；白球和蓝球的体积一样，都为2个单位。红球、白球和黑球的质量一样，都是1个单位，蓝球的质量为2个单位。

我们应该优先选择体积这个特征，还是优先选择质量这个特征来作为数据集划分依据呢？根据前面介绍的结论，我们先计算基础信息熵，即划分数据集前的信息熵。从已知信息容易知道，红球、白球、黑球、蓝球出现的概率分别为2/16，、2/16、4/16、8/16，因此基础信息熵为：1.75

接着使用体积来划分数据集，此时会划分出两个数据集，第一个子数据集里是红球和黑球，第二个子数据集里是白球和蓝球，我们计算这种划分方式的信息熵。其中第一个子数据集里，红球2个，黑球4个，其概率分别为2/6和4/6，因此第一个子数据集的信息熵为：0.918296

第二个子数据集里，白球2个，蓝球8个，其概率分别为2/10和8/10，因此第二个子数据集的信息熵为：0.721928

因此，使用体积来划分数据集后，其信息熵为H（D1）=H（D1sub1）+H（D1sub2）=1.640224，其信息增益为H（Dbase）-H（D1）=1.75-1.640224=0.109776

如果我们使用质量来划分数据集，也会划分出两个数据集，第一个子数据集里是红球、白球和黑球，第二个子数据集里是只有蓝球。我们计算这种划分方式的信息熵。针对第一个子数据集，红球、白球和黑球出现的概率分别是2/8，、2/8、4/8，其信息熵为：1.5

第二个子数据集里只有蓝球，其概率为1，因此其信息熵H（D2sub2）=0。我们得出使用使用质量来划分数据集时的信息熵为1.5，其信息增益为1.75-1.5=0.25。由于使用质量划分数据集比使用体积划分数据集得到了更高的信息增益，所以我们优先选择质量这个特征来划分数据集。

熵是热力学中表征物质状态的参量之一，其物理意义是体系混乱程度的度量，被香农借用过来，作为信息量的度量。著名的熵增原理是这样描述的：

熵增原理就是孤立热力学系统的熵不减少，总是增大或者不变。一个孤立系统不可能朝低熵的状态发展，即不会变得有序。

用白话讲就是，如果没有外力的作用，这个世界将是越来越无序的。人活着，在于尽量让熵变低，即让世界变得更有序，降低不确定性。当我们在消费资源时，是一个增熵的过程。我们把有序的食物变成了无序的垃圾。例如，笔者在写书或读者在看书的过程，可以理解为减熵过程。我们通过写作和阅读，减少了不确定的信息，从而实现了减熵的过程。人生价值的实现，在于消费资源（增熵过程）来获取能量，经过自己的劳动付出（减熵过程），让世界变得更加纯净有序，信息增益（减熵量-增熵量）即是衡量人生价值的尺度。希望笔者在暮年之时，回首往事，能自信地说，我给这个世界带来的信息增益是正数，且已经尽力做到最大了。

### 决策树的创建
决策树的构建过程，就是从训练数据集中归纳出一组分类规则，使它与训练数据矛盾较小的同时具有较强的泛化能力。有了信息增益来量化地选择数据集的划分特征，使决策树的创建过程变得容易了。决策树的创建基本上分以下几步。
- 计算数据集划分前的信息熵。
- 遍历所有未作为划分条件的特征，分别计算根据每个特征划分数据集后的信息熵。
- 选择信息增益最大的特征，并使用这个特征作为数据划分节点来划分数据。
- 递归地处理被划分后的所有子数据集，从未被选择的特征里继续选择最优数据划分特征来划分子数据集。
问题来了，递归过程什么时候结束呢？一般来讲，有两个终止条件，一是所有的特征都用完了，即没有新的特征可以用来进一步划分数据集。二是划分后的信息增益足够小了，这个时候就可以停止递归划分了。针对这个停止条件，需要事先选择信息增益的门限值来作为结束递归的条件。

使用信息增益作为特征选择指标的决策树构建算法，称为ID3算法。

### 离散化
细心的读者可能会发现一个问题：如果一个特征是连续值怎么办呢？假设我们有个精力测试仪器，测出来的是一个0~100的数字，这是个连续值，这个时候怎么用决策树来建模呢？答案是：离散化。我们需要对数据进行离散化处理。例如，当精力指数小于等于40时标识为低，当大于40且小于等于70时标识为中，当大于70时标识为高。经过离散处理后，就可以用来构建决策树了。要离散化成几个类别，这个往往和具体的业务相关。

### 正则项
最大化信息增益来选择特征，在决策树的构建过程中，容易造成优先选择类别最多的特征来进行分类。举一个极端的例子，我们把某个产品的唯一标识符ID作为特征之一加入到数据集中，那么构建决策树时，就会优先选择产品ID来作为划分特征，因为这样划分出来的数据，每个叶子节点只有一个样本，划分后的子数据集最“纯净”，其信息增益最大。

这不是我们希望看到的结果。解决办法是，计算划分后的子数据集的信息熵时，加上一个与类别个数成正比的正则项，来作为最后的信息熵。这样，当算法选择的某个类别较多的特征，使信息熵较小时，由于受到类别个数的正则项惩罚，导致最终的信息熵也比较大。这样通过合适的参数，可以使算法训练得到某种程度的平衡。

另外一个解决办法是使用信息增益比来作为特征选择的标准。

### 基尼不纯度
我们知道，信息熵是衡量信息不确定性的指标，实际上也是衡量信息“纯度”的指标。除此之外，基尼不纯度（Gini impurity）也是衡量信息不纯度的指标，其计算公式如下：


其中，P（x）是样本属于这个类别的概率。如果所有的样本都属于一个类别，此时P（x）=1，则Gini（D）=0，即数据不纯度最低，纯度最高。我们以概率P（x）作为横坐标，以这个类别的基尼不纯度Gini（D）=P（x）（1-P（x））作为纵坐标，在坐标轴上画出其函数关系

## 剪枝算法
使用决策树模型拟合数据时，容易造成过拟合。解决过拟合的方法是对决策树进行剪枝处理。决策树的剪枝有两种思路：前剪枝（Pre-Pruning）和后剪枝（Post-Pruning）。

### 前剪枝
前剪枝是在构造决策树的同时进行剪枝。在决策树的构建过程中，如果无法进一步降低信息熵的情况下，就会停止创建分支。为了避免过拟合，可以设定一个阈值，信息熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。这种方法称为前剪枝。还有一些简单的前剪枝方法，如限制叶子节点的样本个数，当样本个数小于一定的阈值时，即不再继续创建分支。

### 后剪枝
后剪枝是指决策树构造完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，信息熵的增加量是否小于某一阈值。如果小于阈值，则这一组节点可以合并一个节点。后剪枝是目前较普遍的做法。后剪枝的过程是删除一些子树，然后用子树的根节点代替，来作为新的叶子节点。这个新叶子节点所标识的类别通过大多数原则来确定，即把这个叶子节点里样本最多的类别，作为这个叶子节点的类别。

后剪枝算法有很多种，其中常用的一种称为降低错误率剪枝法（Reduced-Error Pruning）。其思路是，自底向上，从已经构建好的完全决策树中找出一个子树，然后用子树的根节点代替这棵子树，作为新的叶子节点。叶子节点所标识的类别通过大多数原则来确定。这样就构建出一个新的简化版的决策树。然后使用交叉验证数据集来测试简化版本的决策树，看看其错误率是不是降低了。如果错误率降低了，则可以使用这个简化版的决策树代替完全决策树，否则还是采用原来的决策树。通过遍历所有的子树，直到针对交叉验证数据集，无法进一步降低错误率为止。

## 算法参数
scikit-learn使用sklearn.tree.DecisionTreeClassifier类来实现决策树分类算法。其中几个典型的参数解释如下。
- criterion：特征选择算法。一种是基于信息熵，另外一种是基于基尼不纯度。有研究表明，这两种算法的差异性不大，对模型的准确性没有太大的影响。相对而言，信息熵运算效率会低一些，因为它有对数运算。更详细的信息，可通过搜索decision tree gini vs.entropy获取。
- splitter：创建决策树分支的选项，一种是选择最优的分支创建原则，另外一种是从排名靠前的特征中，随机选择一个特征来创建分支，这个方法和正则项的效果类似，可以避免过拟合问题。
- max_depth：指定决策树的最大深度。通过指定该参数，用来解决模型过拟合问题。
- min_samples_split：这个参数指定能创建分支的数据集的大小，默认是2。如果一个节点的数据样本个数小于这个数值，则不再创建分支。这也是一种前剪枝的方法。
- min_samples_leaf：创建分支后的节点样本数量必须大于等于这个数值，否则不再创建分支。这也是一种前剪枝的方法。
- max_leaf_nodes：除了限制最小的样本节点个数，该参数可以限制最大的样本节点个数。
- min_impurity_split：可以使用该参数来指定信息增益的阈值。决策树在创建分支时，信息增益必须大于这个阈值，否则不创建分支。
从这些参数可以看到，scikit-learn有一系列的参数用来控制决策树生成的过程，从而解决过拟合问题。



















