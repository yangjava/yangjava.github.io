---
layout: post
categories: [Spark]
description: none
keywords: Spark
---
# Spark核心编程
Spark是基于内存的大数据综合处理框架，为大数据处理提供了一个一体化解决方案，而该方案的设计与实现都是基于一个核心概念展开的，即弹性分布式数据集（Resilient Distributed Dataset，RDD）。

## RDD定义
RDD可以理解为由若干个元素构成的分布式集合以及其上的操作，它是Spark中数据的重要组织形式。与MapReduce不同，Spark针对RDD提供了更加丰富的操作，而不只局限于Map和Reduce，用户利用这些操作可以非常方便地编写出复杂的业务逻辑，然后Spark会自动将RDD中的数据与相关任务分发到集群上，并行化地去执行。

Apache将RDD定义为弹性分布式数据集，它是Spark应用程序中数据的基本组织形式。弹性意味着RDD能够自动地进行内存和磁盘数据存储的切换，并且具有非常高的容错性；分布式说明RDD是一个存储在多个节点上的海量数据集合。RDD是一种高度受限的共享内存模型，即RDD是只读的记录分区的集合。RDD具有自动容错、位置感知调度和可伸缩性等数据流模型的特点。

