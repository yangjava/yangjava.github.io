---
layout: post
categories: [Linux]
description: none
keywords: Linux
---
# Linux源码内存管理2mmap的系统调用

## 内存使用方法
1.创建内存映射
```
#include <sys/mman.h>
void *mmap(void *addr，size_t length，int prot，int flags，int fd，off_t offset);
```
mmap：进程创建匿名的内存映射，把内存的物理页映射到进程的虚拟地址空间。进程把文件映射到进程的虚拟地址空间，可以像访问内存一样访问文件，不需要调用系统调用read()/write()访问文件，从而避免用户模式和内核模式之间的切换，提高读写文件速度。两个进程针对同一个文件创建共享的内存映射，实现共享内存。

2.删除内存映射
```
#include <sys/mman.h>
int munmap(void *addr, size_t len);
```
mumap：该调用在进程地址空间中解除一个映射关系，addr是调用mmap()时返回的地址，len是映射区的大小。当映射关系解除后，对原来映射地址的访问将导致段错误发生。

3.设置虚拟内存区域的访问权限
```
#include <sys/mman.h>
int mprotect(void *addr, size_t len, int prot);
```
mprotect：把自start开始的、长度为len的内存区的保护属性修改为prot指定的值。
prot可以取以下几个值，并且可以用“|”将几个属性合起来使用：
1）PROT_READ：表示内存段内的内容可写；
2）PROT_WRITE：表示内存段内的内容可读；
3）PROT_EXEC：表示内存段中的内容可执行；
4）PROT_NONE：表示内存段中的内容根本没法访问。
需要指出的是，指定的内存区间必须包含整个内存页（4K）。区间开始的地址start必须是一个内存页的起始地址，并且区间长度len必须是页大小的整数倍。

## mmap的系统调用
0.查找mmap在内核中的系统调用函数
我现在用的内核版是4.19.40，首先在应用层参考上面解析编写一个mmap使用代码，然后编译成程序，在使用strace工具跟踪其函数调用，可以发现mmap也是调用底层的mmap系统调用，然后我们寻找一下底层的带6个参数的mmap系统调用有哪些：

可以看到，arm64和X86的系统调用位于不同文件。

1.mmap的系统调用
x86的位于arch/x86/kernel/sys_x86_64.c文件，如下所示：
```
SYSCALL_DEFINE6(mmap, unsigned long, addr, unsigned long, len,
		unsigned long, prot, unsigned long, flags,
		unsigned long, fd, unsigned long, off)
{
	long error;
	error = -EINVAL;
	//检查偏移是不是页的整数倍，如果不是页的整数倍，直接返回-EINVAL
	//如果是也得整数倍，那么把偏移转换成页为单位的偏移，然后继续往下走
	if (off & ~PAGE_MASK)
		goto out;

	error = ksys_mmap_pgoff(addr, len, prot, flags, fd, off >> PAGE_SHIFT);
out:
	return error;
}

```

arm64的位于arch/arm64/kernel/sys.c文件，如下所示：
```
SYSCALL_DEFINE6(mmap, unsigned long, addr, unsigned long, len,
		unsigned long, prot, unsigned long, flags,
		unsigned long, fd, off_t, off)
{
	//检查偏移是不是页的整数倍，如果不是页的整数倍，直接返回-EINVAL
	//如果是也得整数倍，那么把偏移转换成页为单位的偏移，然后继续往下走
	if (offset_in_page(off) != 0)
		return -EINVAL;

	return ksys_mmap_pgoff(addr, len, prot, flags, fd, off >> PAGE_SHIFT);
}
```
然后都是进入ksys_mmap_pgoff：
```
unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
			      unsigned long prot, unsigned long flags,
			      unsigned long fd, unsigned long pgoff)
{
	struct file *file = NULL;
	unsigned long retval;

	if (!(flags & MAP_ANONYMOUS)) {//如果不是匿名映射
		audit_mmap_fd(fd, flags);
		file = fget(fd);//文件映射，根据文件描述符在进程的打开文件表中找到file实例
		if (!file)
			return -EBADF;
		if (is_file_hugepages(file))
			len = ALIGN(len, huge_page_size(hstate_file(file)));
		retval = -EINVAL;
		if (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))
			goto out_fput;
	} else if (flags & MAP_HUGETLB) {//如果是匿名巨型页映射
		struct user_struct *user = NULL;
		struct hstate *hs;

		hs = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
		if (!hs)
			return -EINVAL;

		len = ALIGN(len, huge_page_size(hs));
		/*
		 * VM_NORESERVE is used because the reservations will be
		 * taken when vm_ops->mmap() is called
		 * A dummy user value is used because we are not locking
		 * memory so no accounting is necessary
		 */
		//在hugetlbfs文件系统中创建文件“anon_hugepage”
		file = hugetlb_file_setup(HUGETLB_ANON_FILE, len,
				VM_NORESERVE,
				&user, HUGETLB_ANONHUGE_INODE,
				(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
		if (IS_ERR(file))
			return PTR_ERR(file);
	}

	flags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);

	retval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);
out_fput:
	if (file)
		fput(file);
	return retval;
}

```
然后进入vm_mmap_pgoff：
```
unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,
	unsigned long len, unsigned long prot,
	unsigned long flag, unsigned long pgoff)
{
	unsigned long ret;
	struct mm_struct *mm = current->mm;
	unsigned long populate;
	LIST_HEAD(uf);//初始化userfaultfd链表

	ret = security_mmap_file(file, prot, flag);//security linux安全相关的，一般不会开，返回值为0
	if (!ret) {
		if (down_write_killable(&mm->mmap_sem))//以写者身份申请读写信号量
			return -EINTR;
		ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,//创建内存映射主要工作在此函数中进行
				    &populate, &uf);
		up_write(&mm->mmap_sem);//释放读写信号量
		userfaultfd_unmap_complete(mm, &uf);//等待userfaultfd处理完成
		if (populate)
			mm_populate(ret, populate);//如果调用者要求把也锁定在内存中，或要求
	}
	return ret;
}

```
我们讲解最重要的do_mmap_pgoff函数：
```
static inline unsigned long
do_mmap_pgoff(struct file *file, unsigned long addr,
	unsigned long len, unsigned long prot, unsigned long flags,
	unsigned long pgoff, unsigned long *populate,
	struct list_head *uf)
{
	return do_mmap(file, addr, len, prot, flags, 0, pgoff, populate, uf);
}

```
然后进入do_mmap：
```

/*
 * The caller must hold down_write(&current->mm->mmap_sem).
 */
unsigned long do_mmap(struct file *file, unsigned long addr,
			unsigned long len, unsigned long prot,
			unsigned long flags, vm_flags_t vm_flags,
			unsigned long pgoff, unsigned long *populate,
			struct list_head *uf)
{
	struct mm_struct *mm = current->mm;
	int pkey = 0;

	*populate = 0;

	if (!len)
		return -EINVAL;

	/*
	 * Does the application expect PROT_READ to imply PROT_EXEC?
	 *
	 * (the exception is when the underlying filesystem is noexec
	 *  mounted, in which case we dont add PROT_EXEC.)
	 */ //如果进程带有READ_IMPLIES_EXEC标记且文件系统是可执行的，则这段内存空间使用READ的属性会附带增加EXEC属性。
	if ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))
		if (!(file && path_noexec(&file->f_path)))
			prot |= PROT_EXEC;

	/* force arch specific MAP_FIXED handling in get_unmapped_area */
	if (flags & MAP_FIXED_NOREPLACE)
		flags |= MAP_FIXED;

	//如果不是使用固定地址，则使用的addr会进行向下页对齐
	if (!(flags & MAP_FIXED))
		addr = round_hint_to_min(addr);

	/* Careful about overflows.. */
	len = PAGE_ALIGN(len);//申请内存大小页对齐
	if (!len)
		return -ENOMEM;

	/* offset overflow? */
	if ((pgoff + (len >> PAGE_SHIFT)) < pgoff)//判断申请的内存是否溢出
		return -EOVERFLOW;

	/* Too many mappings? */
	if (mm->map_count > sysctl_max_map_count)//判断内存映射次数是否达到上限
		return -ENOMEM;

	/* Obtain the address to map to. we verify (or select) it and ensure
	 * that it represents a valid section of the address space.
	 */
	addr = get_unmapped_area(file, addr, len, pgoff, flags);//获取未映射的地址空间并且验证它
	if (offset_in_page(addr))
		return addr;

	if (flags & MAP_FIXED_NOREPLACE) {
		struct vm_area_struct *vma = find_vma(mm, addr);

		if (vma && vma->vm_start < addr + len)
			return -EEXIST;
	}

	if (prot == PROT_EXEC) {
		pkey = execute_only_pkey(mm);
		if (pkey < 0)
			pkey = 0;
	}

	/* Do simple checking here so the lower-level routines won't have
	 * to. we assume access permissions have been handled by the open
	 * of the memory object, so we don't do any here.
	 */
	 //计算虚拟内存的标志并且下面做一些简单的检查
	vm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |
			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;

	if (flags & MAP_LOCKED)
		if (!can_do_mlock())
			return -EPERM;

	if (mlock_future_check(mm, vm_flags, len))
		return -EAGAIN;

	//从下面的代码可以看到，各种映射之间，主要是vm_flag之间的差别
	if (file) {
		struct inode *inode = file_inode(file);
		unsigned long flags_mask;

		if (!file_mmap_ok(file, inode, pgoff, len))
			return -EOVERFLOW;

		flags_mask = LEGACY_MAP_MASK | file->f_op->mmap_supported_flags;

		switch (flags & MAP_TYPE) {
		case MAP_SHARED:
			/*
			 * Force use of MAP_SHARED_VALIDATE with non-legacy
			 * flags. E.g. MAP_SYNC is dangerous to use with
			 * MAP_SHARED as you don't know which consistency model
			 * you will get. We silently ignore unsupported flags
			 * with MAP_SHARED to preserve backward compatibility.
			 */
			flags &= LEGACY_MAP_MASK;
			/* fall through */
		case MAP_SHARED_VALIDATE:
			if (flags & ~flags_mask)
				return -EOPNOTSUPP;
			if ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure we don't allow writing to an append-only
			 * file..
			 */
			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure there are no mandatory locks on the file.
			 */
			if (locks_verify_locked(file))
				return -EAGAIN;

			vm_flags |= VM_SHARED | VM_MAYSHARE;
			if (!(file->f_mode & FMODE_WRITE))
				vm_flags &= ~(VM_MAYWRITE | VM_SHARED);

			/* fall through */
		case MAP_PRIVATE:
			if (!(file->f_mode & FMODE_READ))
				return -EACCES;
			if (path_noexec(&file->f_path)) {
				if (vm_flags & VM_EXEC)
					return -EPERM;
				vm_flags &= ~VM_MAYEXEC;
			}

			if (!file->f_op->mmap)
				return -ENODEV;
			if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
				return -EINVAL;
			break;

		default:
			return -EINVAL;
		}
	} else {
		switch (flags & MAP_TYPE) {
		case MAP_SHARED:
			if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
				return -EINVAL;
			/*
			 * Ignore pgoff.
			 */
			pgoff = 0;
			vm_flags |= VM_SHARED | VM_MAYSHARE;
			break;
		case MAP_PRIVATE:
			/*
			 * Set pgoff according to addr for anon_vma.
			 */
			pgoff = addr >> PAGE_SHIFT;
			break;
		default:
			return -EINVAL;
		}
	}

	/*
	 * Set 'VM_NORESERVE' if we should not account for the
	 * memory use of this mapping.
	 */
	if (flags & MAP_NORESERVE) {
		/* We honor MAP_NORESERVE if allowed to overcommit */
		if (sysctl_overcommit_memory != OVERCOMMIT_NEVER)
			vm_flags |= VM_NORESERVE;

		/* hugetlb applies strict overcommit unless MAP_NORESERVE */
		if (file && is_file_hugepages(file))
			vm_flags |= VM_NORESERVE;
	}

	addr = mmap_region(file, addr, len, vm_flags, pgoff, uf);//真正创建虚拟内存区域
	if (!IS_ERR_VALUE(addr) &&
	    ((vm_flags & VM_LOCKED) ||
	     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))
		*populate = len;
	return addr;
}

```
do_mmap_pgoff这个函数主要做了两件事，get_unmapped_area获取未映射地址，mmap_region映射。
先看下get_unmapped_area ，他是先找到mm_struct的get_unmapped_area成员，再去执行他：
```
unsigned long
get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,
		unsigned long pgoff, unsigned long flags)
{
	......
	get_area = current->mm->get_unmapped_area;
	......
	addr = get_area(file, addr, len, pgoff, flags);
	......
}
```
再看mmap_region的实现：
```
unsigned long mmap_region(struct file *file, unsigned long addr,
		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
		struct list_head *uf)
{
	struct mm_struct *mm = current->mm;
	struct vm_area_struct *vma, *prev;
	int error;
	struct rb_node **rb_link, *rb_parent;
	unsigned long charged = 0;

	/* Check against address space limit. */
	//检查进程虚拟内存限制
	if (!may_expand_vm(mm, vm_flags, len >> PAGE_SHIFT)) {
		unsigned long nr_pages;

		/*
		 * MAP_FIXED may remove pages of mappings that intersects with
		 * requested mapping. Account for the pages it would unmap.
		 */
		nr_pages = count_vma_pages_range(mm, addr, addr + len);

		if (!may_expand_vm(mm, vm_flags,
					(len >> PAGE_SHIFT) - nr_pages))
			return -ENOMEM;
	}

	/* Clear old maps */
	//检查是否和旧的虚拟内存区域有重叠并且把重叠部分释放掉
	while (find_vma_links(mm, addr, addr + len, &prev, &rb_link,
			      &rb_parent)) {
		if (do_munmap(mm, addr, len, uf))
			return -ENOMEM;
	}

	/*
	 * Private writable mapping: check memory availability
	 */
	//如果有私有的可写的内存映射，可以先修改标志位
	if (accountable_mapping(file, vm_flags)) {
		charged = len >> PAGE_SHIFT;
		if (security_vm_enough_memory_mm(mm, charged))
			return -ENOMEM;
		vm_flags |= VM_ACCOUNT;
	}

	/*
	 * Can we just expand an old mapping?
	 */
	//如果可以和已有的虚拟内存合并，则合并并且使用该vma，然后结束
	vma = vma_merge(mm, prev, addr, addr + len, vm_flags,
			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX);
	if (vma)
		goto out;
	//如果不可以合并，继续往下走
	/*
	 * Determine the object being mapped and call the appropriate
	 * specific mapper. the address has already been validated, but
	 * not unmapped, but the maps are removed from the list.
	 */
	//创建一个虚拟内存区域，也就是申请一个vma
	vma = vm_area_alloc(mm);
	if (!vma) {
		error = -ENOMEM;
		goto unacct_error;
	}

	vma->vm_start = addr;
	vma->vm_end = addr + len;
	vma->vm_flags = vm_flags;
	vma->vm_page_prot = vm_get_page_prot(vm_flags);
	vma->vm_pgoff = pgoff;

	if (file) {//文件映射
		if (vm_flags & VM_DENYWRITE) {
			error = deny_write_access(file);
			if (error)
				goto free_vma;
		}
		if (vm_flags & VM_SHARED) {
			error = mapping_map_writable(file->f_mapping);
			if (error)
				goto allow_write_and_free_vma;
		}

		/* ->mmap() can change vma->vm_file, but must guarantee that
		 * vma_link() below can deny write-access if VM_DENYWRITE is set
		 * and map writably if VM_SHARED is set. This usually means the
		 * new file must not have been exposed to user-space, yet.
		 */
		vma->vm_file = get_file(file);
		error = call_mmap(file, vma);//调用文件系统提供的文件映射函数
		if (error)
			goto unmap_and_free_vma;

		/* Can addr have changed??
		 *
		 * Answer: Yes, several device drivers can do it in their
		 *         f_op->mmap method. -DaveM
		 * Bug: If addr is changed, prev, rb_link, rb_parent should
		 *      be updated for vma_link()
		 */
		WARN_ON_ONCE(addr != vma->vm_start);
		
		//文件系统提供的文件映射函数可能会修改映射的一些参数。在这里需要在调用vma_link前回置
		addr = vma->vm_start;
		vm_flags = vma->vm_flags;
	} else if (vm_flags & VM_SHARED) {//匿名映射
		error = shmem_zero_setup(vma);//这里是映射到了/dev/zero这个文件，很巧妙，不需要提前将页面清0
		if (error)
			goto free_vma;
	} else {
		vma_set_anonymous(vma);
	}

	vma_link(mm, vma, prev, rb_link, rb_parent);//将vma链接回进程的mm_struct结构体中
	/* Once vma denies write, undo our temporary denial count */
	if (file) {
		if (vm_flags & VM_SHARED)
			mapping_unmap_writable(file->f_mapping);
		if (vm_flags & VM_DENYWRITE)
			allow_write_access(file);
	}
	file = vma->vm_file;
out:
	perf_event_mmap(vma);//perf在这里安插了个event

	vm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);//进程内存状态统计，在开启了proc才会有
	if (vm_flags & VM_LOCKED) {
		if ((vm_flags & VM_SPECIAL) || vma_is_dax(vma) ||
					is_vm_hugetlb_page(vma) ||
					vma == get_gate_vma(current->mm))
			vma->vm_flags &= VM_LOCKED_CLEAR_MASK;
		else
			mm->locked_vm += (len >> PAGE_SHIFT);
	}

	if (file)
		uprobe_mmap(vma);

	/*
	 * New (or expanded) vma always get soft dirty status.
	 * Otherwise user-space soft-dirty page tracker won't
	 * be able to distinguish situation when vma area unmapped,
	 * then new mapped in-place (which must be aimed as
	 * a completely new data area).
	 */
	vma->vm_flags |= VM_SOFTDIRTY;//设置标志位为脏状态

	vma_set_page_prot(vma);//新映射的区域必须将其设置为一个全新的数据区域（脏）

	return addr;

unmap_and_free_vma:
	vma->vm_file = NULL;
	fput(file);

	/* Undo any partial mapping done by a device driver. */
	unmap_region(mm, vma, prev, vma->vm_start, vma->vm_end);
	charged = 0;
	if (vm_flags & VM_SHARED)
		mapping_unmap_writable(file->f_mapping);
allow_write_and_free_vma:
	if (vm_flags & VM_DENYWRITE)
		allow_write_access(file);
free_vma:
	vm_area_free(vma);
unacct_error:
	if (charged)
		vm_unacct_memory(charged);
	return error;
}

```
现在，我们看看匿名映射的函数shmem_zero_setup到底做了什么，其实匿名页实际也映射了文件，只是映射到了/dev/zero上，这样有个好处是，不需要对所有页面进行提前置0，只有当访问到某具体页面的时候才会申请一个0页。
```
/**
 * shmem_zero_setup - setup a shared anonymous mapping
 * @vma: the vma to be mmapped is prepared by do_mmap_pgoff
 */
int shmem_zero_setup(struct vm_area_struct *vma)
{
	struct file *file;
	loff_t size = vma->vm_end - vma->vm_start;

	/*
	 * Cloning a new file under mmap_sem leads to a lock ordering conflict
	 * between XFS directory reading and selinux: since this file is only
	 * accessible to the user through its mapping, use S_PRIVATE flag to
	 * bypass file security, in the same way as shmem_kernel_file_setup().
	 */
	file = shmem_kernel_file_setup("dev/zero", size, vma->vm_flags);
	if (IS_ERR(file))
		return PTR_ERR(file);

	if (vma->vm_file)
		fput(vma->vm_file);
	vma->vm_file = file;
	vma->vm_ops = &shmem_vm_ops;

	if (IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE) &&
			((vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK) <
			(vma->vm_end & HPAGE_PMD_MASK)) {
		khugepaged_enter(vma, vma->vm_flags);
	}

	return 0;
}

```
其实说白了，mmap就是在进程mm中创建或者扩展一个vma映射到某个文件，而共享、私有、文件、匿名这些mmap所具有的属性是在哪里体现的呢？上面的源码在不断的设置一些标记位，这些标记位就决定了进程在访问这些内存时内核的行为，mmap仅负责创建一个映射而已。

下面是mmap系统调用的函数调用以及返回情况说明：
```
SYSCALL_DEFINE6(mmap,
		
		offset_in_page(off) //检查偏移是不是页的整数倍，
		ksys_mmap_pgoff(addr, len, prot, flags, fd, off >> PAGE_SHIFT);
				
				if (!(flags & MAP_ANONYMOUS)) {//如果不是匿名映射
					fget(fd);//文件映射，根据文件描述符在进程的打开文件表中找到file实例
				else if (flags & MAP_HUGETLB) {//如果是匿名巨型页映射
					hugetlb_file_setup//在hugetlbfs文件系统中创建文件“anon_hugepage”				
				vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);//
						LIST_HEAD(uf);//初始化userfaultfd链表
						security_mmap_file//security linux安全相关的，一般不会开，返回值为0
						down_write_killable(&mm->mmap_sem))//以写者身份申请读写信号量
						do_mmap_pgoff//创建内存映射主要工作在此函数中进行
								
								do_mmap(file, addr, len, prot, flags, 0, pgoff, populate, uf);
								
										round_hint_to_min(addr);//如果不是使用固定地址，则使用的addr会进行向下页对齐
										PAGE_ALIGN(len);//申请内存大小页对齐
										//判断申请的内存是否溢出
										//判断内存映射次数是否达到上限
										get_unmapped_area//获取未映射的地址空间并且验证它
										find_vma(mm, addr);//查找申请的虚拟地址所在的vma，用来判断vma是否合理
										calc_vm_prot_bits//计算虚拟内存的标志并且下面做一些简单的检查
										//根据申请的内存确定vm_flag
										mmap_region//真正创建虚拟内存区域
												
												//检查进程虚拟内存大小是否超出限制
												while（find_vma_links）//检查是否和旧的虚拟内存区域有重叠
													do_munmap//并且把重叠部分释放掉
												accountable_mapping//如果有私有的可写的内存映射，可以先修改标志位
												vma_merge//如果可以和已有的虚拟内存合并，则合并并且使用该vma，然后结束
												//如果不可以合并，继续往下走
												vm_area_alloc//创建一个虚拟内存区域，也就是申请一个vma
												if (file) {//文件映射
													call_mmap(file, vma);//调用文件系统提供的文件映射函数
												else if (vm_flags & VM_SHARED) {//匿名映射
													shmem_zero_setup(vma);//匿名映射操作函数
													
															shmem_kernel_file_setup("dev/zero", size, vma->vm_flags);//这里是映射到了/dev/zero这个文件
													
												vma_link//将vma链接回进程的mm_struct结构体中
												perf_event_mmap(vma);//perf在这里安插了个event
												vm_stat_account//进程内存状态统计，在开启了proc才会有
												vma_set_page_prot(vma);//新映射的区域必须将其设置为一个全新的数据区域（脏）
													
								
						up_write(&mm->mmap_sem);//释放读写信号量
						mm_populate(ret, populate);//如果调用者要求把也锁定在内存中，或要求

```

## munmap的系统调用
0.查找munmap在内核中的系统调用函数
```
#include <sys/mman.h>
int munmap(void *addr, size_t len);
```
我现在用的内核版是4.19.40，首先在应用层参考上面解析编写一个munmap使用代码，然后编译成程序，在使用strace工具跟踪其函数调用，可以发现munmap也是调用底层的munmap系统调用，然后我们寻找一下底层的带2个参数的munmap系统调用有哪些：

1.munmap的系统调用
```
SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)
{
	profile_munmap(addr);//使用内核通知链唤醒munmap_notifier
	return vm_munmap(addr, len);
}
```
vm_munmap函数跟上次的vm_mmap_pgoff函数很相似，
```
int vm_munmap(unsigned long start, size_t len)
{
	int ret;
	struct mm_struct *mm = current->mm;
	LIST_HEAD(uf);//初始化userfaultfd链表

	if (down_write_killable(&mm->mmap_sem))//以写者身份申请读写信号量
		return -EINTR;

	ret = do_munmap(mm, start, len, &uf);
	up_write(&mm->mmap_sem);//释放读写信号量
	userfaultfd_unmap_complete(mm, &uf);//等待userfaultfd处理完成
	return ret;
}
```
然后进入do_munmap：
```
/* Munmap is split into 2 main parts -- this part which finds
 * what needs doing, and the areas themselves, which do the
 * work.  This now handles partial unmappings.
 * Jeremy Fitzhardinge <jeremy@goop.org>
 */
int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
	      struct list_head *uf)
{
	unsigned long end;
	struct vm_area_struct *vma, *prev, *last;

	if ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)
		return -EINVAL;

	//要ummap的长度也是page size对齐的
	len = PAGE_ALIGN(len);
	if (len == 0)
		return -EINVAL;

	/* Find the first overlapping VMA */
	vma = find_vma(mm, start);//找到起始地址是落在哪个vma内
	if (!vma)
		return 0;
	prev = vma->vm_prev;
	/* we have  start < vma->vm_end  */

	/* if it doesn't overlap, we have nothing.. */
	//如果要释放空间的结束地址都小于vma的起始地址，说明这两者就没有重叠，直接退出
	end = start + len;
	if (vma->vm_start >= end)
		return 0;

	/*
	 * If we need to split any vma, do it now to save pain later.
	 *
	 * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially
	 * unmapped vm_area_struct will remain in use: so lower split_vma
	 * places tmp vma above, and higher split_vma places tmp vma below.
	 */
	//如果要释放的内存起始地址在vma中间，不在开头的位置
	if (start > vma->vm_start) {
		int error;

		/*
		 * Make sure that map_count on return from munmap() will
		 * not exceed its limit; but let map_count go just above
		 * its limit temporarily, to help free resources as expected.
		 */
		//确保从map_count不会超过限制
		if (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)
			return -ENOMEM;
		
		//我们要在start处分裂vma成两份，因为我们只需要释放start后面的内存
		error = __split_vma(mm, vma, start, 0);
		if (error)
			return error;
		prev = vma;
	}

	/* Does it split the last one? */
	last = find_vma(mm, end);//找到结束地址是落在哪个vma内
	if (last && end > last->vm_start) {
		//我们要在last处分裂vma成两份，因为我们只需要释放last前面的内存
		int error = __split_vma(mm, last, end, 1);//分裂内存区域
		if (error)
			return error;
	}
	vma = prev ? prev->vm_next : mm->mmap;

	if (unlikely(uf)) {
		/*
		 * If userfaultfd_unmap_prep returns an error the vmas
		 * will remain splitted, but userland will get a
		 * highly unexpected error anyway. This is no
		 * different than the case where the first of the two
		 * __split_vma fails, but we don't undo the first
		 * split, despite we could. This is unlikely enough
		 * failure that it's not worth optimizing it for.
		 */
		int error = userfaultfd_unmap_prep(vma, start, end, uf);
		if (error)
			return error;
	}

	/*
	 * unlock any mlock()ed ranges before detaching vmas
	 */
	if (mm->locked_vm) {//如果这段要释放的空间是lock的
		struct vm_area_struct *tmp = vma;
		while (tmp && tmp->vm_start < end) {
			if (tmp->vm_flags & VM_LOCKED) {
				mm->locked_vm -= vma_pages(tmp);
				munlock_vma_pages_all(tmp);//解除锁定
			}
			tmp = tmp->vm_next;
		}
	}

	/*
	 * Remove the vma's, and unmap the actual pages
	 */
	detach_vmas_to_be_unmapped(mm, vma, prev, end);//把要删除的vma区域移出红黑树
	unmap_region(mm, vma, prev, start, end);//针对删除的目标，在进程的页表和cpu缓存中删除映射

	arch_unmap(mm, vma, start, end);//进行处理器架构的特定操作

	/* Fix up all other VM information */
	remove_vma_list(mm, vma);//从进程虚拟内存区域中删除要删除的vma区域

	return 0;
}

```
这里主要看detach_vmas_to_be_unmapped，unmap_region，arch_unmap，remove_vma_list这4个函数。
先看detach_vmas_to_be_unmapped是怎么把要删除的vma区域移出红黑树的：
```
/*
 * Create a list of vma's touched by the unmap, removing them from the mm's
 * vma list as we go..
 */
static void
detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,
	struct vm_area_struct *prev, unsigned long end)
{
	struct vm_area_struct **insertion_point;
	struct vm_area_struct *tail_vma = NULL;

	insertion_point = (prev ? &prev->vm_next : &mm->mmap);
	vma->vm_prev = NULL;
	do {
		vma_rb_erase(vma, &mm->mm_rb);//从红黑树中删除一个个vma
		mm->map_count--;
		tail_vma = vma;
		vma = vma->vm_next;
	} while (vma && vma->vm_start < end);
	*insertion_point = vma;
	if (vma) {
		vma->vm_prev = prev;
		vma_gap_update(vma);
	} else
		mm->highest_vm_end = prev ? vm_end_gap(prev) : 0;
	tail_vma->vm_next = NULL;

	/* Kill the cache */
	vmacache_invalidate(mm);//释放vma cache 信号量
}

```
在看unmap_region是怎么删除映射的：
```

/*
 * Get rid of page table information in the indicated region.
 *
 * Called with the mm semaphore held.
 */
static void unmap_region(struct mm_struct *mm,
		struct vm_area_struct *vma, struct vm_area_struct *prev,
		unsigned long start, unsigned long end)
{
	struct vm_area_struct *next = prev ? prev->vm_next : mm->mmap;
	struct mmu_gather tlb;

	lru_add_drain();//当前CPU实现缓存的刷新
	tlb_gather_mmu(&tlb, mm, start, end);//初始化一个mmu_gather结构体，用于拆解页表
	update_hiwater_rss(mm);//更新高水位线上的rss，也就是已经占用的物理页页数
	unmap_vmas(&tlb, vma, start, end);//清空线性地址空间的所有页表项
	
	//回收上一步已经清空的进程页表
	free_pgtables(&tlb, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS,
				 next ? next->vm_start : USER_PGTABLES_CEILING);
	tlb_finish_mmu(&tlb, start, end);//刷新TLB，释放页框
}

```
再看arch_unmap是到底做了什么：

从上图可以看到，不同架构的cpu有不同的调用函数，还有很多架构没有定义这个函数，就用到了通用的arch_unmap，我们先在include/asm-generic/mm_hooks.h文件中看看通用的函数做了什么：
```
static inline void arch_unmap(struct mm_struct *mm,
			struct vm_area_struct *vma,
			unsigned long start, unsigned long end)
{
}

```
竟然是一个空函数，啥都没有做，我们在看看那X86架构做了什么：
```
static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
			      unsigned long start, unsigned long end)
{
	/*
	 * mpx_notify_unmap() goes and reads a rarely-hot
	 * cacheline in the mm_struct.  That can be expensive
	 * enough to be seen in profiles.
	 *
	 * The mpx_notify_unmap() call and its contents have been
	 * observed to affect munmap() performance on hardware
	 * where MPX is not present.
	 *
	 * The unlikely() optimizes for the fast case: no MPX
	 * in the CPU, or no MPX use in the process.  Even if
	 * we get this wrong (in the unlikely event that MPX
	 * is widely enabled on some system) the overhead of
	 * MPX itself (reading bounds tables) is expected to
	 * overwhelm the overhead of getting this unlikely()
	 * consistently wrong.
	 */
	//判断CPU中有没有MPX，或者进程中有没有使用MPX
	if (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))
		//读取一个很少热的cacheline，通知mpx
		mpx_notify_unmap(mm, vma, start, end);
}

```
最后看看那remove_vma_list是怎么从进程虚拟内存区域中删除要删除的vma区域
```
/*
 * Ok - we have the memory areas we should free on the vma list,
 * so release them, and do the vma updates.
 *
 * Called with the mm semaphore held.
 */
static void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)
{
	unsigned long nr_accounted = 0;

	/* Update high watermark before we lower total_vm */
	update_hiwater_vm(mm);//更新高水位线上的内存使用情况
	do {
		long nrpages = vma_pages(vma);

		if (vma->vm_flags & VM_ACCOUNT)
			nr_accounted += nrpages;
		vm_stat_account(mm, vma->vm_flags, -nrpages);
		vma = remove_vma(vma);//从进程的mm_struct的vm_area_struct链表移除删除的vma
	} while (vma);
	vm_unacct_memory(nr_accounted);//减少已经使用的虚拟内存空间
	validate_mm(mm);//如果开启debug则查看mm_struct的状态，否则极易啥也不干
}

```
其实说白了，要先判断要释放的内存区域块有一下几种情况：
1.要删除的内存区域块刚刚好覆盖了n（n是整数）个vma，我们只要删除这n块vma就行了。
2.要删除的内存区域块的起始位置位于某个vma中间，需要把这个vma分为2块vma，前面的vma不需要删除，后面的vma需要删除。
3.要删除的内存区域块的结束位置位于某个vma中间，需要把这个vma分为2块vma，后面的vma不需要删除，前面的vma需要删除。
3.要删除的内存区域块的起始位置和结束位置都位于某个vma中间，需要把这两个vma都分为2块vma，第一个vma分的两块vma，前面的vma不需要删除，后面的vma需要删除；第二个vma分的两块vma，后面的vma不需要删除，前面的vma需要删除。
下面是munmap系统调用的函数调用以及返回情况说明：
```
SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)

		profile_munmap//使用内核通知链唤醒munmap_notifier
		vm_munmap
		
				LIST_HEAD(uf);//初始化userfaultfd链表
				down_write_killable(&mm->mmap_sem))//以写者身份申请读写信号量
				do_munmap//主要函数
				
						find_vma(mm, start);//找到起始地址是落在哪个vma内
						//如果要释放空间的结束地址都小于vma的起始地址，说明这两者就没有重叠，直接退出
						find_vma(mm, start);//找到起始地址是落在哪个vma内
						__split_vma//如果要释放的内存起始地址在vma中间，则分裂vma
						find_vma(mm, end);//找到结束地址是落在哪个vma内
						__split_vma//如果要释放的内存结束地址在vma中间，则分裂vma
						munlock_vma_pages_all(tmp);//如果虚拟内存锁定在内存中，则解除锁定
						detach_vmas_to_be_unmapped(mm, vma, prev, end);//把要删除的vma区域移出红黑树
						
								do{vma_rb_erase} while //从红黑树中删除一个个vma
								vmacache_invalidate(mm);//释放vma cache 信号量
								
						unmap_region(mm, vma, prev, start, end);//针对删除的目标，在进程的页表和cpu缓存中删除映射
						
								lru_add_drain();//当前CPU实现缓存的刷新
								tlb_gather_mmu(&tlb, mm, start, end);//初始化一个mmu_gather结构体，用于拆解页表
								update_hiwater_rss(mm);//更新高水位线上的rss，也就是已经占用的物理页页数
								unmap_vmas(&tlb, vma, start, end);//清空线性地址空间的所有页表项
								free_pgtables//回收上一步已经清空的进程页表
								tlb_finish_mmu(&tlb, start, end);//刷新TLB，释放页框
								
						arch_unmap(mm, vma, start, end);//进行处理器架构的特定操作		
						remove_vma_list(mm, vma);//从进程虚拟内存区域中删除要删除的vma区域
								
								update_hiwater_vm(mm);//更新高水位线上的内存使用情况
								do{remove_vma}while//从进程的mm_struct的vm_area_struct链表移除删除的vma
								vm_unacct_memory(nr_accounted);//减少已经使用的虚拟内存空间
								validate_mm(mm);//如果开启debug则查看mm_struct的状态，否则极易啥也不干
						
				up_write(&mm->mmap_sem);//释放读写信号量
				userfaultfd_unmap_complete(mm, &uf);//等待userfaultfd处理完成

```



