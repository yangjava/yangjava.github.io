---
layout: post
categories: [Lucene]
description: none
keywords: Lucene
---
# Lucene分析器
分析器是lucene中非常重要的一个组件，分析(Analysis)在Lucene中指的是将域(Field)文本转换为最基本的索引表示单元—项(Term)的过程。

## 标准分析器
分析器(Analyzer)对分析操作进行了封装，通过执行一系列操作，将文本语汇单元化。

这些操作包括提取单词、去除标点符号、去除语汇单元上的音调符号、将大写字母转换成小写、移除常用词、将单词转换为词干（词干还原）等。

示例代码如下：
```java

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;

public class StandardAnalyzerDemo {

    //默认分析器
    public static void main(String[] args) throws Exception {
        //创建一个标准分析器对象
        Analyzer analyzer = new StandardAnalyzer();
        //获得tokenStream对象
        //第一个参数：域名，可以随便给一个
        //第二个参数：要分析的文本内容
        TokenStream tokenStream = analyzer.tokenStream("test", "The Spring Framework provides a comprehensive programming and configuration model.");
        //添加一个引用，可以获得每个关键词
        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);
        //添加一个偏移量的引用，记录了关键词的开始位置以及结束位置
        OffsetAttribute offsetAttribute = tokenStream.addAttribute(OffsetAttribute.class);
        //将指针调整到列表的头部
        tokenStream.reset();
        //遍历关键词列表，通过incrementToken方法判断列表是否结束
        while (tokenStream.incrementToken()) {
            //关键词的起始位置
            System.out.println("start->" + offsetAttribute.startOffset());
            //取关键词
            System.out.println(charTermAttribute);
            //结束位置
            System.out.println("end->" + offsetAttribute.endOffset());
        }
        tokenStream.close();
    }
}
```

## lucene中的分析器
Lucene Analyzer分析器设计遵循一个基本过程：

- 文档
- token对象
- 过滤器链表，比如，小写过滤，停用词过滤，自定义逻辑过滤
- 生成的token对象将由索引模块，生成关于term分词的倒排索引
- 无论是内置的还是自定义的Analyzer分析器，底层都由三个基本模块组成，分别是：字符过滤器，tokenize和token 过滤器。

分析器可能会做的事情有：将文本拆分为单词，去除标点，将字母变为小写，去除停用词，词干还原，词形归并，敏感词过滤等等。

lucene中默认自带的分析器有4个：WhitespaceAnalyzer,SimpleAnalyzer,StopAnalyzer, StandardAnalyzer，分别用来过滤空白字符，过滤空白符并自动变小写，去掉停用词，标准化分词。

其中，最常用的是StandardAnalyzer。分析器之所以能做这么多事，与之简洁而强大的类设计是分不开的。

## 概念介绍

### Term(词)和Term Dictionary
Lucene中索引和搜索的最小单位，一个Field会由一个或多个Term组成，Term是由Field经过Analyzer（分词）产生。Term Dictionary即Term词典，是根据条件查找Term的基本索引。

### 标记 （token）
表示在字段文本中出现的词，由这个词的文本、开始和结束偏移量以及类型组成。

Lucene索引管理term的倒排数据结构。简单类比是：Lucene中field字段类似MySQL表中的列，而term类似数据库列的内容。

### 什么是token?
Lucene中的分析器是一个文本分析的功能。简单地说，它接收文本（比如文档）分解成单词，并返回token对象流。

token是一个对象，它有如下属性：

- Offset: token在文档的开始和结束字符偏移量
- Payload
- type: token的类型
- Position Increment: token的位置增量

### position increment位置增量
文档被tokenizer标记后，产生token对象流。每个token的位置信息都是相对于前一个token位置的增量值进行保存，表示所有token都是连续的。

位置增量将当前token与前一个token在位置上关联起来。通常来说，有如下几种情况：

- 位置增量值为1，表示每个token存在于field中唯一并且连续的位置上。
- 位置增量值大于1，表示token之间并不连续。该token前面的词基于规则不算作token，比如stop word。
- 位置增量为0，表示该token与前一个token放置在相同的位置。即当前的token与前一个token是重叠词。别名alias或者同义词分析器将一个token的位置增量设为0，用来实现插入token是一个同义词。

### 什么是term?
StandardAnalyzer从文档中提取出token对象后，与其field进行结合后，就形成term分词概念了。`Field + Token = Term`

位置信息，起点，终点偏移量以及负载payload是token对象的属性，这些属性随着token一起附带到field最终形成 term分词。


## TokenStream
TokenStream是语汇单元流，tokenStream是一个抽象类，它是所有分析器的基类。

Analyzer中最重要的就是tokenStream方法，它得到一个TokenStream对象。这个方法最开始从reuseStrategy中取得一个TokenStreamComponent。

reuseStrategy是重用的策略，默认有两个实现GLOBAL_REUSE_STRATEGY和PER_FIELD_REUSE_STRATEGY。

前者让所有field共用一个TokenStreamComponent，后者每个field一个TokenStreamComponent，而实际存储这些Component的地方是Analyzer中的storedValue。

接下来初始化reader并设置到component。最后从component得到TokenStream
```

  public final TokenStream tokenStream(final String fieldName,
                                       final Reader reader) throws IOException {
    TokenStreamComponents components = reuseStrategy.getReusableComponents(this, fieldName);
    final Reader r = initReader(fieldName, reader);
    if (components == null) {
      components = createComponents(fieldName, r);
      reuseStrategy.setReusableComponents(this, fieldName, components);
    } else {
      components.setReader(r);
    }
    return components.getTokenStream();
  }
```


## 流程分析
借助于 ES 分词DSL 看一下 analyzer的组成：
```
# 可以看出一个 analyzer 由 char_filter(任意个) + tokenizer(固定一个) + filter(任意个)
# char_filter ：对原文中的char 进行中转换
# tokenizer ： 将接收到的 text 切分出一系列的 token
# filter ：对tokenier 切分出来的结果的增加、删除、更改 操作。 
POST _analyze
{
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "٠ => 0",
        "١ => 1",
        "٢ => 2",
        "٣ => 3",
        "٤ => 4",
        "٥ => 5"
      ]
    },
    {
      "type": "pattern_replace",
      "pattern": """(\d+)-(?=\d)""",
      "replacement": "$1_"
    }
  ],
  "tokenizer": {
    "type": "whitespace"
  },
  "filter": [
    {
      "type": "lowercase"
    },
    {
      "type": "stop"
    }
  ],
  "text": "My license plate is ٢٥-٠١٥"
} 
# response
{
  "tokens" : [
    {
      "token" : "my",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "license",
      "start_offset" : 3,
      "end_offset" : 10,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "plate",
      "start_offset" : 11,
      "end_offset" : 16,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "25_015",
      "start_offset" : 20,
      "end_offset" : 26,
      "type" : "word",
      "position" : 4
    }
  ]
}
```

## CharFilter
从类图继承来看CharFilter 本身为java.io.Reader 子类，同时其中的 属性 input 是 java.io.Reader，可以看出为 CharFilter 就是 java.io.Reader 代理类。其中需要代理的方法就是java.io.Reader#read() 方法。 其具体子类在实现种按照自己逻辑，对原始的文本进行 char 变换。
```java
public abstract class CharFilter extends Reader {
  // 用于承接输入的 text 
  protected final Reader input;

  // 构造函数
  public CharFilter(Reader input) {
    super(input);
    this.input = input;
  }

  // 在子类中需要改变原始 text 的内容，但是为了避免 offset 引起变化，要做出修正
  protected abstract int correct(int currentOff);

  // 得到修正后的 offset， 并且是向上链式修正
  public final int correctOffset(int currentOff) {
    final int corrected = correct(currentOff);
    return (input instanceof CharFilter)
        ? ((CharFilter) input).correctOffset(corrected)
        : corrected;
  }
}
```

## Tokenizer
Tokenizer是分词器，负责将reader转换为语汇单元即进行分词，Lucene提供了很多的分词器，也可以使用第三方的分词，比如IKAnalyzer一个中文分词器。

Tokenizer基于规则将一个字符串分解成多个子串。 继承自 TokenStream,字段 input 用于接收 CharFilter或 Reader的输入。输出为切词之后的 token。

比如：LetterTokenizer，它是继承自CharTokenizer，通过next方法遍历输入字符串，根据特殊符号将字符串分解成一个个单词，然后封装成Token返回。只保留字符，并删除任何特殊字符或者数字。

```
Input => "to Address address 2 Problems"
Output => [to, Address, address, Problems]
```

org.apache.lucene.analysis.Tokenizer源码如下：
```
// 这个类里面额外设置了 其他字段，用于规范用户使用 setReader -> reset -> close 的使用流程。避免数据混入或 内存泄露
public abstract class Tokenizer extends TokenStream {
  /** The text source for this Tokenizer. */
  protected Reader input = ILLEGAL_STATE_READER;

  /** Pending reader: not actually assigned to input until reset() */
  private Reader inputPending = ILLEGAL_STATE_READER;

  /**
   * Expert: Set a new reader on the Tokenizer. Typically, an analyzer (in its tokenStream method)
   * will use this to re-use a previously created tokenizer.
   */
  public final void setReader(Reader input) {
    if (input == null) {
      throw new NullPointerException("input must not be null");
    } else if (this.input != ILLEGAL_STATE_READER) {
      throw new IllegalStateException("TokenStream contract violation: close() call missing");
    }
    this.inputPending = input;
    setReaderTestPoint();
  }

  @Override
  public void reset() throws IOException {
    super.reset();
    input = inputPending;
    inputPending = ILLEGAL_STATE_READER;
  }

  @Override
  public void close() throws IOException {
    input.close();
    // LUCENE-2387: don't hold onto Reader after close, so
    // GC can reclaim
    inputPending = input = ILLEGAL_STATE_READER;
  }

  private static final Reader ILLEGAL_STATE_READER =
      new Reader() {
        @Override
        public int read(char[] cbuf, int off, int len) {
          throw new IllegalStateException(
              "TokenStream contract violation: reset()/close() call missing, "
                  + "reset() called multiple times, or subclass does not call super.reset(). "
                  + "Please see Javadocs of TokenStream class for more information about the correct consuming workflow.");
        }

        @Override
        public void close() {}
      };
}
```

## TokenFilter
TokenFilter是分词过滤器，负责对语汇单元进行过滤，tokenFilter可以是一个过滤器链儿，Lucene提供了很多的分词器过滤器，比如大小写转换、去除停用词等。

Filter对Tokenizer返回的一系列token列表进行基于规则的操作。

比如，LowerCaseFilter将接受的token列表转换成小写。
```
Input => [to, Address, address, Problems]
Output => [to, address, address, problems]
```

比如，PorterStemFilter进行词干过滤
```
Input => [to, Addressing, address, Problems]
Output => [to, address, address, problem]
```

TokenFilter： 同样继承自 TokenStream, 但是数据源input对应的是 TokenStream(也就说上游为 Tokenizer 或 TokenFilter ) 。作用是对 Tokenizer或TokenFilter处理后的 切词结果 token 做各种处理。

```
public abstract class TokenFilter extends TokenStream {
  /** The source of tokens for this filter. */
  protected final TokenStream input;

  /** Construct a token stream filtering the given input. */
  protected TokenFilter(TokenStream input) {
    super(input);
    this.input = input;
  }

  @Override
  public void end() throws IOException {
    input.end();
  }

  @Override
  public void close() throws IOException {
    input.close();
  }

  @Override
  public void reset() throws IOException {
    input.reset();
  }
}
```

Tokenizer 主动向 Attribute 中添加值
```
public final class StandardTokenizer extends Tokenizer {

  // this tokenizer generates three attributes:
  // term offset, positionIncrement and type
  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
  private final PositionIncrementAttribute posIncrAtt =
      addAttribute(PositionIncrementAttribute.class);
  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);

  // Tokenizer 在调用 incrementToken() 时，添加值
  @Override
  public final boolean incrementToken() throws IOException {
    clearAttributes();
    skippedPositions = 0;

    while (true) {
      int tokenType = scanner.getNextToken();

      if (tokenType == StandardTokenizerImpl.YYEOF) {
        return false;
      }

      if (scanner.yylength() <= maxTokenLength) {
        posIncrAtt.setPositionIncrement(skippedPositions + 1);
        scanner.getText(termAtt);
        final int start = scanner.yychar();
        offsetAtt.setOffset(correctOffset(start), correctOffset(start + termAtt.length()));
        typeAtt.setType(StandardTokenizer.TOKEN_TYPES[tokenType]);
        return true;
      } else
        // When we skip a too-long term, we still increment the
        // position increment
        skippedPositions++;
    }
  }
    
}
```

tokenFilter 改变其中的数据
```
public class LowerCaseFilter extends TokenFilter {
  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);

  public LowerCaseFilter(TokenStream in) {
    super(in);
  }

  @Override
  public final boolean incrementToken() throws IOException {
    if (input.incrementToken()) {
      CharacterUtils.toLowerCase(termAtt.buffer(), 0, termAtt.length());
      return true;
    } else {
      return false;
    }
  }
}
```

### Tokenizer vs Filter
TokenFilter聚合了TokenStream基类，而TokenFilter与Tokenzier二个子类体系都是从TokenStream继承下来。换名话说，TokenFilter与Tokenzier的子类体系是一个组合关系。

TokenFilter的字段TokenStream类似next指针的作用，将Tokenzier的子类与TokenFilter子类组成了一个filter调用链。Lucene引擎将输入文本传递进filter过滤链，产生经过tokenizer产生token对象流，然后经过若干个tokenFilter，最后输出token流。

## Analyzer
分析器可以看成Tokenizert和Filter的组合，可以应用于任何全文检索系统中。如果将Analyzer分析器看成一个pipeline管道，

Tokenizer即是第一个分析阶段，有且只有一个，用来读取原始文件并转换成token对象列表。Filter是第二个分析阶段，

允许有多个filters组成一个过滤器列表，用来将输入的token对象列表进行分析，输出后仍然是token对象列表。

analyzer 作用是将 各种 CharFilter 、Tokenier 、TokenFilter 组合起来使用，构成一个复合体。
```
// StandardAnalyzer 中的 createComponents() 将各种组件组合在一起
public final class StandardAnalyzer extends StopwordAnalyzerBase {

  @Override
  protected TokenStreamComponents createComponents(final String fieldName) {
    final StandardTokenizer src = new StandardTokenizer();
    src.setMaxTokenLength(maxTokenLength);
    TokenStream tok = new LowerCaseFilter(src);
    tok = new StopFilter(tok, stopwords);
    return new TokenStreamComponents(
        r -> {
          src.setMaxTokenLength(StandardAnalyzer.this.maxTokenLength);
          src.setReader(r);
        },
        tok);
  }
}
```

Analyzer 的线程安全性
```
public abstract class Analyzer implements Closeable {

  private final ReuseStrategy reuseStrategy;
  // 线程安全的保障
  CloseableThreadLocal<Object> storedValue = new CloseableThreadLocal<>();
  // 默认使用全局策略，适用于只有一个字段类型
  public Analyzer() {
    this(GLOBAL_REUSE_STRATEGY);
  }

  public final TokenStream tokenStream(final String fieldName,
                                       final Reader reader) {
    TokenStreamComponents components = reuseStrategy.getReusableComponents(this, fieldName);
    final Reader r = initReader(fieldName, reader);
    if (components == null) {
      components = createComponents(fieldName);
      reuseStrategy.setReusableComponents(this, fieldName, components);
    }
    components.setReader(r);
    return components.getTokenStream();
  }
  

  @Override
  public void close() {
    if (storedValue != null) {
      storedValue.close();
      storedValue = null;
    }
  }


  public static abstract class ReuseStrategy {

    /** Sole constructor. (For invocation by subclass constructors, typically implicit.) */
    public ReuseStrategy() {}


    public abstract TokenStreamComponents getReusableComponents(Analyzer analyzer, String fieldName);

    public abstract void setReusableComponents(Analyzer analyzer, String fieldName, TokenStreamComponents components);

    protected final Object getStoredValue(Analyzer analyzer) {
      if (analyzer.storedValue == null) {
        throw new AlreadyClosedException("this Analyzer is closed");
      }
      return analyzer.storedValue.get();
    }

    protected final void setStoredValue(Analyzer analyzer, Object storedValue) {
      if (analyzer.storedValue == null) {
        throw new AlreadyClosedException("this Analyzer is closed");
      }
      analyzer.storedValue.set(storedValue);
    }

  }
  // 全局使用一个 组件，适用于一个字段类型
  public static final ReuseStrategy GLOBAL_REUSE_STRATEGY = new ReuseStrategy() {

    @Override
    public TokenStreamComponents getReusableComponents(Analyzer analyzer, String fieldName) {
      return (TokenStreamComponents) getStoredValue(analyzer);
    }

    @Override
    public void setReusableComponents(Analyzer analyzer, String fieldName, TokenStreamComponents components) {
      setStoredValue(analyzer, components);
    }
  };

  // 每个字段使用一个组件，适用于多个字段类型
  public static final ReuseStrategy PER_FIELD_REUSE_STRATEGY = new ReuseStrategy() {

    @SuppressWarnings("unchecked")
    @Override
    public TokenStreamComponents getReusableComponents(Analyzer analyzer, String fieldName) {
      Map<String, TokenStreamComponents> componentsPerField = (Map<String, TokenStreamComponents>) getStoredValue(analyzer);
      return componentsPerField != null ? componentsPerField.get(fieldName) : null;
    }

    @SuppressWarnings("unchecked")
    @Override
    public void setReusableComponents(Analyzer analyzer, String fieldName, TokenStreamComponents components) {
      Map<String, TokenStreamComponents> componentsPerField = (Map<String, TokenStreamComponents>) getStoredValue(analyzer);
      if (componentsPerField == null) {
        componentsPerField = new HashMap<>();
        setStoredValue(analyzer, componentsPerField);
      }
      componentsPerField.put(fieldName, components);
    }
  };

}
```

## 自定义Analyzer和实现自己的analysis模块

添加字长过滤器
有时候我们需要对字符串中的短字符进行过滤，比如welcome to BeiJIng中过滤掉长度小于2的字符串，我们期望的结果就变成了Welcome BeiJing,我们仅需要重新实现createComponents方法，相关代码如下:

```
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.Tokenizer;

import org.apache.lucene.analysis.core.WhitespaceTokenizer;
import org.apache.lucene.analysis.miscellaneous.LengthFilter;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;


public class LengFilterAanlyzer extends Analyzer {
    private int len;

    public int getLen() {
        return len;
    }


    public void setLen(int len) {
        this.len = len;
    }


    public LengFilterAanlyzer() {
        super();
    }


    public LengFilterAanlyzer(int len) {
        super();
        this.len = len;
    }


    @Override
    protected TokenStreamComponents createComponents(String fieldName) {
        final Tokenizer source = new WhitespaceTokenizer();
        TokenStream result = new LengthFilter(source, len, Integer.MAX_VALUE);
        return new TokenStreamComponents(source,result);

    }
    public static void main(String[] args) {
        Analyzer analyzer = new LengFilterAanlyzer(2);
        String words = "I am a java coder";
        TokenStream stream = null;

        try {
            stream = analyzer.tokenStream("myfield", words);
            stream.reset();
            CharTermAttribute  offsetAtt = stream.addAttribute(CharTermAttribute.class);
            while (stream.incrementToken()) {
                System.out.println(offsetAtt.toString());
            }
            stream.end();
        } catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }finally{
            try {
                stream.close();
            } catch (Exception e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            }
        }
    }
}
```

## Analyzer典型场景
从Lucene引擎中，无论是建立索引还是检索信息，输入部分都需要通过分析器Analyzer进行解析。它将输入文档或者输入的查询串解析成token对象流，进一步传递给索引模块构建倒排索引或者传递给检索模块进行数据查询。

QueryParser使用的analyzer分析器与创建索引时依赖的分析器必须相同吗？

推荐创建索引与查询QueryParser时使用相同的分析器，从而让索引的内容与检索的内容完全匹配。从这个角度来看，Analyzer分析器是Lucene搜索引擎的关键模块。

### 创建索引时使用Analyzer
输入关键字进行搜索，当需要让该关键字与文档域内容所包含的词进行匹配时需要对文档域内容进行分析，需要经过Analyzer分析器处理生成语汇单元（Token）。分析器分析的对象是文档中的Field域。当Field的属性tokenized（是否分词）为true时会对Field值进行分析。

对于一些Field可以不用分析：
- 不作为查询条件的内容，比如文件路径
- 不是匹配内容中的词而匹配Field的整体内容，比如订单号、身份证号等。

Lucene索引过程中，将输入文档中field的field-value进行分词后，写入索引文件。
```
RAMDirectory indexStore = new RAMDirectory();
IndexWriter writer = new IndexWriter(indexStore, new StandardAnalyzer(new String[]{}), true);
```
Lucene索引过程主要分成二部分操作阶段：将文档进行解析成token对象流，并将token进一步分词后保存到索引库中。

### 搜索时使用Analyzer
对搜索关键字进行分析和索引分析一样，使用Analyzer对搜索关键字进行分析、分词处理，使用分析后每个词语进行搜索。比如：搜索关键字：spring web ，经过分析器进行分词，得出：spring  web拿词去索引词典表查找 ，找到索引链接到Document，解析Document内容。

对于匹配整体Field域的查询可以在搜索时不分析，比如根据订单号、身份证号查询等。

用户提交一个检索字符串来表达查询需求，Lucene使用QueryParse进行解析生成布尔逻辑表达式，但首先需要将原始查询经过Analyzer分词器切分成token对象流，接着通过filter链表生成一个分词的序列。 每一个分词将作为布尔逻辑表达树的叶子节点，比如query={ term1 AND term2 or term3}。

在构建倒排索引时，它是一个以词term为中心数据结构。对于查询语句中出现的每个分词term1,term2和term3分别检索倒排索引库，将返回各自分词对应的文档列表 集合，最后依据分词之间的AND,OR逻辑运算操作符，对多个文档集取交集和并集后，即为查询的结果集。

### PerFieldAnalyzerWrapper支持不同域使用不同的解析器
PerFieldAnalyzerWrapper类的构造函数接受一个默认的分析器作为参数，同时提供了一个addAnalyzer方法用来接受不同的field使用不同的分析器。

PerFieldAnalyzerWrapper使用了装饰设计模式。通过向一个analyzer分析器（PerFieldAnalyzerWrapper.defaultAnalyzer)添加新的额外功能，同时又不改变其结构。那么，新的额外功能是什么？PerFieldAnalyzerWrapper.analyzerMap哈希表字段记录了不同field域使用的不同解析器对象。
```
public PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer) {
  this.defaultAnalyzer = defaultAnalyzer;
}

public void addAnalyzer(String fieldName, Analyzer analyzer{
  analyzerMap.put(fieldName, analyzer);
}
                        
public TokenStream tokenStream(String fieldName, Reader reader) {
    Analyzer analyzer = (Analyzer) analyzerMap.get(fieldName);
    return analyzer.tokenStream(fieldName, reader);
  }
```
当QueryParser解析输入查询串时，比如partnum:Q36，QueryParser#getFieldQuery方法的调用点从StandardAnalyzer变成PerFieldAnalyzerWrapper#tokenStream。PerFieldAnalyzerWrapper作为装饰对象，tokenStream方法内对"field:term"实施了额外的一层逻辑判断。

如果当前field在PerFieldAnalyzerWrapper.analyzerMap中提前注册了自己的解析器，自然会取出这个自定义的解析器来为当前field的term进行分词处理。否则退回到defaultAnalyzer进行分词处理。
```
public class QueryParser implements QueryParserConstants {
  protected Query getFieldQuery(String field, 
                                String queryText){
    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
  }
}
```
testPerFieldAnalyzer
KeywordAnalyzer对"Q36"解析后结果保持不变。使用SimpleAnalyzer对“Q36"解析输出为"q"。
```
public void testPerFieldAnalyzer() throws Exception {
    PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer());
    analyzer.addAnalyzer("partnum", new SimpleAnalyzer());
  
    QueryParser queryParser = new QueryParser("description", 
                                              analyzer);
    Query query = queryParser.parse("partnum:Q36 AND SPACE");
    assertEquals("Q36 kept as-is","+partnum:q +space", 
                 query.toString("description"));

    analyzer.addAnalyzer("partnum", new KeywordAnalyzer());
    queryParser = new QueryParser("description", analyzer);
    assertEquals("Q36 kept as-is", "+partnum:Q36 +space", 
                 query.toString("description"));
}
```
PerFieldAnalyzerWrapper#tokenStream
```
public class PerFieldAnalyzerWrapper extends Analyzer {  
  public TokenStream tokenStream(String fieldName, Reader reader) {
    Analyzer analyzer = (Analyzer) analyzerMap.get(fieldName);
    if (analyzer == null) {
      analyzer = defaultAnalyzer;
    }
    return analyzer.tokenStream(fieldName, reader);
  }
}
```

### Position Increment位置增量
自定义MultiAnalyzer实现同义词功能，基本思想是创建了一个自定义的TestFilter，并将它插入到Analyzer分析器的过滤器链表中。

对于查询字符串（比如"+(foo multi) field:"bar multi""），经过Analyzer分析器的TestFilter#next方法中进行语言规则的处理。TestFilter中定义了一个有趣的规则，用来实现同义词功能：它通过硬编码的方式，对于查询串中出现"multi"关键字，会动态实例化一个token对象并作为token对象流的一部分进行返回。
```
Token token = new Token("multi"+(multiToken+1), 
                              prevToken.startOffset(),
                              prevToken.endOffset(),
                              prevToken.type());
 token.setPositionIncrement(0);
```
需要说明的是：新创建的token对象的positionIncrement设置为0，然后QueryParse查询解析对象检测到某个token.positionIncrement属性为0后，会视情况启动MultiPhraseQuery查询。

```
assertEquals(
  "+(foo (multi multi2)) field:\"bar (multi multi2)\"",
  qp.parse("+(foo multi) field:\"bar multi\"").toString()
);

private class MultiAnalyzer extends Analyzer {
  public TokenStream tokenStream(String fieldName, Reader reader) {
    TokenStream result = new StandardTokenizer(reader);
    result = new TestFilter(result);
    result = new LowerCaseFilter(result);
    return result;
  }
}

private final class TestFilter extends TokenFilter {
  private Token prevToken;
  public TestFilter(TokenStream in) {
    super(in);
  }

  public final Token next() {
    if (multiToken > 0) {
      Token token = new Token("multi"+(multiToken+1), 
                              prevToken.startOffset(),
                              prevToken.endOffset(),
                              prevToken.type());
      token.setPositionIncrement(0);
      multiToken--;
      return token;
    } else {
      org.apache.lucene.analysis.Token t = input.next();
      prevToken = t;
      if (t == null)
        return null;
      String text = t.termText();
      if (text.equals("triplemulti")) {
        multiToken = 2;
        return t;
      } else if (text.equals("multi")) {
        multiToken = 1;
        return t;
      } else {
        return t;
      }
    }
  }
}
```

### QueryParser#getFieldQuery
解析"+(foo multi) field:"bar multi""输入查询串的时候，主要存在二个阶段：第一阶段使用analyzer分析器将查询串转换为token对象流。第二阶段通过while循环遍历所有的token对象，发现positionIncrement为0的token后，需要考虑该token与前一个token构成一组同义词进行查询。

比如，查询"foo multi"，自定义MultiAnalyzer分析器将生成三个token: foo-token, multi-token和multi-2-token，并且multi-2-token.positionIncrement被设置为0。经过QueryParser解析后，将生成一个MultiPhraseQuery对象。

```
public class QueryParser implements QueryParserConstants {
  protected Query getFieldQuery(String field, String queryText){
    TokenStream source = analyzer.tokenStream(
              field, new StringReader(queryText));
    Vector v = new Vector();
    org.apache.lucene.analysis.Token t;
    int positionCount = 0;
    boolean severalTokensAtSamePosition = false;

    while (true) {
      try {
        t = source.next();
      }
      v.addElement(t);
      if (t.getPositionIncrement() != 0)
        positionCount += t.getPositionIncrement();
      else
        severalTokensAtSamePosition = true;
    }

    if (v.size() == 0)
      return null;
    else if (v.size() == 1) {
      t = (Token) v.elementAt(0);
      return new TermQuery(new Term(field, t.termText()));
    } else {
       if (severalTokensAtSamePosition) {
          if (positionCount == 1) {
            // no phrase query:
            BooleanQuery q = new BooleanQuery(true);
            for (int i = 0; i < v.size(); i++) {
              t = (Token) v.elementAt(i);
              TermQuery currentQuery = new TermQuery(
                  new Term(field, t.termText()));
              q.add(currentQuery, BooleanClause.Occur.SHOULD);
            }
            return q;
          }
          else {
            // phrase query:
            MultiPhraseQuery mpq = new MultiPhraseQuery();
            mpq.setSlop(phraseSlop);
            List multiTerms = new ArrayList();
            int position = -1;
            for (int i = 0; i < v.size(); i++) {
              t = (Token) v.elementAt(i);
              if (t.getPositionIncrement() > 0 
                  && multiTerms.size() > 0) {
                if (enablePositionIncrements) {
                  mpq.add((Term[])multiTerms.toArray(
                    new Term[0]),position);
                } else {
                  mpq.add((Term[])multiTerms.toArray(
                    new Term[0]));
                }
                multiTerms.clear();
              }
              position += t.getPositionIncrement();
              multiTerms.add(new Term(field, t.termText()));
            }
            if (enablePositionIncrements) {
              mpq.add((Term[])multiTerms.toArray(
                new Term[0]),position);
            } else {
              mpq.add((Term[])multiTerms.toArray(
                new Term[0]));
            }
            return mpq;
          }
        }
    }
  }
```
Case1 -booleanQuery with should (OR) logic
比如，查询"multi"，自定义MultiAnalyzer分析器将生成二个token：multi-token和multi-2-token，并且multi-2-token.positionIncrement被设置为0。经过QueryParser解析后，会生成一个booleanQuery对象。它通过“或”逻辑操作符组织二个叶子节点termQuery对象，分别代表multi和multi-2的查询项。
```
if (severalTokensAtSamePosition) {
  if (positionCount == 1) {
    // no phrase query:
    BooleanQuery q = new BooleanQuery(true);
    for (int i = 0; i < v.size(); i++) {
      t = (org.apache.lucene.analysis.Token) v.elementAt(i);
      TermQuery currentQuery = new TermQuery(
          new Term(field, t.termText()));
      q.add(currentQuery, BooleanClause.Occur.SHOULD);
    }
    return q;
  }
```

Case2 - MultiPhraseQuery
PhraseQuery实现短语搜索，比如"address problems"由二个term组成的查询。有二个重点：

phrase查询需要依赖term的position信息，index索引阶段没有保存term的position信息，就无法支持phrase短语查询。
slop用来指定二个term之间允许的edit distance。
MultiPhraseQuery如何理解？

MultiPhraseQuery支持多短语查询，可能通过多个查询短语的拼接来实现复杂的查询。

先举例说明：提供term组成的数组["address","resolve"]，["problem","issue"]。然后将这二个数组传递给MultiPhraseQuery构造函数，Lucence搜索引擎会组合成4种形式"address problem"，"address issue"，"resolve problem"和"resolve issue"对索引进行全文检索。每一种组成其实就是PhraseQuery短语搜索类型。

上面例子说明PhraseQuery是MultiPhraseQuery的一种特殊形式。如果构造MultiPhraseQuery对象时传递的每个数组中只有一个term元素，就退化成PhraseQuery短语查询。

在MultiPhraseQuery中，一个数组内的多个term元素被定义成逻辑OR的关系，也就是说这些term是共享同一个position，用来实现同义词搜索。

比如：使用StandardAnalyzer分词器建立索引，希望找出同时包含“spicy food”和“spicy diet”二个查询短语的文档。 其中一个解决方案是：指定一个前缀“spicy”，一个后缀数组Term[] (new Term(“food”), new Term(“diet”))，则查询的结果即满足要求。
```
public void testMPQ2() throws Exception {
  MultiPhraseQuery q = new MultiPhraseQuery();
  q.add(ta(new String[] {"w1"}));
  q.add(ta(new String[] {"w2","w3"}));
  qtest(q, new int[] { 0,1,3 });
}
```

Case3 - Stop Filter
StopFilter添加自定义停止词，管理着一个stopWords名单。StopFilter#next方法用来对token对象流进行遍历，判断每个token是否出现在stopWords名单中，存在直接过滤掉当前token，并累计过滤token的数目。如果token没有出现在stopWords名单中，需要对token的位置增量进行修改，默认positionIncrement为1。此时需要考虑该token前面有几个被过滤的token，即将累计过滤掉token的数目赋值给该token.positionIncrement。
```
public final class StopFilter extends TokenFilter {
  private final CharArraySet stopWords;
  public static final Set makeStopSet(String[] stopWords, 
                                      boolean ignoreCase) {
    CharArraySet stopSet = 
      new CharArraySet(stopWords.length, ignoreCase);
    stopSet.addAll(Arrays.asList(stopWords));
    return stopSet;  
  }
}

  public final Token next(Token result) throws IOException {
    int skippedPositions = 0;
    while((result = input.next(result)) != null) {
      if (!stopWords.contains(result.termBuffer(),
                              0, result.termLength)) {
        if (enablePositionIncrements) {
         result.setPositionIncrement(
           result.getPositionIncrement()+ skippedPositions);
        }
        return result;
      }
      skippedPositions += result.getPositionIncrement();
    }
    return null;
  }
}
```