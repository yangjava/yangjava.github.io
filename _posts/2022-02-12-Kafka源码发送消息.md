---
layout: post
categories: [Kafka]
description: none
keywords: Kafka
---
# Kafka源码发送消息

## 生产者发送消息流程
Kafka的源码最核心的是由client模块和core模块构成，在开始剖析源码之前，先用一幅图大致介绍一下生产者发送消息的流程。
- 将消息封装成ProducerRecord对象
- Serializer对消息的key和value做序列化
- 根据Partitioner将消息分发到不同的分区，需要先获取集群的元数据
- RecordAccumulator封装很多分区的消息队列，每个队列代表一个分区，每个分区里面有很多的批次，每个批次里面由多条消息组成
- Sender会从RecordAccumulator拉取消息，封装成批次，发送请求
- 通过网络将请求发送到kafka集群

如果你不知道从何入手，那就打开Kafka源码example目录下的Producer这个类，层层递进。你会发现Producer继承了Thread，这就有点意思了，所以run（）方法是我们一定要看的。
```
public class Producer extends Thread {    
//定义了KafkaProducer对象    
private final KafkaProducer<Integer, String> producer;    
//消息的主题    
private final String topic;    
//发送消息的方式：同步发送或者异步发送    
private final Boolean isAsync;    
......

```
通过Producer的代码结构可以看到，Producer主要包括了构造函数和内部类DemoCallBack。

```

/**
 * 初始化生产者对象
 * @param topic
 * @param isAsync
 */
public Producer(String topic, Boolean isAsync) {
    Properties props = new Properties();
    // 指定kafka服务端的主机名和端口号，从kafka集群获取元数据信息
    props.put("bootstrap.servers", "localhost:9092");
    //客户端ID
    props.put("client.id", "DemoProducer");
    //IntegerSerializer将消息key序列化成字节数组
    props.put("key.serializer", "org.apache.kafka.common.serialization.IntegerSerializer");
    //StringSerializer将String对象序列化成字节数组
    props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
    //初始化kafkaProducer，生产者的核心类
    producer = new KafkaProducer<>(props);
    this.topic = topic;
    this.isAsync = isAsync;
}
```

初始KafkaProducer
```

/**
 * A producer is instantiated by providing a set of key-value pairs as configuration. Valid configuration strings
 * are documented <a href="http://kafka.apache.org/documentation.html#producerconfigs">here</a>.
 * @param properties   The producer configs
 */
 //通过producer的配置文件进行KafkaProducer初始化
public KafkaProducer(Properties properties) {
    this(new ProducerConfig(properties), null, null);
}
```

KafkaProducer核心功能详解

Producer初始化了KafkaProducer这个核心类，消息就是调用了这个类的send（xxx）方法进行发送的。感兴趣的小伙伴可以去看一下KafkaProducer这个类的注释，

因为内容有点长，这里就简单做一下介绍，主要说明了KafkaProducer是发送消息的客户端，并且是线程安全的，支持高并发，kafkaProducer还包含一个用于缓冲待提交消息的缓冲空间，发送消息是异步的，支持ACK消息确认机制，以及包含producer的配置，详情如下，截取了重点部分，可以参考一下。
```

/**
 * A Kafka client that publishes records to the Kafka cluster.
 * <P>
 * The producer is <i>thread safe</i> and sharing a single producer instance across threads will generally be faster than
 * having multiple instances.
 * <p>
 * The producer consists of a pool of buffer space that holds records that haven't yet been transmitted to the server
 * as well as a background I/O thread that is responsible for turning these records into requests and transmitting them
 * to the cluster. Failure to close the producer after use will leak these resources.
 * <p>
 * The {@link #send(ProducerRecord) send()} method is asynchronous. When called it adds the record to a buffer of pending record sends
 * and immediately returns. This allows the producer to batch together individual records for efficiency.
 * <p>
 * The <code>acks</code> config controls the criteria under which requests are considered complete. The "all" setting
 * we have specified will result in blocking on the full commit of the record, the slowest but most durable setting.
 */
private static final Logger log = LoggerFactory.getLogger(KafkaProducer.class);
//用于生产clientID
private static final AtomicInteger PRODUCER_CLIENT_ID_SEQUENCE = new AtomicInteger(1);
private static final String JMX_PREFIX = "kafka.producer";
private String clientId;
//分区器
private final Partitioner partitioner;
//消息的最大长度
private final int maxRequestSize;
//缓冲区大小
private final long totalMemorySize;
//管理的元数据的对象
private final Metadata metadata;
//用于收集并缓存消息，等待sender线程调用
private final RecordAccumulator accumulator;
//消息发送任务，是一个线程，实现了Runnable接口
private final Sender sender;
private final Metrics metrics;
//执行sender任务，发送消息的线程
private final Thread ioThread;
//压缩算法，有none、gzip、snappy和lz4，针对RecordAccumulator的消息使用
private final CompressionType compressionType;
private final Sensor errors;
private final Time time;
//key的序列化器
private final Serializer<K> keySerializer;
//value的序列化器
private final Serializer<V> valueSerializer;
//配置对象
private final ProducerConfig producerConfig;
//等待更新kafka集群元数据的最大时长
private final long maxBlockTimeMs;
//消息超时时间
private final int requestTimeoutMs;
//消息拦截器，发送消息之前可以先初步过滤
private final ProducerInterceptors<K, V> interceptors;
```
了解了KafkaProducer主要的功能概述之后，继续回到KafkaProducer的初始化流程，找到KafkaProducer（xxx）构造方法。
```

@SuppressWarnings({"unchecked", "deprecation"})
private KafkaProducer(ProducerConfig config, Serializer<K> keySerializer, Serializer<V> valueSerializer) {
    try {
        log.trace("Starting the Kafka producer");
        // 配置用户自定义的参数
        Map<String, Object> userProvidedConfigs = config.originals();
        this.producerConfig = config;
        this.time = new SystemTime();
        //生成clientId
        clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);
        if (clientId.length() <= 0)
            clientId = "producer-" + PRODUCER_CLIENT_ID_SEQUENCE.getAndIncrement();
        Map<String, String> metricTags = new LinkedHashMap<String, String>();
        metricTags.put("client-id", clientId);
        MetricConfig metricConfig = new MetricConfig().samples(config.getInt(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG))              .timeWindow(config.getLong(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS).tags(metricTags);
        List<MetricsReporter> reporters = config.getConfiguredInstances(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,
                MetricsReporter.class);
        reporters.add(new JmxReporter(JMX_PREFIX));
        this.metrics = new Metrics(metricConfig, reporters, time);
 
 
        //设置分区器，可以引用自定义的分区器
        this.partitioner = config.getConfiguredInstance(ProducerConfig.PARTITIONER_CLASS_CONFIG, Partitioner.class);
       
        //重试时间 
        //RETRY_BACKOFF_MS_CONFIG=retry.backoff.ms 默认100ms
        long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);
        //设置key序列化器
        if (keySerializer == null) {
            this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                    Serializer.class);
            this.keySerializer.configure(config.originals(), true);
        } else {
            config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);
            this.keySerializer = keySerializer;
        }
        //设置value序列化器
        if (valueSerializer == null) {
            this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                    Serializer.class);
            this.valueSerializer.configure(config.originals(), false);
        } else {
            config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);
            this.valueSerializer = valueSerializer;
        }
 
 
        // load interceptors and make sure they get clientId      
        userProvidedConfigs.put(ProducerConfig.CLIENT_ID_CONFIG, clientId);
        //设置拦截器，一般用不到
        List<ProducerInterceptor<K, V>> interceptorList = (List) (new ProducerConfig(userProvidedConfigs)).getConfiguredInstances(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,
                ProducerInterceptor.class);
        this.interceptors = interceptorList.isEmpty() ? null : new ProducerInterceptors<>(interceptorList);
        ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keySerializer, valueSerializer, interceptorList, reporters);
        //METADATA_MAX_AGE_CONFIG=metadata.max.age.ms,默认5分钟
        //生产者每隔一段时间都要去更新一下集群的元数据。
        this.metadata = new Metadata(retryBackoffMs, config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG), true, clusterResourceListeners);
        //MAX_REQUEST_SIZE_CONFIG=max.request.size 默认是1M
        //代表单个消息请求的最大值，可以根据公司的实际情况设置
        this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);
        //指的是用于存放待发送消息的缓存区大小，默认是32M        
        this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);
        //kafka可以通过设置压缩格式提高吞吐量，相应的会消耗更多的cpu资源
        this.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));
        /* check for user defined settings.
         * If the BLOCK_ON_BUFFER_FULL is set to true,we do not honor METADATA_FETCH_TIMEOUT_CONFIG.
         * This should be removed with release 0.9 when the deprecated configs are removed.
         */
        if (userProvidedConfigs.containsKey(ProducerConfig.BLOCK_ON_BUFFER_FULL_CONFIG)) {
            log.warn(ProducerConfig.BLOCK_ON_BUFFER_FULL_CONFIG + " config is deprecated and will be removed soon. " +
                    "Please use " + ProducerConfig.MAX_BLOCK_MS_CONFIG);
            boolean blockOnBufferFull = config.getBoolean(ProducerConfig.BLOCK_ON_BUFFER_FULL_CONFIG);
            if (blockOnBufferFull) {
                this.maxBlockTimeMs = Long.MAX_VALUE;
            } else if (userProvidedConfigs.containsKey(ProducerConfig.METADATA_FETCH_TIMEOUT_CONFIG)) {
                log.warn(ProducerConfig.METADATA_FETCH_TIMEOUT_CONFIG + " config is deprecated and will be removed soon. " +
                        "Please use " + ProducerConfig.MAX_BLOCK_MS_CONFIG);
                this.maxBlockTimeMs = config.getLong(ProducerConfig.METADATA_FETCH_TIMEOUT_CONFIG);
            } else {
                this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);
            }
        } else if (userProvidedConfigs.containsKey(ProducerConfig.METADATA_FETCH_TIMEOUT_CONFIG)) {
            log.warn(ProducerConfig.METADATA_FETCH_TIMEOUT_CONFIG + " config is deprecated and will be removed soon. " +
                    "Please use " + ProducerConfig.MAX_BLOCK_MS_CONFIG);
            this.maxBlockTimeMs = config.getLong(ProducerConfig.METADATA_FETCH_TIMEOUT_CONFIG);
        } else {
            this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);
        }
 
 
        /* check for user defined settings.
         * If the TIME_OUT config is set use that for request timeout.
         * This should be removed with release 0.9
         */
        if (userProvidedConfigs.containsKey(ProducerConfig.TIMEOUT_CONFIG)) {
            log.warn(ProducerConfig.TIMEOUT_CONFIG + " config is deprecated and will be removed soon. Please use " +
                    ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);
            this.requestTimeoutMs = config.getInt(ProducerConfig.TIMEOUT_CONFIG);
        } else {
            this.requestTimeoutMs = config.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);
        }
        //创建了RecordAccumulator这个重要的对象
        this.accumulator = new RecordAccumulator(config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),
                this.totalMemorySize,
                this.compressionType,
                config.getLong(ProducerConfig.LINGER_MS_CONFIG),
                retryBackoffMs,
                metrics,
                time);
 
 
        List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG));
        //去更新元数据
        this.metadata.update(Cluster.bootstrap(addresses), time.milliseconds());
        ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config.values());
        //初始化了网络组件，用于接收ClientRequest请求，通过Selector将请求发送给kafka集群
        //CONNECTIONS_MAX_IDLE_MS_CONFIG=connections.max.idle.ms:默认值是9分钟,表明网络连接处于空闲状态的最长时间，超过设置的时间就关闭网络
             //MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=max.in.flight.requests.per.connection,默认值是5，限制客户端在单个连接上能够发送的未响应请求的个数
        //RECONNECT_BACKOFF_MS_CONFIG=reconnect.backoff.ms,尝试重新连接到给定主机之前等待的时间
        //SEND_BUFFER_CONFIG=send.buffer.bytes,表示socket发送数据的缓冲区的大小，默认值是128K
        //RECEIVE_BUFFER_CONFIG=receive.buffer.bytess，表示socket接受数据的缓冲区的大小，默认值是32K。
        NetworkClient client = new NetworkClient(
                new Selector(config.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), this.metrics, time, "producer", channelBuilder),
                this.metadata,
                clientId,                config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION),            config.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),
                config.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
                config.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
                this.requestTimeoutMs, time);
 
        //这个就是一个线程
        this.sender = new Sender(client,
                this.metadata,
                this.accumulator,                config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION) == 1, config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),               (short)parseAcks(config.getString(ProducerConfig.ACKS_CONFIG)),
                config.getInt(ProducerConfig.RETRIES_CONFIG),
                this.metrics,
                new SystemTime(),
                clientId,
                this.requestTimeoutMs);
        String ioThreadName = "kafka-producer-network-thread" + (clientId.length() > 0 ? " | " + clientId : "");
 
 
        //创建KafkaThread线程，将sender对象传入，点进去会发现KafkaThread只是接收了线程的名字，并把线程设置为后台线程，没有其他逻辑操作，这样做的好处是可以实现线程控制和业务逻辑解耦。
        this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
        //启动线程。
        this.ioThread.start();
 
 
        this.errors = this.metrics.sensor("errors");
        config.logUnused();
        AppInfoParser.registerAppInfo(JMX_PREFIX, clientId);
        log.debug("Kafka producer started");
    } catch (Throwable t) {
        // call close methods if internal objects are already constructed
        // this is to prevent resource leak. see KAFKA-2121
        close(0, TimeUnit.MILLISECONDS, true);
        // now propagate the exception
        throw new KafkaException("Failed to construct kafka producer", t);
    }
}
```

到这里就完成了KafkaProducer的初始化流程分析。接下来就要看一下发送消息的流程，我们重点看一下run（）方法。
```
public void run() {
    //消息的key  
    int messageNo = 1;
    
    while (true) {
        //消息的value
        String messageStr = "Message_" + messageNo;
        long startTime = System.currentTimeMillis();    
        //isAsync: kafka发送数据的方式，true的时候是异步发送，false就是同步发送
        if (isAsync) { // Send asynchronously
            //ProducerRecord封装了消息的topic、消息的kye和value，并且调用CallBack回调对象，里面包含回调方法，当生产者收到kafka发送过来的ACK确认信号时，就会调用CallBack对象里面的onCompletion（）方法
            //异步发送，一直发送消息，消息响应结果交给回调函数处理，性能好，生产者绝大部分场景都是使用这种机制
            producer.send(new ProducerRecord<>(topic,
                messageNo,
                messageStr), new DemoCallBack(startTime, messageNo, messageStr));
        } else { // Send synchronously
            try {
                //同步发送，发送一条消息，需要等到这条消息确认已经发送出去了，才能继续发送下一条，性能很差，一般不使用
                producer.send(new ProducerRecord<>(topic,
                    messageNo,
                    messageStr)).get();
                System.out.println("Sent message: (" + messageNo + ", " + messageStr + ")");
            } catch (InterruptedException | ExecutionException e) {
                e.printStackTrace();
            }
        }
        //对消息的key进行递增
        ++messageNo;
    }
}
```
我们接着先看CallBack这个回调对象，然后再回头深究producer的send（xxx）方法。

```
class DemoCallBack implements Callback {
 
 
    private final long startTime;
    private final int key;
    private final String message;
 
 
    public DemoCallBack(long startTime, int key, String message) {
        this.startTime = startTime;
        this.key = key;
        this.message = message;
    }
 
 
    /**
     * A callback method the user can implement to provide asynchronous handling of request completion. This method will
     * be called when the record sent to the server has been acknowledged. Exactly one of the arguments will be
     * non-null.
     *
     * @param metadata  The metadata for the record that was sent (i.e. the partition and offset). Null if an error
     *                  occurred.
     * @param exception The exception thrown during processing of this record. Null if no error occurred.
     */
    public void onCompletion(RecordMetadata metadata, Exception exception) {
        long elapsedTime = System.currentTimeMillis() - startTime;
      //RecordMetadata 包含了分区信息、offset信息等等
        if (metadata != null) {
            System.out.println(
                "message(" + key + ", " + message + ") sent to partition(" + metadata.partition() +
                    "), " +
                    "offset(" + metadata.offset() + ") in " + elapsedTime + " ms");
        } else {
            exception.printStackTrace();
        }
    }
```
把关注点先放到send（xxx）这个方法上面来。

```
@Override
public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {
    // intercept the record, which can be potentially modified; this method does not throw exceptions
    ProducerRecord<K, V> interceptedRecord = this.interceptors == null ? record : this.interceptors.onSend(record);
    //一把抓住重点
    return doSend(interceptedRecord, callback);
}
```

```

/**
 * 将消息异步发送给topic
 * Implementation of asynchronously send a record to a topic.
 */
private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {
    TopicPartition tp = null;
    try {
        
        ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);
        //clusterAndWaitTime.waitedOnMetadataMs 代表的是拉取元数据耗费的时间。
        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);
        //更新集群的元数据
        Cluster cluster = clusterAndWaitTime.cluster;      
         //对消息的key进行序列化。         
        byte[] serializedKey;
        try {
            serializedKey = keySerializer.serialize(record.topic(), record.key());
        } catch (ClassCastException cce) {
            throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
                    " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
                    " specified in key.serializer");
        }
        //对消息的value进行序列化。
        byte[] serializedValue;
        try {
            serializedValue = valueSerializer.serialize(record.topic(), record.value());
        } catch (ClassCastException cce) {
            throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
                    " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
                    " specified in value.serializer");
        }
        
         //根据分区器和集群元数据信息将消息分发到对应的分区。
        int partition = partition(record, serializedKey, serializedValue, cluster);
 
 
        int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);      
         //根据分区器选择消息应该发送的分区,默认是1M
        ensureValidRecordSize(serializedSize);
        //根据元数据信息，封装分区对象
        tp = new TopicPartition(record.topic(), partition);
        long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();
        log.trace("Sending record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);
        // producer callback will make sure to call both 'callback' and interceptor callback
       //消息绑定回调函数
        Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback<>(callback, this.interceptors, tp);       
         //把消息放入accumulator,缓冲区大小默认是32M，
        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);
        //如果批次满了或者新创建出来一个批次
        if (result.batchIsFull || result.newBatchCreated) {
            log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
             //唤醒sender线程，sender才是真正发送数据的线程。
            this.sender.wakeup();
        }
        return result.future;
        // handling exceptions and record the errors;
        // for API exceptions return them in the future,
        // for other exceptions throw directly
    } catch (ApiException e) {
        log.debug("Exception occurred during message send:", e);
        if (callback != null)
            callback.onCompletion(null, e);
        this.errors.record();
        if (this.interceptors != null)
            this.interceptors.onSendError(record, tp, e);
        return new FutureFailure(e);
    } catch (InterruptedException e) {
        this.errors.record();
        if (this.interceptors != null)
            this.interceptors.onSendError(record, tp, e);
        throw new InterruptException(e);
    } catch (BufferExhaustedException e) {
        this.errors.record();
        this.metrics.sensor("buffer-exhausted-records").record();
        if (this.interceptors != null)
            this.interceptors.onSendError(record, tp, e);
        throw e;
    } catch (KafkaException e) {
        this.errors.record();
        if (this.interceptors != null)
            this.interceptors.onSendError(record, tp, e);
        throw e;
    } catch (Exception e) {
        // we notify interceptor about all exceptions, since onSend is called before anything else in this method
        if (this.interceptors != null)
            this.interceptors.onSendError(record, tp, e);
        throw e;
    }
```
sender线程被唤醒之后就开始往kafka集群发送消息，以上就是对生产者发送消息流程的整体分析，本篇文章分析的粒度比较粗，主要包括了消息的封装、序列化、分区，批次封装、sender线程通过NetworkClient发送请求到Kafka集群这几个重要流程，还介绍了一些常见配置参数，具体的细节没有进行展开。

相信看到这里有很多小伙伴都会有疑问，难道生产者发送消息这么重要的过程就这么简单地介绍完啦，答案肯定不是的，因为生产者发送消息的源码以及实现过程其实还是比较复杂的，为了方便读者阅读，并且更好地理解生产者发送消息的整个过程，我选择将生产者发送消息的整体流程和相关的细节剖析分为两篇写。细节剖析篇涉及的内容比较多，主要包括了集群元数据的管理，元数据的结构，元数据的获取和更新，分区器解析，RecordAccumulator消息核心类封装解析，batch的封装和发送，发送消息的机制，以及如何实现读写安全和高性能，内存池的设计和使用，还有网络通信组件NetworkClient应用等等这些重要的内容，实际展开的时候可能会引出更多的内容。

## 元数据
首先kafka发送消息时，KafkaProducer要将此消息追加到指定Topic的某个分区的Leader副本中，首先需要知道Topic的分区数量，经过路由后确定目标分区，之后KafkaProducer需要知道目标分区的Leader副本所在服务器的地址、端口等信息，才能建立连接，将消息发送到Kafka中。因此，在KafkaProducer中维护了Kafka集群的元数据，这些元数据记录了：某个Topic中有哪几个分区，每个分区的Leader副本分布在哪个节点上，Follower副本分布在哪些节点上，哪些副本在ISR集合中以及这些节点的网络地址、端口。

关于的Kafka的元数据的信息，我们可从以下几个核心类入手：Node、TopicPartition、PartitionInfo、MetaData、Cluster。

### Node
代表集群的节点，包含域名、主机ip、端口、机架这些信息。
```

**
 * Information about a Kafka node
 * 代表的就是kafka的一个节点信息
 */
public class Node {
    private static final Node NO_NODE = new Node(-1, "", -1);
    //id 编号，这个编号是我们配置参数的时候指定的broker id。
    private final int id;
    private final String idString;
    //主机名
    private final String host;
    //端口号，默认是9092
    private final int port;
    //机架
    private final String rack;
    ....
 }
```

### TopicPartition
代表Kafka中某个Topic的分区映射，主要由主题（topic）名和分区编号组成。
```

package org.apache.kafka.common;
import java.io.Serializable;
/**
 * A topic name and partition number
 * topic名称和分区编号
 */
public final class TopicPartition implements Serializable {
    private int hash = 0;
    private final int partition;
    private final String topic;
    ....
    }
```

### PartitionInfo
主要阐述了集群主题和分区的映射信息，包括分区的leader和replica分布情况等，比如某个topic在集群中的分区和副本所在的位置。
```

package org.apache.kafka.common;
/**
 * Information about a topic-partition.
 */
public class PartitionInfo {
    //主题
    private final String topic;
    //分区编号
    private final int partition;
    //leader partition 在哪台服务器上面
    private final Node leader;
    //这个分区的所有的replica都在哪些节点上面
    private final Node[] replicas;
    //ISR列表。
    private final Node[] inSyncReplicas;
    public PartitionInfo(String topic, int partition, Node leader, Node[] replicas, Node[] inSyncReplicas) {
        this.topic = topic;
        this.partition = partition;
        this.leader = leader;
        this.replicas = replicas;
        this.inSyncReplicas = inSyncReplicas;
    }
    /**
     * The topic name
     */
    public String topic() {
        return topic;
    }
    /**
     * The partition id
     */
    public int partition() {
        return partition;
    }
    /**
     * The node id of the node currently acting as a leader for this partition or null if there is no leader
     */
    public Node leader() {
        return leader;
    }
    /**
     * The complete set of replicas for this partition regardless of whether they are alive or up-to-date
     */
    public Node[] replicas() {
        return replicas;
    }
    /**
     * The subset of the replicas that are in sync, that is caught-up to the leader and ready to take over as leader if
     * the leader should fail
     */
    public Node[] inSyncReplicas() {
        return inSyncReplicas;
    }
}
```

### Cluster
代表Kafka集群节点、主题和分区的组成信息。
```

/**
 * A representation of a subset of the nodes, topics, and partitions in the Kafka cluster.
 */
public final class Cluster {
    private final boolean isBootstrapConfigured;
    //kafka集群节点信息列表
    private final List<Node> nodes;
    //没有授权的topic
    private final Set<String> unauthorizedTopics;
    //内部主题，主要真的Kafka Stream应用程序执行时的内部主题
    //例如状态存储的更改日志主题。这些主题由应用程序创建，仅供该流应用程序使用
    private final Set<String> internalTopics;
 
 
    //用于记录TopicPartition和PartitionInfo的映射关系
    private final Map<TopicPartition, PartitionInfo> partitionsByTopicPartition;
    //记录topic和PartitionInfo的映射信息
    private final Map<String, List<PartitionInfo>> partitionsByTopic;
    //一个topic对应哪些可用partition，topic和PartitionInfo的映射关系
    private final Map<String, List<PartitionInfo>> availablePartitionsByTopic;
    //一台服务器上面有哪些partition，Node与PartitionInfo的映射关系
    private final Map<Integer, List<PartitionInfo>> partitionsByNode;
    //服务器编号和服务器对应的关系，方便按照brokerId索引
    private final Map<Integer, Node> nodesById;
    //kafka集群的id信息
    private final ClusterResource clusterResource;
 
 
    /**
     * 新的构造方法增加了internalTopics，旧的构造方法已经废弃掉了
     * Create a new cluster with the given id, nodes and partitions
     * @param nodes The nodes in the cluster
     * @param partitions Information about a subset of the topic-partitions this cluster hosts
     */
    public Cluster(String clusterId,
                   Collection<Node> nodes,
                   Collection<PartitionInfo> partitions,
                   Set<String> unauthorizedTopics,
                   Set<String> internalTopics) {
        this(clusterId, false, nodes, partitions, unauthorizedTopics, internalTopics);
    }
    .......
  } 
```

### MetaData
在Cluster类的基础上进一步做了封装，包含了集群信息最后的更新时间、版本号以及是否需要等待更新等信息。
```

/**
 * A class encapsulating some of the logic around metadata.
 * <p>
 * This class is shared by the client thread (for partitioning) and the background sender thread.
 *
 * Metadata is maintained for only a subset of topics, which can be added to over time. When we request metadata for a
 * topic we don't have any metadata for it will trigger a metadata update.
 * <p>
 * If topic expiry is enabled for the metadata, any topic that has not been used within the expiry interval
 * is removed from the metadata refresh set after an update. Consumers disable topic expiry since they explicitly
 * manage topics while producers rely on topic expiry to limit the refresh set.
 */
public final class Metadata {
    private static final Logger log = LoggerFactory.getLogger(Metadata.class);
    //过期时间5m
    public static final long TOPIC_EXPIRY_MS = 5 * 60 * 1000;
    //过期值
    private static final long TOPIC_EXPIRY_NEEDS_UPDATE = -1L;
    //两个更新元数据的请求的最小的时间间隔，默认值是100ms
    private final long refreshBackoffMs;
    //多久自动更新一次元数据，默认值是5分钟更新一次。
    private final long metadataExpireMs;
    //对于producer端来讲，元数据是有版本号
    //每次更新元数据，都会修改一下这个版本号。
    private int version;
    //上一次更新元数据的时间。
    private long lastRefreshMs;
    //上一次成功更新元数据的时间。
    //如果正常情况下，如果每次都是更新成功的，那么lastRefreshMs和lastSuccessfulRefreshMs 应该是相同的。
    private long lastSuccessfulRefreshMs;
    //Kafka集群本身的元数据。
    private Cluster cluster;
    //是否更新元数据的标识
    private boolean needUpdate;
    /* Topics with expiry time */
    //记录了当前已有的topics
    private final Map<String, Long> topics;
    //用于监听MetaData更新的监听器集合
    private final List<Listener> listeners;
    //集群资源监听器
    private final ClusterResourceListeners clusterResourceListeners;
    //是否需要更新全部topic的元数据信息，
    private boolean needMetadataForAllTopics;
    private final boolean topicExpiryEnabled;
    /**
     * Create a metadata instance with reasonable defaults
     */
    public Metadata() {
        this(100L, 60 * 60 * 1000L);
    }
    public Metadata(long refreshBackoffMs, long metadataExpireMs) {
        this(refreshBackoffMs, metadataExpireMs, false, new ClusterResourceListeners());
    }
    /**
     * Create a new Metadata instance
     * @param refreshBackoffMs The minimum amount of time that must expire between metadata refreshes to avoid busy
     *        polling
     * @param metadataExpireMs The maximum amount of time that metadata can be retained without refresh
     * @param topicExpiryEnabled If true, enable expiry of unused topics
     * @param clusterResourceListeners List of ClusterResourceListeners which will receive metadata updates.
     */
    public Metadata(long refreshBackoffMs, long metadataExpireMs, boolean topicExpiryEnabled, ClusterResourceListeners clusterResourceListeners) {
        this.refreshBackoffMs = refreshBackoffMs;
        this.metadataExpireMs = metadataExpireMs;
        this.topicExpiryEnabled = topicExpiryEnabled;
        this.lastRefreshMs = 0L;
        this.lastSuccessfulRefreshMs = 0L;
        this.version = 0;
        this.cluster = Cluster.empty();
        this.needUpdate = false;
        this.topics = new HashMap<>();
        this.listeners = new ArrayList<>();
        this.clusterResourceListeners = clusterResourceListeners;
        this.needMetadataForAllTopics = false;
    }
    /**
     * Get the current cluster info without blocking
     */
    public synchronized Cluster fetch() {
        return this.cluster;
    }
    ........
}
```
kafka在发送消息的过程中很重要的一点就是消息的封装、消息的请求和分发。

大致的流程如下：
- ProducerInterceptor对消息进行拦截
- Serializer对消息的key和value进行序列化
- Partitioner对消息进行分发到对应的Partition
- RecordAccumulator负责收集消息并封装成批次，通过批量发送消息
- Sender为实际发送的线程，负责从RecordAccumulator获取消息
- 构建ClientRequest请求
- 将ClientRequest请求发送给NetworkClient组件
- NetworkClient组件将ClientRequest请求放入KafkaChannel缓存区
- 执行网络I/O，发送请求
- 收到响应，调用ClientRequest的回调函数onComplete

### 消息封装（RecordAccumulator对象）
首先我们先认识一下RecordAccumulator是何方神圣。通过类的注释可以知道RecordAccumulator本质就是用于收集消息的队列，底层实现的是MemoryRecords。
```

/**
 * This class acts as a queue that accumulates records into {@link org.apache.kafka.common.record.MemoryRecords}
 * instances to be sent to the server.
 * <p>
 * The accumulator uses a bounded amount of memory and append calls will block when that memory is exhausted, unless
 * this behavior is explicitly disabled.
 */
public final class RecordAccumulator {
    private static final Logger log = LoggerFactory.getLogger(RecordAccumulator.class);
    private volatile boolean closed;
    //注意这里使用了原子类，所以说封装消息的过程中会涉及到线程安全
    private final AtomicInteger flushesInProgress;
    private final AtomicInteger appendsInProgress;
    //消息批次大小
    private final int batchSize;
    //消息压缩类型，支持GZip、snappy、lz4
    private final CompressionType compression;
    private final long lingerMs;
    private final long retryBackoffMs;
    
    private final BufferPool free;
    private final Time time;
    //一个消息批次包括TopicPartition 分区 和  Deque<RecordBatch> 队列
    private final ConcurrentMap<TopicPartition, Deque<RecordBatch>> batches;
    private final IncompleteRecordBatches incomplete;
    // The following variables are only accessed by the sender thread, so we don't need to protect them.
    private final Set<TopicPartition> muted;
    private int drainIndex;
    .......
}
```
Kafka有两种发送消息的方式，分别是同步发送和异步发送，基本使用到的都是异步发送，主线程通过调用KafkaProducer.send()方法将消息缓存在RecordAccumulator，到达一定条件之后就会唤醒Sender线程发送消息。RecordAccumulator包含了这样的一种数据结构ConcurrentMap<TopicPartition, Deque<RecordBatch>> batches；以TopicPartition作为key，Deque作为value，Deque中存放了RecordBatch，正如我们上图所示。每一个RecordBatch就是一个MemoryRecords对象，可以了解一下MemoryRecords的数据结构。

### MemoryRecords
```
public class MemoryRecords implements Records {
    private final static int WRITE_LIMIT_FOR_READABLE_ONLY = -1;
    // 压缩消息数据，并写入到buffer
    private final Compressor compressor;
    // 表示buffer中最多可以写入的字节数据大小
    private final int writeLimit;
    // buffer初始化大小
    private final int initialCapacity;
    // 用于保存消息数据的缓冲区
    private ByteBuffer buffer;
    // 用于表示MemoryRecords的读写模型，默认是只读
    private boolean writable;
    .......
    }
```
MemoryRecords有几个重要的方法，有兴趣的可以下去了解一下。
其中
- buffer：用于保存消息数据的Java NIO ByteBuffer。
- writeLimit：记录buffer字段最多可以写入多少个字节的数据。
- compressor：压缩器，对消息数据进行压缩，将压缩后的数据输出到buffer。
- writable：此MemoryRecords对象是只读的模式，还是可写模式。在MemoryRecords发送前时，会将其设置成只读模式。

MemoryRecords中的Compressor的压缩类型是由“compression.type”配置参数指定的，即KafkaProducer.compressionType字段的值。目前KafkaProducer支持GZIP、SNAPPY、LZ4三种压缩方式，对应源码的位置如下。

```

// the following two functions also need to be public since they are used in MemoryRecords.iteration
public static DataOutputStream wrapForOutput(ByteBufferOutputStream buffer, CompressionType type, int bufferSize) {
    try {
        switch (type) {
            case NONE:
                return new DataOutputStream(buffer);
            case GZIP:
                return new DataOutputStream(new GZIPOutputStream(buffer, bufferSize));
            case SNAPPY:
                try {
                    OutputStream stream = (OutputStream) snappyOutputStreamSupplier.get().newInstance(buffer, bufferSize);
                    return new DataOutputStream(stream);
                } catch (Exception e) {
                    throw new KafkaException(e);
                }
            case LZ4:
                try {
                    OutputStream stream = (OutputStream) lz4OutputStreamSupplier.get().newInstance(buffer);
                    return new DataOutputStream(stream);
                } catch (Exception e) {
                    throw new KafkaException(e);
                }
            default:
                throw new IllegalArgumentException("Unknown compression type: " + type);
        }
    } catch (IOException e) {
        throw new KafkaException(e);
    }
}
public static DataInputStream wrapForInput(ByteBufferInputStream buffer, CompressionType type, byte messageVersion) {
    try {
        switch (type) {
            case NONE:
                return new DataInputStream(buffer);
            case GZIP:
                return new DataInputStream(new GZIPInputStream(buffer));
            case SNAPPY:
                try {
                    InputStream stream = (InputStream) snappyInputStreamSupplier.get().newInstance(buffer);
                    return new DataInputStream(stream);
                } catch (Exception e) {
                    throw new KafkaException(e);
                }
            case LZ4:
                try {
                    InputStream stream = (InputStream) lz4InputStreamSupplier.get().newInstance(buffer,
                            messageVersion == Record.MAGIC_VALUE_V0);
                    return new DataInputStream(stream);
                } catch (Exception e) {
                    throw new KafkaException(e);
                }
            default:
                throw new IllegalArgumentException("Unknown compression type: " + type);
        }
    } catch (IOException e) {
        throw new KafkaException(e);
    }
}
```

### RecordBatch
在简单了解完MemoryRecords类结构之后，上面说到每个RecordBatch其实就是一个MemoryRecord，所以接下来我们继续了解RecordBatch。
```

/**
 * A batch of records that is or will be sent.
 * 最终被发送出去的消息对象
 * 这个类不是线程安全的，在修改的时候需要使用同步锁
 */
public final class RecordBatch {
    private static final Logger log = LoggerFactory.getLogger(RecordBatch.class);
    //用于记录保存的record数量
    public int recordCount = 0;
    //最大Record的字节数
    public int maxRecordSize = 0;
    //尝试发送RecordBatch的次数
    public volatile int attempts = 0;
    //创建时间
    public final long createdMs;
    public long drainedMs;
    //最后一次尝试发送的时间
    public long lastAttemptMs;
    //真正存储数据的地方
    public final MemoryRecords records;
    //当前RecordBatch中缓存的消息都会发送给此TopicPartition
    public final TopicPartition topicPartition;
    //ProduceRequestResult类型，标识RecordBatch状态的Future对象
    public final ProduceRequestResult produceFuture;
    //最后一次向RecordBatch追加消息的时间
    public long lastAppendTime;
    //Thunk对象的集合
    private final List<Thunk> thunks;
    //用来记录消息在RecordBatch中的偏移量
    private long offsetCounter = 0L;
    //是否正在重试。如果RecordBatch中的数据发送失败，则会重新尝试发送
    private boolean retry;
    .......
    }
```
从上面的流程图可以看到，将消息封装成一个一个的RecordBatch之后，放到Dqueue队列中，一个RecordAccumulator由一个至多个的Dqueue组成，这样可以减少通信成本，批量发送消息，从而也能提高吞吐量。










