---
layout: post
categories: [Flume]
description: none
keywords: Flume
---
# Flume日志采集
在大数据技术架构中，主要包括数据采集、数据存储、数据计算、数据分析、数据可视化等核心步骤。其中数据采集至关重要，只有将数据源的数据采集过来，才可以进行计算和分析等工作。

## 什么是Flume
目前的数据采集主要分为结构化数据采集和非结构化数据采集，采集的方式也略有区别。主要介绍非结构化的数据收集工具Flume。

Flume是一种用于高效收集、聚合和移动大量日志数据的分布式的可靠可用服务。它具有基于流式数据的简单灵活架构，基于它的可靠性机制和许多故障切换及恢复机制来说，它具有健壮性和容错性，使用了一个允许在线分析应用程序的简单可扩展数据模型。

Apache Flume是一种分布式、可靠和可用的系统，用于高效收集、聚合，以及将大量日志数据从许多不同的来源移动到集中式数据存储上。

使用Apache Flume不仅限于日志数据的聚合。由于数据源是可定制的，因此可以使用Flume来传输大量的事件数据，包括但不限于网络流量数据、社交媒体生成的数据、电子邮件消息和其他数据源。

## Flume的特点
- 事务
Flume使用两个独立的事务负责从Source到Channel及从Channel到Sink的事件传递。在Source到Channel的过程中，一旦所有事件全部传递到Channel并且提交成功，那么Source就将该文件标记为完成；从Channel到Sink的过程同样以事务的方式传递。由于某种原因使得事件无法记录时，事务将回滚，所有事件仍保留在Channel中重新等待传递。

- 可靠性
Channel中的File Channel具有持久性，事件写入File Channel后，即使Agent重新启动，事件也不会丢失。Flume中还提供了一种Memory Channel的方式，但它不具有持久存储的能力，数据完整性不能得到保证；与File Channel相比，Memory Channel的优点是具有较高的吞吐量。

- 多层代理
使用分层结构的Flume代理，实现了Flume事件的汇聚，也就是第一层代理采集原始Source的事件，并将它们发送到第二层，第二层代理数量比第一层少，汇总了第一层事件后再把这些事件写入HDFS。

将一组节点的事件汇聚到一个文件中，这样可以减少文件数量，增加文件大小，减轻施加在HDFS上的压力。另外，因为向文件输入数据的节点变多，所以文件可以更快地推陈出新，从而使得这些文件可用于分析的时间更接近于事件的创建时间。

## Flume架构
Flume由一组以分布式拓扑结构相互连接的代理构成。Flume中有多个Agent，即Flume代理。Agent是由Source、Sink和Channel共同构成的Java进程，Flume的Source产生事件后将其传给Channel，Channel存储这些事件直至转发给Sink。
```
+--------+        +--------+        +--------- +        +-------+        +--------+
|   WEB  |  --->  | Source |   ---> | Channel |  --->   | Sink  |  --->  |  HDFS  |
+--------+        +--------+        +--------- +        +-------+        +--------+
```

## Flume的主要组件
Flume的主要组件有Event、Client、Agent、Source、Channel和Sink等。接下来我们一起认识一下各组件的作用。

### Event、Client与Agent——数据传输
事件Event是Flume数据传输的基本单元，Flume以事件的形式将数据从源头传送到最终目的。

假设我们需要进行日志传输，Client把原始需要收集的日志信息包装成Events并且发送到一个或多个Agent上。这样做的主要目的是从数据源系统中将Flume解耦。

代理Agent是Flume流的基础部分，一个Agent包含Source、Channel、Sink和其他组件，它基于这些组件把Event从一个节点传输到另一个节点或最终目的地上，由Flume为这些组件提供配置、生命周期管理和监控支持。

### Source—Event接收
Source的主要职责是接收Event，并将Event批量地放到一个或者多个Channel中。接下来我们分别介绍一下常用的两类Source：Spooling Directory Source和Exec Source。

- Spooling Directory Source获取数据
Spooling Directory Source是通过读取硬盘上需要被收集数据的文件到spooling目录来获取数据，然后再将数据发送到Channel。该Source会监控指定的目录来发现新文件并解析新文件。在给定的文件已被读完之后，它被重命名为指示完成（或可选地删除）

与Exec源不同，即使Flume重新启动或死机，这种Source也是可靠的，不会丢失数据。同时需要注意的是，产生的文件不能进行任意修改，否则会停止处理；在实际应用中可以将文件写到一个临时目录下之后再统一移动到监听目录下。

- Exec Source收集数据
Exec源在启动时运行给定的UNIX命令，并期望该进程在标准输出上连续生成数据。如果进程由于任何原因退出，则源也将退出并且不会继续产生数据。这意味着诸如`cat[named pipe]`或`tail-F[file]`的配置将产生期望的结果。

## Channel—Event传输
Channel位于Source和Sink之间，用于缓存Event，当Sink成功将Event发送到下一个Agent或最终目的处之后，会将Event从Channel上移除。不同的Channel提供的持久化水平也是不一样的，并且Channel可以和任何数量的Source和Sink工作。

- Memory Channel内存中存储
Memory Channel是指Events被存储在已配置最大容量的内存队列中，因此它不具有持久存储能力

在使用Memory Channel时，如果出现问题导致虚拟机宕机或操作系统重新启动，事件就会丢失，在这种情况下，数据完整性不能保证，这种情况是否可以接受，主要取决于具体应用。与File Channel相比，Memory Channel的优势在于具有较高的吞吐量，在要求高吞吐量并且允许Agent Event失败所导致数据丢失的情况下，Memory Channel是理想的选择。

- File Channel持久化存储

File Channel具有持久性，只要事件被写入Channel，即使代理重新启动，事件也不会丢失，能保障数据的完整性

### Sink—Event发送
Sink的主要职责是将Event传输到下一个Agent或最终目的处，成功传输完成后将Event从Channel中移除。Sink主要分为两大类：File Roll Sink和Hdfs Sink。

- File Roll Sink写入本地

File Roll Sink是指将事件写入本地文件系统中，首先我们要在本地文件系统中创建一个缓冲目录，新增文件是由手工添加的。

- HDFS Sink写入HDFS

HDFS Sink是指将事件写入Hadoop分布式文件系统（HDFS）。它可以根据经过的时间、数据大小或事件数量定期滚动文件，也就是关闭当前文件并创建新文件。对于正在进行写操作处理的文件，其文件名会添加一个后缀“.tmp”，以表明文件处理尚未完成

### 其他组件
Interceptor组件主要作用于Source，可以按照特定的顺序对Events进行装饰或过滤。Sink Group允许用户将多个Sink组合在一起，Sink Processor则能够通过组中的Sink切换来实现负载均衡，也可以在一个Sink出现故障时切换转到另一个Sink。

## Flume安装
- 下载Flume。
从Flume下载页面（http://flume.apache.org/download.html ）下载一个稳定版本的Flume二进制发行包，比如apache-flume-1.6.0-bin.tar.gz版本。

- 上传包并安装。
将其放入适当的位置，这里的路径为/opt/software，然后将其解压，命令如下：
```
tar -zxvf apache-flume-1.6.0-bin.tar.gz
```
- 把Flume配置到环境变量中
这样可以在任意目录下启动Flume，配置文件如下：
```
JAVA_HOME=/usr/java/jdk1.7.0_79
HADOOP_HOME=/opt/software/hadoop-2.5.1
HIVE_HOME=/opt/software/apache-hive-1.2.1-bin
FLUME_HOME=/opt/software/apache-flume-1.6.0-bin
ZOOKEEPER_HOME=/opt/software/zookeeper-3.4.6
PATH=$PATH:$HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:
$HIVE_HOME/bin:$ZOOKEEPER_HOME/bin:$FLUME_HOME/bin
export PATH
```
- 使环境变量生效：
```
source .bash_profile
```

## Flume应用典型实例
在配置使用Flume时，最核心的是编写配置文件，在配置文件中配置好Source、Sink和Channel属性，在启动Flume时会读取这个配置文件，并根据配置完成数据的采集工作。

- 本地数据读取（conf1）
首先，我们看一个从本地目录下收集数据的实例。

首先在目录（如/opt）下分别创建flume和sink文件夹，flume文件夹作为数据源，sink文件夹作为输出目录，输入Flume代理的启动命令，此时我们在/opt目录下上传并编辑conf1文件，之后上传到flume的文件会被收集到sink文件夹中。具体步骤如下。

编辑配置文件。在/opt文件夹下编辑conf1配置文件，内容如下：
```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置 Source
a1.sources.r1.type =spooldir
a1.sources.r1.spoolDir=/opt/flume

#配置Sink
a1.sinks.k1.type =file_roll
a1.sinks.k1.sink.directory=/opt/sink

# 设置Channel类型为Memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 把 Source 和 Sink 绑到 Channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

- 创建相应的文件夹
在/opt文件夹下分别建立flume和sink文件夹，分别作为Source和Sink的目录。
```
mkdir flume
mkdir sink 
```

- 启动Flume代理
使用flume-ng命令启动Flume代理，同时在console控制台打印日志，命令如下：
```
flume-ng agent --conf-file ./opt/conf1 --name a1-Dflume.root.logger=INFO,console
```

- 测试数据收集
向/opt/flume文件夹下导入一个test.txt文本文件，Flume客户端会有如下显示：
```
INFO instrumentation.MonitoredCounterGroup:Componenttype:SOURCE,name:r1 started 
INFO avro.ReliableSpoolingFileEventReader:
Last read took us just up to a file boundary.Rolling to the next file,if there is one.
INFO avro.ReliableSpoolingFileEventReader:Preparingtomov
efile/opt/fluno/test.txt to/opt/flume/test.txt. 
COMPLETED
```
一旦Flume成功启动并完成日志收集，再查看flume文件夹中的test.txt文件时，则变成了text.txt.COMPLETED，sink文件夹中会收集到文件。这个实例中演示了Flume的工作流程，实现test.txt的收集工作。

## 收集至HDFS
我们看一个从本地目录下收集数据到HDFS的实例。首先在目录（如/opt）下创建一个flume文件夹，flume作为数据源，输入Flume代理的启动命令，在/opt目录下创建并编辑conf2文件，最终实现上传的flume的文件会被收集到HDFS中。conf2文件内容如下：
```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置 Source
a1.sources.r1.type =spooldir
a1.sources.r1.spoolDir=/opt/flume/

# 配置 Sink
a1.sinks.k1.type =hdfs
#配置收集后的文件，放在HDFS的哪个位置
a1.sinks.k1.hdfs.path=hdfs://master:9000/flume/data/
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240000
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.idleTimeout=3
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.round=true
a1.sinks.k1.hdfs.roundValue=10
a1.sinks.k1.hdfs.roundUnit=minute
a1.sinks.k1.hdfs.useLocalTimeStamp=true
#a1.sinks.k1.type =hdfs

# 配置channel为Memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 绑定Source 和 Sink 到Channel上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```
完成配置之后，就可以启动Flume代理了，启动命令如下：
```
flume-ng agent --conf-file conf2 --name a1 -Dflume.root.logger=INFO,console
```
接着，我们上传文件到/opt/flume目录下。接着打开NameNode节点，可以看到在HDFS上生成了多个文件，这些文件就是Flume收集过来的

## 基于日期分区的数据收集
有些时候，由于收集的文件很大，或者因业务需求，需要将收集到的数据按照日期分开存储。本节实例就是基于HDFS收集，再根据年、月、日、时分分别生成文件，具体步骤如下。

- 在目录（如/opt）下创建一个flume文件夹，flume作为数据源。
- 在/opt目录下创建并编辑conf 3文件，并在conf 3文件中设置HDFS的存储路径，并设置以年月日的格式作为分区条件。
- 将在本地flume文件夹下生成的文本日志收集到HDFS里，并设置以日期%Y-%m-%d/%H%M格式进行分类储存。

conf3的主要内容如下：
```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置 Source
a1.sources.r1.type =spooldir
a1.sources.r1.spoolDir=/opt/flume/

# 配置 Sink
a1.sinks.k1.type =hdfs
#按照%Y-%m-%d/%H%M格式分开存储文件
a1.sinks.k1.hdfs.path=hdfs://master:9000/flume/data/%Y-%m-%d/%H%M
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240000
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.idleTimeout=3
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.round=true
a1.sinks.k1.hdfs.roundValue=10
a1.sinks.k1.hdfs.roundUnit=minute
a1.sinks.k1.hdfs.useLocalTimeStamp=true
#a1.sinks.k1.type =hdfs

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 绑定 Source 和 Sink 到 Channel 上
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```
随后，可以查看上传的文件是否已经被收集到HDFS。

## 通过exec命令实现数据收集
还有一种收集文件的方法是使用exec命令。比如，如果想要读取文件内容持续增加的文件，可以使用tail-f命令来实现。本实例演示Flume如何结合tail–f命令完成数据收集，通过tail–f命令查看新增内容，通过Flume持续将新增内容收集到HDFS中。这个实例中，我们使用了一个py脚本来持续生成模拟数据。

配置文件conf4如下：
```
logAgent.sources = logSource
logAgent.channels = fileChannel
logAgent.sinks = hdfsSink
#指定Source的类型是exec
logAgent.sources.logSource.type = exec
#指定命令是tial -F，持续监测/opt/data/loganalysis/record.list中的数据
logAgent.sources.logSource.command = tail -F /opt/data/loganalysis/
record.list

# 将Channel设置为fileChannel
logAgent.sources.logSource.channels = fileChannel

# 设置Sink为HDFS
logAgent.sinks.hdfsSink.type = hdfs
#文件生成的时间
logAgent.sinks.hdfsSink.hdfs.path = hdfs://master:9000/flume/record/%Y-
%m-%d/%H%M
logAgent.sinks.hdfsSink.hdfs.filePrefix= transaction_log
logAgent.sinks.hdfsSink.hdfs.rollInterval= 600
logAgent.sinks.hdfsSink.hdfs.rollCount= 10000
logAgent.sinks.hdfsSink.hdfs.rollSize= 0
logAgent.sinks.hdfsSink.hdfs.round = true
logAgent.sinks.hdfsSink.hdfs.roundValue = 10
logAgent.sinks.hdfsSink.hdfs.roundUnit = minute
logAgent.sinks.hdfsSink.hdfs.fileType = DataStream
logAgent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true
#Specify the channel the sink should use
logAgent.sinks.hdfsSink.channel = fileChannel

# 设置 Channel的类型为file，并设置断点目录和channel数据存放目录
logAgent.channels.fileChannel.type = file
logAgent.channels.fileChannel.checkpointDir= /opt/software/dataCheckpointDir
logAgent.channels.fileChannel.dataDirs= /opt/software/dataDir
```
开始运行Flume，执行命令如下：
```
flume-ng agent --conf-file conf4 --name logAgent -Dflume.root.logger=INFO,console
```
查看生成的文件，这里查看record.list文件：



