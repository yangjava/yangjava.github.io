---
layout: post
categories: [Python,scikitlearn]
description: none
keywords: Python
---
# 线性回归算法
线性回归算法是使用线性方程对数据集进行拟合的算法，是一个非常常见的回归算法。本章首先从最简单的单变量线性回归算法开始介绍，然后介绍了多变量线性回归算法，其中成本函数以及梯度下降算法的推导过程会用到部分线性代数和偏导数；接着重点介绍了梯度下降算法的求解步骤以及性能优化方面的内容；

- 单变量线性回归算法的原理；
- 多变量线性回归算法的原理；
- 梯度下降算法的原理及步骤；

## 算法原理
我们先考虑最简单的单变量线性回归算法，即只有一个输入特征。

### 预测函数
针对数据集x和y，预测函数会根据输入特征x来计算输出值h（x）。其输入和输出的函数关系如下：
```shell

```
这个方程表达的是一条直线。我们的任务是构造一个hθ函数，来映射数据集中的输入特征x和输出值y，使得预测函数hθ计算出来的值与真实值y的整体误差最小。构造hθ函数的关键，就是找到合适θ0，θ1的值，θ0，θ1称为模型参数。
假设我们有如下数据集：
```shell

```
假设模型参数θ0=1，θ1=3，则模型函数为hθ（x）=1+3x。针对数据集中的第一个样本，输入为1，根据模型函数预测出来的值是4，与输出值y是吻合的。针对第二个样本，输入为2，根据模型函数预测出来的值是7，与实际输出值y相差1。模型的求解过程，就是找出一组最合适的模型参数θ0，θ1，以便能最好地拟合数据集。

怎样来判断最好地拟合了数据集呢？回顾之前学过的知识，不难猜出，当拟合成本最小时，即找到了最好的拟合参数。

### 成本函数
单变量线性回归算法的成本函数是：
```shell

```
其中，h（x（i））-y（i）是预测值和实际值的差，故成本就是预测值和实际值的差的平方的平均值，之所以乘以1/2是为了计算方便。这个函数也称为均方差方程。有了成本函数，就可以精确地测量模型对训练样本拟合的好坏程度。

### 梯度下降算法
有了预测函数，也可以精确地测量预测函数对训练样本的拟合情况。我们要怎样求解模型参数θ0，θ1的值呢？这时梯度下降算法就派上了用场。

我们的任务是找到合适的θ0，θ1，使得成本函数J（θ0，θ1）最小。为了便于理解，我们切换到三维空间来描述这个任务。在一个三维空间里，以θ0作为x轴，以θ1作为y轴，以成本函数J（θ0，θ1）为z轴，那么我们的任务，就是要找出当z轴上的值最小的时候所对应的x轴上的值和y轴上的值。

梯度下降算法的原理是，先随机选择一组θ0，θ1，同时选择一个参数α作为移动的步幅。然后，让x轴上的θ0和y轴上的θ1分别向特定的方向移动一小步，这个步幅的大小就由参数α指定。经过多次迭代之后，x轴和y轴上的值决定的点就慢慢地靠近z轴上的最小值处，如图5-1所示。

这是个等高线图，就是说在我们描述的三维空间里，你的视角在正上方，看到一圈一圈z轴值相同的点构成的线。在图5-1中，随机选择的点在X0处，经过多次迭代后，慢慢地靠近圆心处，即z轴上最小值附近。

问题来了，X0（由[θ0，θ1]描述）怎么知道往哪个方向移动，才能靠近z轴上最小值附近？答案是往成本函数逐渐变小的方向移动。怎么表达成本函数逐渐变小的方向呢？答案是偏导数。


可以简单地把偏导数理解为斜率。我们要让θj不停地迭代，由当前θj的值，根据J（θ）的偏导数函数，算出J（θ）在θj上的斜率，然后再乘以学习率α，就可以让θj往前J（θ）变小的方向迈一小步。

用数学来描述上述过程，梯度下降的公式为：


公式中，下标j就是参数的序号，针对单变量线性回归，即0和1。α称为学习率，它决定每次要移动的幅度大小，它会乘以成本函数对参数θj的偏导数，以这个结果作为参数移动的幅度。如果幅度太小，意味着要计算很多次才能到达目的地，如果幅度太大，可能会直接跨过目的地，从而无法收敛。

把成本函数J（θ）的定义代入上面的公式中，不难推导出梯度下降算法公式：


对公式推导过程感兴趣的读者，可以参阅本章的扩展阅读的内容。

公式中，α是学习率；m是训练样本的个数；h（x（i））-y（i）是模型预测值和真实值的误差。需要注意的是，针对θ0和θ1分别求出了其迭代公式，在θ1的迭代公式里，累加器中还需要乘以xi。


## 数据归一化
当线性回归模型有多个输入特征时，特别是使用多项式添加特征时，需要对数据进行归一化处理。比如，特征x1的范围在[1，4]之间，特征x2的范围在[1，2000]之间，这种情况下，可以让x1除以4来作为新特征x1，同时让x2除以2000来作为新特征x2，该过程称为特征缩放（feature scaling）。可以使用特征缩放来对训练样本进行归一化处理，处理后的特征值范围在[0，1]之间。

为什么要进行数据归一化处理？归一化处理有哪些注意事项？

归一化处理的目的是让算法收敛更快，提升模型拟合过程中的计算效率。进行归一化处理后，当有个新的样本需要计算预测值时，也需要先进行归一化处理，再通过模型来计算预测值，计算出来的预测值要再乘以归一化处理的系数，这样得到的数据才是真实的预测数据。

在scikit-learn里，使用LinearRegression进行线性回归时，可以指定normalize=True来对数据进行归一化处理。具体可查阅scikit-learn文档。

## 示例：使用线性回归算法拟合正弦函数
本节用线性回归算法来模拟正统函数。首先，生成200个在区间内的正弦函数上的点，并且给这些点加上一些随机的噪声。
```shell
n_dots = 200
 
X = np.linspace(-2 * np.pi, 2 * np.pi, n_dots)
Y = np.sin(X) + 0.2 * np.random.rand(n_dots) - 0.1
X = X.reshape(-1, 1)
Y = Y.reshape(-1, 1);

```
其中，reshape（）函数的作用是把Numpy的数组整形成符合scikit-learn输入格式的数组，否则scikit-learn会报错。接着，我们使用PolynomialFeatures和Pipeline创建一个多项式拟合模型：
```shell
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
 
def polynomial_model(degree=1):
    polynomial_features = PolynomialFeatures(degree=degree,
                                             include_bias=False)
    linear_regression = LinearRegression(normalize=True)
    pipeline = Pipeline([("polynomial_features", polynomial_features),
                         ("linear_regression", linear_regression)])
    return pipeline
```
分别用2、3、5、10阶多项式来拟合数据集：
```shell
from sklearn.metrics import mean_squared_error
 
degrees = [2, 3, 5, 10]
results = []
for d in degrees:
    model = polynomial_model(degree=d)
    model.fit(X, Y)
    train_score = model.score(X, Y)
    mse = mean_squared_error(Y, model.predict(X))
    results.append({"model": model, "degree": d, "score":
      train_score, "mse": mse})
for r in results:
    print("degree: {}; train score: {}; mean squared error: {}".format(
        r["degree"], r["score"], r["mse"]))
```
算出每个模型拟合的评分，此外，使用mean_squared_error算出均方根误差，即实际的点和模型预测的点之间的距离，均方根误差越小说明模型拟合效果越好——上述代码的输出结果为：
```shell
degree: 2; train score: 0.147285454656; mean squared error: 0.42388701419
degree: 3; train score: 0.271740750281; mean squared error: 0.36201990526
degree: 5; train score: 0.895448999212; mean squared error: 0.0519726229563
degree: 10; train score: 0.993239572763; mean squared error: 
0.00336062910102
```
从输出结果可以看出，多项式的阶数越高，拟合评分越高，均方差误差越小，拟合效果越好。最后，我们把不同模型的拟合效果在二维坐标上画出来，可以清楚地看到不同阶数的多项式的拟合效果：
```shell
from matplotlib.figure import SubplotParams
 
plt.figure(figsize=(12, 6), dpi=200, subplotpars=SubplotParams(hspace=0.3))
for i, r in enumerate(results):
    fig = plt.subplot(2, 2, i+1)
    plt.xlim(-8, 8)
    plt.title("LinearRegression degree={}".format(r["degree"]))
    plt.scatter(X, Y, s=5, c='b', alpha=0.5)
    plt.plot(X, r["model"].predict(X), 'r-')
```





