---
layout: post
categories: [Spark]
description: none
keywords: Spark
---
# Spark源码开发接口

## Spark API
Spark早期版本只提供了基于SparkContext的API。现在Spark又增加了SparkSession和DataFrame相关的API，熟练使用它们，可以使大数据的开发工作变得更加简单。

Spark的word count例子，也是为了能够把之前的内容串联起来。Spark的任务运行在存储体系、调度系统及计算引擎之上。Spark Application、Application的配置信息、Application依赖的Jar包、map任务的输出缓存、reduce任务的缓存等都基于存储体系；程序转换为RDD构成的DAG后，RDD的阶段划分、提交Task、资源申请等都离不开调度系统；Task在执行时对数据的缓存、聚合、Shuffle管理等都由计算引擎支撑。

## 基本概念
基本概念，包括数据类型（DataType）、结构类型（StructType）、结构字段（StructField）及元数据（Metadata）。

### DataType简介
DataType是Spark SQL的所有数据类型的基本类型，Spark SQL的所有数据类型都继承自DataType。Spark SQL中定义的数据类型与Java的基本数据类型大部分都是一致的。

### Metadata简介
Metadata用来保存StructField的元数据信息，其本质是底层的`Map[String，Any]`。Meta-data可以对Boolean、Long、Double、String、Metadata、Array[Boolean]、Array[Long]、Array[Double]、Array[String]和Array[Metadata]等类型的元数据进行存储或读取。Metadata属于Spark SQL中的内容。

### StructType与StructField
StructField中共定义了4个属性：字段名称（name）、数据类型（dataType）、是否允许为null（nullable）、元数据（metadata）。

StructField的定义如下。
```
case class StructField(
  name: String,
  dataType: DataType,
  nullable: Boolean = true,
  metadata: Metadata = Metadata.empty)
```
StructType的属性中最为重要的是类型为Array[StructField]的fields，由此可以看出一个StructType中可以包括零到多个StructField。为了便于理解，下面定义了一个简单的StructType。
```
val struct =
   StructType(
     StructField("a", IntegerType, true) ::
     StructField("b", LongType, false) ::
     StructField("c", BooleanType, false) :: Nil)
```
对于构建的struct，可以用以下语句获取名称为b的StructField。
```
val singleField = struct("b")
```
如果要获取struct中不存在的StructField，就像下面这样。
```
val nonExisting = struct("d")
```
那么nonExisting等于null。

如果想要获取struct中的多个StructField，需要像下面这样使用。
```
val twoFields = struct(Set("b", "c"))
```
此时的twoFields实际为StructType（List（StructField（b，LongType，false），StructField（c，BooleanType，false）））。

如果我们以以下代码定义一个新的数据结构struct。
```
val innerStruct =
  StructType(
     StructField("f1", IntegerType, true) ::
     StructField("f2", LongType, false) ::
     StructField("f3", BooleanType, false) :: Nil)

 val struct = StructType(
   StructField("a", innerStruct, true) :: Nil)

```
那么我们可以使用这个schema创建Row，就像下面这样。
```
val row = Row(Row(1, 2, true))
```

## 数据源DataSource
从Spark 1.3.0开始，Spark推出了DataFrame的API，与此同时DataSource也被引入到Spark中。Spark将文本文件、CSV文件、JSON文件等一系列格式的输入数据都作为数据源。特质DataSourceRegister是对所有数据源的抽象，DataSourceRegister的所有具体实现都被注册到了DataSource中。DataSource将负责对不同类型数据源的查找、创建不同类型数据源的信息、解析不同数据源的关系等。

### DataSourceRegister详解
DataSourceRegister是数据源的抽象，所有数据源都应该实现它。DataSourceRegister的定义非常简单，代码如下。
```
trait DataSourceRegister {
  def shortName(): String
}
```
shortName方法意在获取数据源提供者使用的格式或格式的别名。 Spark中实现了大量的数据源提供者

这里以TextFileFormat实现的shortName方法为例。
```
override def shortName（）：String="text"
```
可以看到TextFileFormat的别名为text。此外，其他DataSourceRegister的实现类也都有属于自己的别名

### DataSource详解
DataSource用于表示在Spark SQL中可插拔的数据源。DataSource中包括以下属性。
- sparkSession：即SparkSession。
- className：此名称用于决定要使用的类。
- paths：数据源的多个路径。
- userSpecifiedSchema：用户指定的StructType。
- partitionColumns：分区字段的序列。
- bucketSpec：类型为BucketSpec。BucketSpec是一种用于将数据集分解为更多可管理部分的技术。由于桶的数量是固定的，因此数据不会随数据而波动。
- options：类型为Map[String，String]，用于保存选项。
- catalogTable：类型为CatalogTable。CatalogTable是用于定义表的字典。
- providingClass：数据源的类。providingClass实际是以className为参数，调用Data-Source伴生对象的lookupDataSource方法（见代码清单10-1）获得的。
- sourceInfo：类型为SourceInfo。样例类SourceInfo用于表示数据源的名称和元数据信息，其定义如下。
```
case class SourceInfo(name: String, schema: StructType, partitionColumns: Seq[String])
```
sourceInfo实际是通过调用DataSource的sourceSchema方法（见代码清单10-2）得到的。

·caseInsensitiveOptions：忽略大小写的选项配置。caseInsensitiveOptions实际是通过CaseInsensitiveMap对options的包装，以忽略配置的大小写。

DataSource伴生对象的backwardCompatibilityMap属性缓存了类名与对应的DataSource-Register实现类之间的映射关系，代码如下。
```
private val backwardCompatibilityMap: Map[String, String] = {
  val jdbc = classOf[JdbcRelationProvider].getCanonicalName
  val json = classOf[JsonFileFormat].getCanonicalName
  val parquet = classOf[ParquetFileFormat].getCanonicalName
  val csv = classOf[CSVFileFormat].getCanonicalName
  val libsvm = "org.apache.spark.ml.source.libsvm.LibSVMFileFormat"
  val orc = "org.apache.spark.sql.hive.orc.OrcFileFormat"

  Map(
    "org.apache.spark.sql.jdbc" -> jdbc,
    "org.apache.spark.sql.jdbc.DefaultSource" -> jdbc,
    "org.apache.spark.sql.execution.datasources.jdbc.DefaultSource" -> jdbc,
    "org.apache.spark.sql.execution.datasources.jdbc" -> jdbc,
    "org.apache.spark.sql.json" -> json,
    "org.apache.spark.sql.json.DefaultSource" -> json,
    "org.apache.spark.sql.execution.datasources.json" -> json,
    "org.apache.spark.sql.execution.datasources.json.DefaultSource" -> json,
    "org.apache.spark.sql.parquet" -> parquet,
    "org.apache.spark.sql.parquet.DefaultSource" -> parquet,
    "org.apache.spark.sql.execution.datasources.parquet" -> parquet,
    "org.apache.spark.sql.execution.datasources.parquet.DefaultSource" -> parquet,
    "org.apache.spark.sql.hive.orc.DefaultSource" -> orc,
    "org.apache.spark.sql.hive.orc" -> orc,
    "org.apache.spark.ml.source.libsvm.DefaultSource" -> libsvm,
    "org.apache.spark.ml.source.libsvm" -> libsvm,
    "com.databricks.spark.csv" -> csv
  )
}

```
lookupDataSource

DataSource的伴生对象中提供了根据指定的名字找到对应的数据源类的方法lookup-DataSource。
```
def lookupDataSource(provider: String): Class[_] = {
  val provider1 = backwardCompatibilityMap.getOrElse(provider, provider)
  val provider2 = s"$provider1.DefaultSource"
  val loader = Utils.getContextOrSparkClassLoader
  val serviceLoader = ServiceLoader.load(classOf[DataSourceRegister], loader)

  try {
    serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList match {
      case Nil =>
        try {
          Try(loader.loadClass(provider1)).orElse(Try(loader.loadClass(provider2))) match {
            case Success(dataSource) =>
              dataSource
            case Failure(error) =>
              // 忽略抛出异常的代码
          }
        } catch {
          // 忽略捕获异常的处理代码
        }
      case head :: Nil =>
        head.getClass
      case sources =>
        sys.error(s"Multiple sources found for $provider1 " +
          s"($ {sources.map(_.getClass.getName).mkString(", ")}), " +
          "please specify the fully qualified class name.")
    }
  } catch {
      // 忽略捕获异常的处理代码
  }
}
```
lookupDataSource方法的执行步骤如下。
- 从backwardCompatibilityMap中查找指定名称对应的数据源类provider1，还在provider1后拼接DefaultSource构成provider2。
- 由于所有的数据源类都必须实现DataSourceRegister，且DataSourceRegister的shortName方法定义了获取数据源类的别名的规范，所以这里使用类加载器找到与provider1匹配的数据源类，如果未能匹配，则尝试加载由provider1或provider2指定的类。

DataSource中提供了很多方法，限于篇幅不能一一介绍。笔者挑选出DataSource中与本章内容相关的一些方法进行介绍。

sourceSchema

sourceSchema方法用于根据providingClass得到对应的SourceInfo
```
private def sourceSchema(): SourceInfo = {
  providingClass.newInstance() match {
    case s: StreamSourceProvider =>
      val (name, schema) = s.sourceSchema(
        sparkSession.sqlContext, userSpecifiedSchema, className, caseInsensitive-Options)
      SourceInfo(name, schema, Nil)

    case format: FileFormat =>
      val path = caseInsensitiveOptions.getOrElse("path", {
        throw new IllegalArgumentException("'path' is not specified")
      })

      val hdfsPath = new Path(path)
      if (!SparkHadoopUtil.get.isGlobPath(hdfsPath)) {
        val fs = hdfsPath.getFileSystem(sparkSession.sessionState.newHadoop-Conf())
        if (!fs.exists(hdfsPath)) {
          throw new AnalysisException(s"Path does not exist: $path")
        }
      }

      val isSchemaInferenceEnabled = sparkSession.sessionState.conf.streaming-SchemaInference
      val isTextSource = providingClass == classOf[text.TextFileFormat]
      // 省略抛出IllegalArgumentException的代码
      val (dataSchema, partitionSchema) = getOrInferFileFormatSchema(format)
      SourceInfo(
        s"FileSource[$path]",
        StructType(dataSchema ++ partitionSchema),
        partitionSchema.fieldNames)

    case _ =>
      throw new UnsupportedOperationException(
        s"Data source $className does not support streamed reading")
  }
}
```
sourceSchema方法通过反射得到providingClass对应的实例，然后根据实例的不同类型构造SourceInfo。

resolveRelation

resolveRelation方法用于根据数据源的提供类（providingClass）和userSpecifiedSchema创建读写数据源所需的BaseRelation。由于resolveRelation方法的内容很多，这里只展示resolveRelation方法的一部分内容
```
def resolveRelation(checkFilesExist: Boolean = true): BaseRelation = {
  val relation = (providingClass.newInstance(), userSpecifiedSchema) match {
    case (dataSource: SchemaRelationProvider, Some(schema)) =>
      dataSource.createRelation(sparkSession.sqlContext, caseInsensitiveOptions, schema)
    case (dataSource: RelationProvider, None) =>
      dataSource.createRelation(sparkSession.sqlContext, caseInsensitiveOptions)
    case (_: SchemaRelationProvider, None) =>
      // 忽略创建BaseRelation的代码
    case (format: FileFormat, _) =>
      // 忽略创建BaseRelation的代码
    case _ =>
      throw new AnalysisException(
        s"$className is not a valid Spark SQL Data Source.")
  }
  relation
}
```

## 检查点的实现
检查点是很多分布式系统为了容灾容错引入的机制，其实质是将系统运行期的内存数据结构和状态持久化到磁盘上，在需要时通过对这些持久化数据的读取，重新构造出之前的运行期状态。Spark使用检查点主要是为了将RDD的执行状态保留下来，在重新执行时就不用重新计算，而直接从检查点读取。CheckpointRDD是对检查点数据进行操作的RDD，例如，读写检查点数据。RDDCheckpointData表示RDD检查点的数据。Spark的检查点离不开CheckpointRDD和RDDCheckpointData的支持，本节将对它们的代码实现进行分析。

CheckpointRDD的实现

CheckpointRDD是特殊的RDD，用来从存储体系中恢复检查点数据。CheckpointRDD的定义如下。
```
rivate[spark] abstract class CheckpointRDD[T: ClassTag](sc: SparkContext)
extends RDD[T](sc, Nil) {

override def doCheckpoint(): Unit = { }
override def checkpoint(): Unit = { }
override def localCheckpoint(): this.type = this

protected override def getPartitions: Array[Partition] = ???
override def compute(p: Partition, tc: TaskContext): Iterator[T] = ???
}
```
可以看到CheckpointRDD重写了RDD的5个方法，分别如下。
- doCheckpoint：保存检查点。
- checkpoint：读取检查点数据。
- localCheckpoint：本地的检查点。
- getPartitions：获取检查点的分区数组。
- compute：名为计算，实际是从检查点恢复数据。

CheckpointRDD有LocalCheckpointRDD和ReliableCheckpointRDD两个子类，它们都没有完全实现CheckpointRDD中的方法。下面以ReliableCheckpointRDD为例，来介绍Check-pointRDD的具体实现。