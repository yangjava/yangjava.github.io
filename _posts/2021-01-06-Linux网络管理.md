---
layout: post
categories: Linux
description: none
keywords: Linux
---
# Linux网络管理
在性能分析中网络子系统是另一个重要的子系统。除了Linux，网络操作还与很多组件相互影响，比如交换机、路由器、网关、PC客户端，等等。虽然这些组件可能在Linux控制之外，但它们对整体性能有很多影响。请记住，在网络系统中，你要与网络工作人员紧密合作。

这里重点了解Linux如何处理网络操作。

1.5.1　网络化的实现
TCP/IP协议有一个类似于OSI分层模型的分层结构。Linux网络化的实现采用类似的方法。图1-23显示了分层的Linux TCP/IP协议栈，并给出了TCP/IP通信的大致图示。


图1-23　网络分层结构和网络操作概述

如同许多UNIX系统那样，Linux进行TCP/IP网络操作使用socket接口。socket为用户应用程序提供一个接口。下面我们看看在网络数据传输期间发生的基本操作：

①当一个应用程序发送数据到对等主机的时候，应用程序创建数据。

②应用程序打开socket，并通过socket接口写入数据。

③socket缓冲区被用来处理传输的数据。socket缓冲区引用数据，并向下穿过各层。

④在每一层中，执行适当的操作，比如，解析报头，添加和修改报头，校验和，路由操作，分片，等等。当socket缓冲区向下穿过各层时，在各层之间的数据自身是不能够复制的。因为在不同层之间复制实际数据是无效的，内核仅通过改变在socket缓冲区中的引用来避免不必要的开销并传递到下一层。

⑤网络接口卡向线缆发送数据，当传输时增加一个中断。

⑥以太网帧到达对等主机的网络接口卡。

⑦如果MAC地址匹配接口卡的MAC地址，将帧移动到网络接口卡的缓冲区。

⑧网络接口卡最终将数据包移动到一个socket缓冲区，并发出一个硬件中断给CPU。

⑨CPU之后处理数据包，并使其向上穿过各层，直到它到达一个应用程序的TCP端口，比如Apache。

1．socket buffer
正如之前指出的，内核使用缓冲区发送和接收数据。图1-24显示了可用于网络缓冲区的配置，它们可以通过/proc/sys/net中的文件进行调整。


图1-24　socket缓冲区的内存分配

/proc/sys/net/core/rmem_max
/proc/sys/net/core/rmem_default
/proc/sys/net/core/wmem_max
/proc/sys/net/core/wmem_default
/proc/sys/net/ipv4/tcp_mem
/proc/sys/net/ipv4/tcp_rmem
/proc/sys/net/ipv4/tcp_wmem
有时它可能对网络性能产生影响。我们将在5.7.4节中讨论细节。

2．Network API (NAPI)
网络子系统经历了一些改变，引入了新网络API（NAPI）。在Linux网络堆栈的标准实现中，可靠性和低延迟要比低开销和高吞吐量更重要。当创建一个防火墙时这些特征是有利的，大多数企业级应用程序，比如文件打印或者数据库，要比安装在Windows下执行得慢。

使用传统的方法处理网络数据包，网络接口卡最终将数据包移动到操作系统内核的一个网络缓冲区，并向CPU发出一个硬中断。


这个方法的缺点之一是每次一个匹配MAC地址的以太网帧到达接口，都会产生一个硬件中断。CPU每处理一个硬中断，就要停止当前的处理工作，来处理中断，从而导致上下文切换，并刷新相关的处理器缓存。如果仅有少量数据包到达接口，你可能认为这不是一个问题，但是千兆以太网和现代的应用程序每秒能创建数千个数据包，这将导致发生大量的中断和上下文切换。


正因为如此，引入了NAPI，来计算处理网络流量的相关开销。对于第一个数据包，NAPI的工作就像传统的实现一样，为第一个数据包发出一个中断。但是在第一个数据包之后，接口进入一种轮询（polling）模式。只要有数据包就放入网络接口的DMA环形缓冲区，这不会引起新的中断，从而有效地减少了上下文切换的次数和相关的开销。最后一个数据包处理完后，环形缓冲区被清空，接口卡将再次退回到中断模式。NAPI也具有提高多处理器扩展性的优点，它可创建软中断并由多个处理器来处理。对于大多数企业级多处理器系统，NAPI是一个巨大的改进，它需要NAPI-enabled驱动程序。

3．Netfilter
Linux有先进的防火墙功能，其是作为内核的一部分存在的。这种能力是由Netfilter模块提供的。可以使用iptables工具操作和配置Netfilter。

一般来说，Netfilter提供了以下功能：

□　数据包过滤（filter）。 如果一个数据包与一条规则匹配，则Netfilter接受或拒绝该数据包，或基于定义的规则采取适当行动。

□　地址转换（nat）。 如果一个数据包与一条规则匹配，Netfilter将更改数据包，以满足地址转换的需求。


□　改变数据包（mangle）。 如果一个数据包与一条规则匹配，Netfilter将按照规则对数据包进行改变（ttl、tos、mark）。

可以通过以下属性来定义匹配过滤器：

□　网络接口

□　IP地址、IP地址范围、子网

□　协议

□　ICMP类型

□　端口

□　TCP标志

□　状态（参考下文“连接跟踪”）

图1-25给出了数据包是如何穿过Netfilter链的图示，以及在序列中每个点应用的规则列表。


图1-25　防火墙的工作图

如果数据包与规则匹配，Netfilter将采取相应的行动。这个行动被称为目标（target）行动。可能的目标行动有：

□　ACCEPT。 接受数据包，并让它通过。

□　DROP。 默默地丢弃该数据包。

□　REJECT。 通过发送回一个错误数据包来响应匹配的数据包，比如，icmp-net-unreachable、icmp-host-unreachable、icmp-port-unreachable及tcp-reset等。

□　LOG。 开启内核日志记录匹配到的数据包。

□　MASQUERADE、SNAT、DNAT、REDIRECT。 地址转换。

4．连接跟踪
为了实现较复杂的防火墙功能，Netfilter使用连接跟踪机制以对所有网络流量的状态进行跟踪。它使用TCP连接状态（参考1.5.2节内容）和其他网络属性（比如IP地址、端口号、协议、序列号、确认号、ICMP类型，等等），Netfilter根据下面4种状态对每个数据包进行分类：

□　NEW。 该数据包开启一个新的连接。

□　ESTABLISHED。 该数据包关联一个已经建立的连接。

□　RELATED。 该数据包要开启一个新的连接，但是它与一个已经存在的连接相关。比如FTP数据传输和ICMP错误。

□　INVALID。 该数据包与已知连接不相关。不能确定是由于一些什么原因，它不对应任何已知的连接，包括格式不正确或无效、数据包是未知状态、耗尽内存及ICMP错误等。


此外，通过分析协议具体属性和操作，Netfilter可以使用单独的模块执行更详细的连接跟踪。例如，连接跟踪模块ftp、tftp、snmp，等等。

1.5.2　TCP/IP
TCP/IP作为默认的网络协议已经使用很多年了。Linux TCP/IP的实现相当符合其标准。为了得到更好的性能，你应该熟悉基本的TCP/IP网络。

更多详情，参考TCP/IP教程和技术概述。

1．建立连接
在应用程序被传输之前，在客户端和服务器之间的连接已经建立。连接建立的过程被称为TCP/IP 3次握手。图1-26列出了基本的连接建立和终止过程。


图1-26　TCP三次握手

①客户端向它的对等服务器发送一个SYN数据包（有一个SYN标记位的数据包）请求连接。

②服务器接收到数据包，发送回一个SYN+ACK数据包。

③然后客户端向它对等的主机发送一个ACK数据包，建立连接。

连接一旦建立，应用程序的数据可以通过连接传输。当所有数据被传输完成，开始连接关闭过程。

①客户端向服务器发送一个FIN数据包，开始连接终止过程。

②服务器发送ACK回应客户端的FIN，之后如果服务器也不再有数据发送到客户端，那么服务器也向客户端发送FIN数据包。

③客户端向服务器发送一个ACK数据包，终止连接。

图1-27显示了在会话期间连接状态的改变。


图1-27　TCP连接状态图

使用netstat命令可以看到每个TCP/IP会话的连接状态。有关更多详情，可参考2.3.8节的内容。

2．流量控制
TCP/IP实现是一种即使在恶劣的网络传输质量和网络拥塞中，也确保有效的数据传输和保证数据投递的机制。

3．TCP/IP传输窗口
在有关Linux操作系统性能影响因素中，TCP/IP的传输窗口大小有着重要的影响。如图1-28所示，TCP传输窗口是连接的另一边，在请求一个确认之前，一个给定主机能发送和接收的最大数据量。窗口的大小是接收主机提供的，且使用在TCP头部中的窗口大小字段告知发送方。使用传输窗口，主机可以更有效地发送数据包，因为发送主机不需要针对每个发送的数据包等待确认。这使网络利用率更高。延迟确认也提高了效率。TCP窗口开始很小，并且从连接的另一端每一个成功的确认开始慢慢增加。如何优化窗口大小，参考5.7.4节的内容。


图1-28　滑动窗口和延迟ACK

作为一个选择，高速网络可以使用一个被称为窗口缩放（window scaling）的技术，这样甚至可以更大地增加最大传输窗口大小。后续章节将会更详细地分析这些实现的影响。

4．重传
在连接建立、终止、数据传输中，由于各种原因（网络接口故障、低速路由、网络拥塞、奇怪的网络实现，等等）可以引起超时和数据重传。TCP/IP通过排序数据包并试图多次重新发送数据包来处理这种情况。

可以通过配置参数来改变内核的一些行为。在一个高丢包率的网络上，我们可能希望增加TCP尝试SYN连接建立数据包的数量。也可以通过/proc/sys/net下的一些文件改变超时阀值。更多信息，参考5.7.10节的内容。

1.5.3　Offload
如果你的系统上的网络适配器支持硬件Offload功能，则内核可以分出一部分任务给适配器，这样可以降低CPU使用率（参见5.7.7节内容）。

□　Checksum offload（校验和offload）。 IP/TCP/UDP执行校验，通过比较协议头部中的checksum字段的值和计算数据包中数据的值，确保数据包被正确传输。

□　TCP segmentation offload（TSO TCP分段offload）。 当大于支持的最大传输单元（MTU）数据发送到网络适配器时，数据应该被分成MTU大小的数据包。

1.5.4　Bonding模块
有时我们需要比单块网卡所能提供的带宽更多的带宽。升级到更快的网络设备并不总是一个最好的选择。Linux双网卡绑定的实现就是使用两块网卡作为一块网卡使用，这个绑定起来的设备看起来就是一个单独的以太网接口设备，通俗一点讲就是两块网卡具有相同的IP地址，它们被并行链接聚合成一个逻辑链路工作。其实这项技术在Sun和Cisco中早已存在，被称为Trunking和Etherchannel技术。在Linux的2.4.x的内核中也采用这种技术，被称为Bonding。Bonding技术最早应用在集群中，是为了提高集群节点间的数据传输而设计的（参见5.7.1节的内容）。