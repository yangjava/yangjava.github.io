---
layout: post
categories: Lucene
description: none
keywords: Lucene
---
# Lucene源码索引搜索

## 创建IndexReader
FSDirectory的open函数最后返回MMapDirectory、SimpleFSDirectory以及NIOFSDirectory其中之一，后面都假设为NIOFSDirectory。然后调用DirectoryReader的open函数创建一个IndexReader，如下所示，
DirectoryReader::open
```
  public static DirectoryReader open(final Directory directory) throws IOException {
    return StandardDirectoryReader.open(directory, null);
  }
  static DirectoryReader open(final Directory directory, final IndexCommit commit) throws IOException {
    return new SegmentInfos.FindSegmentsFile<DirectoryReader>(directory) {
        ...
    }.run(commit);
  }
```
DirectoryReader的open函数调用StandardDirectoryReader的open函数，进而调用FindSegmentsFile的run函数，最后其实返回一个StandardDirectoryReader。
DirectoryReader::open->FindSegmentsFile::run
```
    public T run() throws IOException {
      return run(null);
    }

    public T run(IndexCommit commit) throws IOException {
      if (commit != null) {
        ...
      }

      long lastGen = -1;
      long gen = -1;
      IOException exc = null;

      for (;;) {
        lastGen = gen;
        String files[] = directory.listAll();
        String files2[] = directory.listAll();
        Arrays.sort(files);
        Arrays.sort(files2);
        if (!Arrays.equals(files, files2)) {
          continue;
        }
        gen = getLastCommitGeneration(files);

        if (gen == -1) {
          throw new IndexNotFoundException();
        } else if (gen > lastGen) {
          String segmentFileName = IndexFileNames.fileNameFromGeneration(IndexFileNames.SEGMENTS, "", gen);
          try {
            T t = doBody(segmentFileName);
            return t;
          } catch (IOException err) {

          }
        } else {
          throw exc;
        }
      }
    }
```
假设索引文件夹下有文件segments_0，segments_1，segments.gen，上面这段代码中的getLastCommitGeneration返回1，即以”segments”开头的文件名里结尾最大的数字，fileNameFromGeneration返回segments_1。最重要的是doBody函数，用来将文件中的段以及域信息读入内存数据结构中，doBody在DirectoryReader的open中被重载，定义如下，
DirectoryReader::open->FindSegmentsFile::run->doBody
```
      protected DirectoryReader doBody(String segmentFileName) throws IOException {
        SegmentInfos sis = SegmentInfos.readCommit(directory, segmentFileName);
        final SegmentReader[] readers = new SegmentReader[sis.size()];
        boolean success = false;
        try {
          for (int i = sis.size()-1; i >= 0; i--) {
            readers[i] = new SegmentReader(sis.info(i), IOContext.READ);
          }
          DirectoryReader reader = new StandardDirectoryReader(directory, readers, null, sis, false, false);
          success = true;

          return reader;
        } finally {

        }
      }

```
doBody首先通过SegmentInfos的readCommit函数读取段信息存入SegmentInfos，然后根据该段信息创建SegmentReader，SegmentReader的构造函数会读取每个段中的域信息并存储在SegmentReader的成员变量里。先来看SegmentInfos的readCommit函数，
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentInfos::readCommit
```
  public static final SegmentInfos readCommit(Directory directory, String segmentFileName) throws IOException {

    long generation = generationFromSegmentsFileName(segmentFileName);
    try (ChecksumIndexInput input = directory.openChecksumInput(segmentFileName, IOContext.READ)) {
      return readCommit(directory, input, generation);
    }
  }

  public ChecksumIndexInput openChecksumInput(String name, IOContext context) throws IOException {
    return new BufferedChecksumIndexInput(openInput(name, context));
  }

  public IndexInput openInput(String name, IOContext context) throws IOException {
    ensureOpen();
    ensureCanRead(name);
    Path path = getDirectory().resolve(name);
    FileChannel fc = FileChannel.open(path, StandardOpenOption.READ);
    return new NIOFSIndexInput("NIOFSIndexInput(path=\"" + path + "\")", fc, context);
  }
```
假设传入的段文件名为segments_1，上面代码中的generationFromSegmentsFileName函数返回1。readCommit函数首先通过openChecksumInput创建BufferedChecksumIndexInput，代表文件的输入流，其中的openInput函数用来创建NIOFSIndexInput，然后根据该输入流通过readCommit函数读取文件内容，
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentInfos::readCommit
```
  public static final SegmentInfos readCommit(Directory directory, ChecksumIndexInput input, long generation) throws IOException {

    int magic = input.readInt();
    if (magic != CodecUtil.CODEC_MAGIC) {
      throw new IndexFormatTooOldException();
    }
    int format = CodecUtil.checkHeaderNoMagic(input, "segments", VERSION_50, VERSION_CURRENT);
    byte id[] = new byte[StringHelper.ID_LENGTH];
    input.readBytes(id, 0, id.length);
    CodecUtil.checkIndexHeaderSuffix(input, Long.toString(generation, Character.MAX_RADIX));

    SegmentInfos infos = new SegmentInfos();
    infos.id = id;
    infos.generation = generation;
    infos.lastGeneration = generation;
    if (format >= VERSION_53) {
      infos.luceneVersion = Version.fromBits(input.readVInt(), input.readVInt(), input.readVInt());
    } else {

    }

    infos.version = input.readLong();
    infos.counter = input.readInt();
    int numSegments = input.readInt();

    if (format >= VERSION_53) {
      if (numSegments > 0) {
        infos.minSegmentLuceneVersion = Version.fromBits(input.readVInt(), input.readVInt(), input.readVInt());
      } else {

      }
    } else {

    }

    long totalDocs = 0;
    for (int seg = 0; seg < numSegments; seg++) {
      String segName = input.readString();
      final byte segmentID[];
      byte hasID = input.readByte();
      if (hasID == 1) {
        segmentID = new byte[StringHelper.ID_LENGTH];
        input.readBytes(segmentID, 0, segmentID.length);
      } else if (hasID == 0) {

      } else {

      }
      Codec codec = readCodec(input, format < VERSION_53);
      SegmentInfo info = codec.segmentInfoFormat().read(directory, segName, segmentID, IOContext.READ);
      info.setCodec(codec);
      totalDocs += info.maxDoc();
      long delGen = input.readLong();
      int delCount = input.readInt();

      long fieldInfosGen = input.readLong();
      long dvGen = input.readLong();
      SegmentCommitInfo siPerCommit = new SegmentCommitInfo(info, delCount, delGen, fieldInfosGen, dvGen);
      if (format >= VERSION_51) {
        siPerCommit.setFieldInfosFiles(input.readSetOfStrings());
      } else {
        siPerCommit.setFieldInfosFiles(Collections.unmodifiableSet(input.readStringSet()));
      }
      final Map<Integer,Set<String>> dvUpdateFiles;
      final int numDVFields = input.readInt();
      if (numDVFields == 0) {
        dvUpdateFiles = Collections.emptyMap();
      } else {
        Map<Integer,Set<String>> map = new HashMap<>(numDVFields);
        for (int i = 0; i < numDVFields; i++) {
          if (format >= VERSION_51) {
            map.put(input.readInt(), input.readSetOfStrings());
          } else {
            map.put(input.readInt(), Collections.unmodifiableSet(input.readStringSet()));
          }
        }
        dvUpdateFiles = Collections.unmodifiableMap(map);
      }
      siPerCommit.setDocValuesUpdatesFiles(dvUpdateFiles);
      infos.add(siPerCommit);

      Version segmentVersion = info.getVersion();
      if (format < VERSION_53) {
        if (infos.minSegmentLuceneVersion == null || segmentVersion.onOrAfter(infos.minSegmentLuceneVersion) == false) {
          infos.minSegmentLuceneVersion = segmentVersion;
        }
      }
    }

    if (format >= VERSION_51) {
      infos.userData = input.readMapOfStrings();
    } else {
      infos.userData = Collections.unmodifiableMap(input.readStringStringMap());
    }
    CodecUtil.checkFooter(input);
    return infos;
  }
```
readCommit函数较长，归纳起来，就是针对所有的段信息，读取并设置id、generation、lastGeneration、luceneVersion、version、counter、minSegmentLuceneVersion、userData等信息；
并且针对每个段，读取或设置段名、段ID、该段删除的文档数、删除文档的gen数字，域文件的gen数字，更新的文档的gen数字、该段域信息文件名、该段更新的文件名，最后将这些信息封装成SegmentInfos并返回。
其中，针对每个段，通过segmentInfoFormat函数获得Lucene50SegmentInfoFormat，调用其read函数读取各个信息封装成SegmentInfo，代码如下，
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentInfos::readCommit->Lucene50SegmentInfoFormat::read
```
  public SegmentInfo read(Directory dir, String segment, byte[] segmentID, IOContext context) throws IOException {
    final String fileName = IndexFileNames.segmentFileName(segment, "", Lucene50SegmentInfoFormat.SI_EXTENSION);
    try (ChecksumIndexInput input = dir.openChecksumInput(fileName, context)) {
      Throwable priorE = null;
      SegmentInfo si = null;
      try {
        int format = CodecUtil.checkIndexHeader(input, Lucene50SegmentInfoFormat.CODEC_NAME,
                                          Lucene50SegmentInfoFormat.VERSION_START,
                                          Lucene50SegmentInfoFormat.VERSION_CURRENT,
                                          segmentID, "");
        final Version version = Version.fromBits(input.readInt(), input.readInt(), input.readInt());

        final int docCount = input.readInt();
        final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;

        final Map<String,String> diagnostics;
        final Set<String> files;
        final Map<String,String> attributes;

        if (format >= VERSION_SAFE_MAPS) {
          diagnostics = input.readMapOfStrings();
          files = input.readSetOfStrings();
          attributes = input.readMapOfStrings();
        } else {
          diagnostics = Collections.unmodifiableMap(input.readStringStringMap());
          files = Collections.unmodifiableSet(input.readStringSet());
          attributes = Collections.unmodifiableMap(input.readStringStringMap());
        }

        si = new SegmentInfo(dir, version, segment, docCount, isCompoundFile, null, diagnostics, segmentID, attributes);
        si.setFiles(files);
      } catch (Throwable exception) {
        priorE = exception;
      } finally {
        CodecUtil.checkFooter(input, priorE);
      }
      return si;
    }
  }

```
该read函数打开.si文件，并从中读取version、docCount、isCompoundFile、diagnostics、attributes、files信息，然后创建SegmentInfo封装这些信息并返回。

回到FindSegmentsFile的doBody函数中，从文件中所有的段信息通过readCommit函数封装成SegmentInfos，然后针对每个段，创建SegmentReader，在其构造函数中读取域信息。
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentReader::SegmentReader
```
  public SegmentReader(SegmentCommitInfo si, IOContext context) throws IOException {
    this.si = si;
    core = new SegmentCoreReaders(si.info.dir, si, context);
    segDocValues = new SegmentDocValues();

    boolean success = false;
    final Codec codec = si.info.getCodec();
    try {
      if (si.hasDeletions()) {
        liveDocs = codec.liveDocsFormat().readLiveDocs(directory(), si, IOContext.READONCE);
      } else {
        liveDocs = null;
      }
      numDocs = si.info.maxDoc() - si.getDelCount();
      fieldInfos = initFieldInfos();
      docValuesProducer = initDocValuesProducer();

      success = true;
    } finally {

    }
  }
```
si.info.dir就是索引文件所在的文件夹，先来看SegmentCoreReaders的构造函数，SegmentCoreReaders的构造函数中会读取域信息，
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentReader::SegmentReader->SegmentCoreReaders::SegmentCoreReaders
```
  SegmentCoreReaders(Directory dir, SegmentCommitInfo si, IOContext context) throws IOException {

    final Codec codec = si.info.getCodec();
    final Directory cfsDir;
    boolean success = false;

    try {
      if (si.info.getUseCompoundFile()) {
        cfsDir = cfsReader = codec.compoundFormat().getCompoundReader(dir, si.info, context);
      } else {
        cfsReader = null;
        cfsDir = dir;
      }

      coreFieldInfos = codec.fieldInfosFormat().read(cfsDir, si.info, "", context);

      final SegmentReadState segmentReadState = new SegmentReadState(cfsDir, si.info, coreFieldInfos, context);
      final PostingsFormat format = codec.postingsFormat();
      fields = format.fieldsProducer(segmentReadState);

      if (coreFieldInfos.hasNorms()) {
        normsProducer = codec.normsFormat().normsProducer(segmentReadState);
        assert normsProducer != null;
      } else {
        normsProducer = null;
      }

      fieldsReaderOrig = si.info.getCodec().storedFieldsFormat().fieldsReader(cfsDir, si.info, coreFieldInfos, context);

      if (coreFieldInfos.hasVectors()) {
        termVectorsReaderOrig = si.info.getCodec().termVectorsFormat().vectorsReader(cfsDir, si.info, coreFieldInfos, context);
      } else {
        termVectorsReaderOrig = null;
      }

      if (coreFieldInfos.hasPointValues()) {
        pointsReader = codec.pointsFormat().fieldsReader(segmentReadState);
      } else {
        pointsReader = null;
      }
      success = true;
    } finally {

    }
  }

```
getUseCompoundFile表示是否会封装成.cfs、.cfe文件，如果封装，就通过compoundFormat函数获得Lucene50CompoundFormat，然后调用其getCompoundReader函数，
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentReader::SegmentReader->SegmentCoreReaders::SegmentCoreReaders->Lucene50CompoundFormat::getCompoundReader
```
  public Directory getCompoundReader(Directory dir, SegmentInfo si, IOContext context) throws IOException {
    return new Lucene50CompoundReader(dir, si, context);
  }

  public Lucene50CompoundReader(Directory directory, SegmentInfo si, IOContext context) throws IOException {
    this.directory = directory;
    this.segmentName = si.name;
    String dataFileName = IndexFileNames.segmentFileName(segmentName, "", Lucene50CompoundFormat.DATA_EXTENSION);
    String entriesFileName = IndexFileNames.segmentFileName(segmentName, "", Lucene50CompoundFormat.ENTRIES_EXTENSION);
    this.entries = readEntries(si.getId(), directory, entriesFileName);
    boolean success = false;

    long expectedLength = CodecUtil.indexHeaderLength(Lucene50CompoundFormat.DATA_CODEC, "");
    for(Map.Entry<String,FileEntry> ent : entries.entrySet()) {
      expectedLength += ent.getValue().length;
    }
    expectedLength += CodecUtil.footerLength(); 

    handle = directory.openInput(dataFileName, context);
    try {
      CodecUtil.checkIndexHeader(handle, Lucene50CompoundFormat.DATA_CODEC, version, version, si.getId(), "");
      CodecUtil.retrieveChecksum(handle);
      success = true;
    } finally {
      if (!success) {
        IOUtils.closeWhileHandlingException(handle);
      }
    }
  }

```
getCompoundReader用来创建Lucene50CompoundReader。Lucene50CompoundReader的构造函数打开.cfs以及.cfe文件，然后通过readEntries函数将其中包含的文件读取出来，存入entries中。

回到SegmentCoreReaders的构造函数。fieldInfosFormat返回Lucene60FieldInfosFormat，其read函数用来读取域信息，
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentReader::SegmentReader->SegmentCoreReaders::SegmentCoreReaders->Lucene60FieldInfosFormat::read
```
  public FieldInfos read(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, IOContext context) throws IOException {
    final String fileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, EXTENSION);
    try (ChecksumIndexInput input = directory.openChecksumInput(fileName, context)) {
      Throwable priorE = null;
      FieldInfo infos[] = null;
      try {
        CodecUtil.checkIndexHeader(input,
                                   Lucene60FieldInfosFormat.CODEC_NAME, 
                                   Lucene60FieldInfosFormat.FORMAT_START, 
                                   Lucene60FieldInfosFormat.FORMAT_CURRENT,
                                   segmentInfo.getId(), segmentSuffix);

        final int size = input.readVInt();
        infos = new FieldInfo[size];

        Map<String,String> lastAttributes = Collections.emptyMap();

        for (int i = 0; i < size; i++) {
          String name = input.readString();
          final int fieldNumber = input.readVInt();
          byte bits = input.readByte();
          boolean storeTermVector = (bits & STORE_TERMVECTOR) != 0;
          boolean omitNorms = (bits & OMIT_NORMS) != 0;
          boolean storePayloads = (bits & STORE_PAYLOADS) != 0;

          final IndexOptions indexOptions = getIndexOptions(input, input.readByte());
          final DocValuesType docValuesType = getDocValuesType(input, input.readByte());
          final long dvGen = input.readLong();
          Map<String,String> attributes = input.readMapOfStrings();

          if (attributes.equals(lastAttributes)) {
            attributes = lastAttributes;
          }
          lastAttributes = attributes;
          int pointDimensionCount = input.readVInt();
          int pointNumBytes;
          if (pointDimensionCount != 0) {
            pointNumBytes = input.readVInt();
          } else {
            pointNumBytes = 0;
          }

          try {
            infos[i] = new FieldInfo(name, fieldNumber, storeTermVector, omitNorms, storePayloads, 
                                     indexOptions, docValuesType, dvGen, attributes,
                                     pointDimensionCount, pointNumBytes);
            infos[i].checkConsistency();
          } catch (IllegalStateException e) {

          }
        }
      } catch (Throwable exception) {
        priorE = exception;
      } finally {
        CodecUtil.checkFooter(input, priorE);
      }
      return new FieldInfos(infos);
    }
  }
```
该read函数打开.fnm文件，读取Field域的基本信息。然后遍历所有域，读取name域名、fieldNumber文档数量，storeTermVector是否存储词向量、omitNorms是否存储norm、storePayloads是否存储payload、indexOptions域存储方式、docValuesType文档内容类型、文档的gen、attributes、pointDimensionCount、pointNumBytes，最后封装成FieldInfo，再封装成FieldInfos。

回到SegmentCoreReaders构造函数。接下来的postingsFormat函数返回PerFieldPostingsFormat，其fieldsProducer函数最终设置fields为FieldsReader。
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentReader::SegmentReader->SegmentCoreReaders::SegmentCoreReaders->PerFieldPostingsFormat::fieldsProducer
```
 public final FieldsProducer fieldsProducer(SegmentReadState state)
      throws IOException {
    return new FieldsReader(state);
  }
```
normsFormat函数返回Lucene53NormsFormat，Lucene53NormsFormat的normsProducer函数返回Lucene53NormsProducer，赋值给normsProducer。
```
  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
    return new Lucene53NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
  }
```
再往下，依次分析，fieldsReaderOrig最终被赋值为CompressingStoredFieldsReader。termVectorsReaderOrig最终被赋值为CompressingTermVectorsReader。pointsReader最终被赋值为Lucene60PointsReader。

回到SegmentReader构造函数，现在已经读取了所有的段信息和域信息了，接下来如果段中有删除信息，就通过liveDocsFormat函数获得Lucene50LiveDocsFormat，并调用其readLiveDocs函数，
DirectoryReader::open->FindSegmentsFile::run->doBody->SegmentReader::SegmentReader->Lucene50LiveDocsFormat::readLiveDocs
```
  public Bits readLiveDocs(Directory dir, SegmentCommitInfo info, IOContext context) throws IOException {
    long gen = info.getDelGen();
    String name = IndexFileNames.fileNameFromGeneration(info.info.name, EXTENSION, gen);
    final int length = info.info.maxDoc();
    try (ChecksumIndexInput input = dir.openChecksumInput(name, context)) {
      Throwable priorE = null;
      try {
        CodecUtil.checkIndexHeader(input, CODEC_NAME, VERSION_START, VERSION_CURRENT, 
                                     info.info.getId(), Long.toString(gen, Character.MAX_RADIX));
        long data[] = new long[FixedBitSet.bits2words(length)];
        for (int i = 0; i < data.length; i++) {
          data[i] = input.readLong();
        }
        FixedBitSet fbs = new FixedBitSet(data, length);
        return fbs;
      } catch (Throwable exception) {
        priorE = exception;
      } finally {
        CodecUtil.checkFooter(input, priorE);
      }
    }
  }
```
readLiveDocs函数打开.liv文件，创建输入流，然后读取并创建FixedBitSet用来标识哪些文件被删除。

回到SegmentReader构造函数。接下来的initFieldInfos函数将SegmentCoreReaders中的coreFieldInfos赋值给fieldInfos，如果段有更新，就重新读取一次。docValuesProducer函数最后会返回FieldsReader。

再回到FindSegmentsFile的doBody函数中，最后创建StandardDirectoryReader并返回。StandardDirectoryReader本身的构造函数较为简单，值得注意的是StandardDirectoryReader的父类CompositeReader的

回到实例中，接下来创建IndexSearcher以及QueryParser，这两个类的构造函数都没有关键内容，这里就不往下看了。
值得注意的是IndexSearcher的构造函数会调用StandardDirectoryReader的getContext函数，进而调用leaves函数，首先是getContext函数，定义在StandardDirectoryReader的父类CompositeReader中，
StandardDirectoryReader::getContext
```
  public final CompositeReaderContext getContext() {
    ensureOpen();
    if (readerContext == null) {
      readerContext = CompositeReaderContext.create(this);
    }
    return readerContext;
  }
```
ensureOpen用来确保IndexWriter未关闭，接下来通过create函数创建CompositeReaderContext，
CompositeReaderContext::create
```
    static CompositeReaderContext create(CompositeReader reader) {
      return new Builder(reader).build();
    }

    public CompositeReaderContext build() {
      return (CompositeReaderContext) build(null, reader, 0, 0);
    }

    private IndexReaderContext build(CompositeReaderContext parent, IndexReader reader, int ord, int docBase) {
      if (reader instanceof LeafReader) {
        final LeafReader ar = (LeafReader) reader;
        final LeafReaderContext atomic = new LeafReaderContext(parent, ar, ord, docBase, leaves.size(), leafDocBase);
        leaves.add(atomic);
        leafDocBase += reader.maxDoc();
        return atomic;
      } else {
        final CompositeReader cr = (CompositeReader) reader;
        final List<? extends IndexReader> sequentialSubReaders = cr.getSequentialSubReaders();
        final List<IndexReaderContext> children = Arrays.asList(new IndexReaderContext[sequentialSubReaders.size()]);
        final CompositeReaderContext newParent;
        if (parent == null) {
          newParent = new CompositeReaderContext(cr, children, leaves);
        } else {
          newParent = new CompositeReaderContext(parent, cr, ord, docBase, children);
        }
        int newDocBase = 0;
        for (int i = 0, c = sequentialSubReaders.size(); i < c; i++) {
          final IndexReader r = sequentialSubReaders.get(i);
          children.set(i, build(newParent, r, i, newDocBase));
          newDocBase += r.maxDoc();
        }
        assert newDocBase == cr.maxDoc();
        return newParent;
      }
    }
```
首先，getSequentialSubReaders函数返回的正是在FindSegmentsFile的doBody函数中为每个段创建的SegmentReader列表，接下来创建CompositeReaderContext，接下来为每个SegmentReader嵌套调用build函数并设置进children中，而SegmentReader继承自LeafReader，因此在嵌套调用的build函数中，会将每个SegmentReader封装为LeafReaderContext并设置进leaves列表中。

因此最后的leaves函数返回封装了SegmentReader的LeafReaderContext列表。

### QueryParser的parse函数
QueryParser类的parse函数，定义在其父类QueryParserBase中，
QueryParserBase::parse
```
  public Query parse(String query) throws ParseException {
    ReInit(new FastCharStream(new StringReader(query)));
    try {
      Query res = TopLevelQuery(field);
      return res!=null ? res : newBooleanQuery().build();
    } catch (ParseException | TokenMgrError tme) {

    } catch (BooleanQuery.TooManyClauses tmc) {

    }
  }
```
parse首先将需要搜索的字符串query封装成FastCharStream，FastCharStream实现了Java的CharStream接口，内部使用了一个缓存，并且可以方便读取并且改变读写指针。然后调用ReInit进行初始化，ReInit以及整个QueryParser都是由JavaCC根据org.apache.lucene.queryparse.classic.QueryParser.jj文件自动生成，设计到的JavaCC的知识可以从网上或者别的书上查找，本博文不会重点分析这块内容。
parse最重要的函数是TopLevelQuery，即返回顶层Query，TopLevelQuery会根据用来搜索的字符串query创建一个树形的Query结构，传入的参数field在QueryParserBase的构造函数中赋值，用来标识对哪个域进行搜索。
QueryParserBase::parse->QueryParser::TopLevelQuery
```
  final public Query TopLevelQuery(String field) throws ParseException {
    Query q;
    q = Query(field);
    jj_consume_token(0);
    {
      if (true) return q;
    }
    throw new Error();
  }
```
TopLevelQuery函数中最关键的是Query函数，由于QueryParser由JavaCC生成，这里只看QueryParser.jj文件。

QueryParser.jj::Query
```
Query Query(String field) :
{
  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
  Query q, firstQuery=null;
  int conj, mods;
}
{
  mods=Modifiers() q=Clause(field)
  {
    addClause(clauses, CONJ_NONE, mods, q);
    if (mods == MOD_NONE)
        firstQuery=q;
  }
  (
    conj=Conjunction() mods=Modifiers() q=Clause(field)
    { addClause(clauses, conj, mods, q); }
  )*
    {
      if (clauses.size() == 1 && firstQuery != null)
        return firstQuery;
      else {
  return getBooleanQuery(clauses);
      }
    }
}
```
Modifiers返回搜索字符串中的”+”或”-“，Conjunction返回连接字符串。Query首先通过Clause函数返回一个子查询，然后调用addClause函数添加该子查询，
QueryParserBase::addClause
```
  protected void addClause(List<BooleanClause> clauses, int conj, int mods, Query q) {
    boolean required, prohibited;

    ...

    if (required && !prohibited)
      clauses.add(newBooleanClause(q, BooleanClause.Occur.MUST));
    else if (!required && !prohibited)
      clauses.add(newBooleanClause(q, BooleanClause.Occur.SHOULD));
    else if (!required && prohibited)
      clauses.add(newBooleanClause(q, BooleanClause.Occur.MUST_NOT));
    else
      throw new RuntimeException("Clause cannot be both required and prohibited");
  }
```
addClause函数中省略的部分是根据参数连接符conj和mods计算required和prohibited的值，然后将Query封装成BooleanClause并添加到clauses列表中。

回到Query函数中，如果子查询clauses列表只有一个子查询，就直接返回，否则通过getBooleanQuery函数封装所有的子查询并最终返回一个BooleanClause。

下面来看Clause函数，即创建一个子查询，
QueryParser.jj::Clause
```
Query Clause(String field) : {
  Query q;
  Token fieldToken=null, boost=null;
}
{
  [
    LOOKAHEAD(2)
    (
    fieldToken=<TERM> <COLON> {field=discardEscapeChar(fieldToken.image);}
    | <STAR> <COLON> {field="*";}
    )
  ]

  (
   q=Term(field)
   | <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?

  )
    {  return handleBoost(q, boost); }
}

```
LOOKAHEAD(2)表示要看两个符号，如果是Field，则要重新调整搜索的域。Clause函数最重要的是Term函数，该函数返回最终的Query，当然Clause函数也可以嵌套调用Query函数生成子查询。

QueryParser.jj::Term
```
Query Term(String field) : {
  Token term, boost=null, fuzzySlop=null, goop1, goop2;
  boolean prefix = false;
  boolean wildcard = false;
  boolean fuzzy = false;
  boolean regexp = false;
  boolean startInc=false;
  boolean endInc=false;
  Query q;
}
{
  (
     (
       term=<TERM>
       | term=<STAR> { wildcard=true; }
       | term=<PREFIXTERM> { prefix=true; }
       | term=<WILDTERM> { wildcard=true; }
       | term=<REGEXPTERM> { regexp=true; }
       | term=<NUMBER>
       | term=<BAREOPER> { term.image = term.image.substring(0,1); }
     )
     [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ]
     [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
     {
       q = handleBareTokenQuery(field, term, fuzzySlop, prefix, wildcard, fuzzy, regexp);
     }
     | ( ( <RANGEIN_START> {startInc=true;} | <RANGEEX_START> )
         ( goop1=<RANGE_GOOP>|goop1=<RANGE_QUOTED> )
         [ <RANGE_TO> ]
         ( goop2=<RANGE_GOOP>|goop2=<RANGE_QUOTED> )
         ( <RANGEIN_END> {endInc=true;} | <RANGEEX_END>))
       [ <CARAT> boost=<NUMBER> ]
        {
          boolean startOpen=false;
          boolean endOpen=false;
          if (goop1.kind == RANGE_QUOTED) {
            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
          } else if ("*".equals(goop1.image)) {
            startOpen=true;
          }
          if (goop2.kind == RANGE_QUOTED) {
            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
          } else if ("*".equals(goop2.image)) {
            endOpen=true;
          }
          q = getRangeQuery(field, startOpen ? null : discardEscapeChar(goop1.image), endOpen ? null : discardEscapeChar(goop2.image), startInc, endInc);
        }
     | term=<QUOTED>
       [ fuzzySlop=<FUZZY_SLOP> ]
       [ <CARAT> boost=<NUMBER> ]
       { q = handleQuotedTerm(field, term, fuzzySlop); }
  )
  { return handleBoost(q, boost); }
}
```
如果一个查询不包括引号(QUOTED)，边界符号(RANGE，例如小括号、中括号等)，大部分情况下最终会通过handleBareTokenQuery函数生成一个Term，代表一个词，然后被封装成一个子查询Clause，最后被封装成一个Query，Clause和Query互相嵌套，即一个Query里可以包含多个Clause，一个Clause里又可以从一个Query开始，最终的叶子节点就是Term对应的Query。

QueryParserBase::handleBareTokenQuery
```
  Query handleBareTokenQuery(String qfield, Token term, Token fuzzySlop, boolean prefix, boolean wildcard, boolean fuzzy, boolean regexp) throws ParseException {
    Query q;

    String termImage=discardEscapeChar(term.image);
    if (wildcard) {
      q = getWildcardQuery(qfield, term.image);
    } else if (prefix) {
      q = getPrefixQuery(qfield,
          discardEscapeChar(term.image.substring
              (0, term.image.length()-1)));
    } else if (regexp) {
      q = getRegexpQuery(qfield, term.image.substring(1, term.image.length()-1));
    } else if (fuzzy) {
      q = handleBareFuzzy(qfield, fuzzySlop, termImage);
    } else {
      q = getFieldQuery(qfield, termImage, false);
    }
    return q;
  }
```
举例来说，查询字符串AAA*代表prefix查询，此时参数prefix为真，A*A代表wildcard查询，此时参数wildcard为真，AA~代表fuzzy模糊查询，此时参数fuzzy为真。这里假设三个都不为真，就是一串平常的单词，最后会通过getFieldQuery生成一个Query，本文重点分析该函数。

QueryParserBase::handleBareTokenQuery->getFieldQuery
```
  protected Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
    return newFieldQuery(getAnalyzer(), field, queryText, quoted);
  }

  protected Query newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)  throws ParseException {
    BooleanClause.Occur occur = operator == Operator.AND ? BooleanClause.Occur.MUST : BooleanClause.Occur.SHOULD;
    return createFieldQuery(analyzer, occur, field, queryText, quoted || autoGeneratePhraseQueries, phraseSlop);
  }
```

getAnalyzer返回QueryParserBase的init函数中设置的分词器，这里为了方便分析，假设为SimpleAnalyzer。quoted以及autoGeneratePhraseQueries表示是否创建PhraseQuery，phraseSlop为位置因子，只有PhraseQuery用得到，这里不管它。下面来看createFieldQuery函数。
QueryParserBase::handleBareTokenQuery->getFieldQuery->newFieldQuery->QueryBuilder::createFieldQuery
```
  protected final Query createFieldQuery(Analyzer analyzer, BooleanClause.Occur operator, String field, String queryText, boolean quoted, int phraseSlop) {

    try (TokenStream source = analyzer.tokenStream(field, queryText);
         CachingTokenFilter stream = new CachingTokenFilter(source)) {

      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);
      PositionIncrementAttribute posIncAtt = stream.addAttribute(PositionIncrementAttribute.class);

      int numTokens = 0;
      int positionCount = 0;
      boolean hasSynonyms = false;

      stream.reset();
      while (stream.incrementToken()) {
        numTokens++;
        int positionIncrement = posIncAtt.getPositionIncrement();
        if (positionIncrement != 0) {
          positionCount += positionIncrement;
        } else {
          hasSynonyms = true;
        }
      }

      if (numTokens == 0) {
        return null;
      } else if (numTokens == 1) {
        return analyzeTerm(field, stream);
      } else if (quoted && positionCount > 1) {
        ...
      } else {
        if (positionCount == 1) {
          return analyzeBoolean(field, stream);
        } else {
          return analyzeMultiBoolean(field, stream, operator);
        }
      }
    } catch (IOException e) {

    }
  }
```

关于分词器的tokenStream以及incrementToken函数在《lucene源码分析—4》中分析过了。直接看最后的结果，假设numTokens==1，则分词器的输出结果只有一个词，则使用analyzeTerm创建最终的Query；
假设positionCount == 1，则表示结果中多个词出现在同一个位置，此时使用analyzeBoolean创建Query；剩下情况表示有多个词，至少两个词出现在不同位置，使用analyzeMultiBoolean创建Query。本文只分析analyzeMultiBoolean函数，
QueryParserBase::handleBareTokenQuery->getFieldQuery->newFieldQuery->QueryBuilder::createFieldQuery->analyzeMultiBoolean
```
 private Query analyzeMultiBoolean(String field, TokenStream stream, BooleanClause.Occur operator) throws IOException {
    BooleanQuery.Builder q = newBooleanQuery();
    List<Term> currentQuery = new ArrayList<>();

    TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);
    PositionIncrementAttribute posIncrAtt = stream.getAttribute(PositionIncrementAttribute.class);

    stream.reset();
    while (stream.incrementToken()) {
      if (posIncrAtt.getPositionIncrement() != 0) {
        add(q, currentQuery, operator);
        currentQuery.clear();
      }
      currentQuery.add(new Term(field, termAtt.getBytesRef()));
    }
    add(q, currentQuery, operator);

    return q.build();
  }

  private void add(BooleanQuery.Builder q, List<Term> current, BooleanClause.Occur operator) {
    if (current.isEmpty()) {
      return;
    }
    if (current.size() == 1) {
      q.add(newTermQuery(current.get(0)), operator);
    } else {
      q.add(newSynonymQuery(current.toArray(new Term[current.size()])), operator);
    }
  }

  public Builder add(Query query, Occur occur) {
    clauses.add(new BooleanClause(query, occur));
    return this;
  }
```
分词器的输出结果保存在TermToBytesRefAttribute中，analyzeMultiBoolean函数将同一个起始位置不同的Term添加到列表currentQuery中，如果同一个位置只有一个Term，则将其封装成TermQuery，如果有多个Term，就封装成SynonymQuery，TermQuery和SynonymQuery最后被封装成BooleanClause，添加到BooleanQuery.Builder中的一个BooleanClause列表中。最后通过BooleanQuery.Builder的build函数根据内置的BooleanClause列表创建一个最终的BooleanClause。


## BooleanQuery的评分过程
BooleanQuery的评分过程，从其score函数开始。

BooleanScorer::score
```
  public int score(LeafCollector collector, Bits acceptDocs, int min, int max) throws IOException {

    ...

    BulkScorerAndDoc top = advance(min);
    while (top.next < max) {
      top = scoreWindow(top, collector, singleClauseCollector, acceptDocs, min, max);
    }

    return top.next;
  }
```
advance函数首先获得第一个匹配文档对应的BulkScorerAndDoc结构，其next成员变量就是文档号，然后通过scoreWindow函数循环处理匹配到的文档，scoreWindow函数默认一次处理最多2048个文档。

BooleanScorer::score->advance
```
  private BulkScorerAndDoc advance(int min) throws IOException {
    final HeadPriorityQueue head = this.head;
    final TailPriorityQueue tail = this.tail;
    BulkScorerAndDoc headTop = head.top();
    BulkScorerAndDoc tailTop = tail.top();
    while (headTop.next < min) {

        ...

        headTop.advance(min);
        headTop = head.updateTop();

        ...

    }
    return headTop;
  }
```
head为HeadPriorityQueue，对应的top函数返回BulkScorerAndDoc，updateTop函数将文档数量小的BulkScorerAndDoc排在前面并返回。

BooleanScorer::score->advance->BulkScorerAndDoc::advance
```
    void advance(int min) throws IOException {
      score(orCollector, null, min, min);
    }

    void score(LeafCollector collector, Bits acceptDocs, int min, int max) throws IOException {
      next = scorer.score(collector, acceptDocs, min, max);
    }

    public int score(LeafCollector collector, Bits acceptDocs, int min, int max) throws IOException {
      collector.setScorer(scorer);
      if (scorer.docID() == -1 && min == 0 && max == DocIdSetIterator.NO_MORE_DOCS) {
        ...
      } else {
        int doc = scorer.docID();
        if (doc < min) {
            doc = iterator.advance(min);
        }
        return scoreRange(collector, iterator, twoPhase, acceptDocs, doc, max);
      }
    }
```
如果是第一次获得文档ID，则docID函数返回-1，min为0，因此此时会调用iterator的advance函数获得文档ID，iterator的类型为BlockDocsEnum，其advance函数从对应的.doc文件中读取文档信息。

BooleanScorer::score->advance->BulkScorerAndDoc::advance->score->DefaultBulkScorer::score->BlockDocsEnum::advance
```
    public int advance(int target) throws IOException {

      if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
        ...
      }

      if (docUpto == docFreq) {
        return doc = NO_MORE_DOCS;
      }

      if (docBufferUpto == BLOCK_SIZE) {
        refillDocs();
      }

      while (true) {
        accum += docDeltaBuffer[docBufferUpto];
        docUpto++;

        if (accum >= target) {
          break;
        }
        docBufferUpto++;
        if (docUpto == docFreq) {
          return doc = NO_MORE_DOCS;
        }
      }

      freq = freqBuffer[docBufferUpto];
      docBufferUpto++;
      return doc = accum;
    }
```
docUpto表示处理的文档指针，docBufferUpto是当前处理的文档指针，BLOCK_SIZE表示缓存大小，如果缓存已满，则调用refillDocs从.doc文件中读取数据到缓存。docDeltaBuffer和freqBuffer缓存分别存储了文档ID和词频，存储方式为差值存储，最后返回需要的文档ID。

获得第一个文档ID后，BooleanScorer的score函数接下来通过scoreWindow函数处理匹配到的文档。

BooleanScorer::score->scoreWindow
```
  private BulkScorerAndDoc scoreWindow(BulkScorerAndDoc top, LeafCollector collector,
      LeafCollector singleClauseCollector, Bits acceptDocs, int min, int max) throws IOException {
    final int windowBase = top.next & ~MASK;
    final int windowMin = Math.max(min, windowBase);
    final int windowMax = Math.min(max, windowBase + SIZE);

    leads[0] = head.pop();
    int maxFreq = 1;
    while (head.size() > 0 && head.top().next < windowMax) {
      leads[maxFreq++] = head.pop();
    }

    if (minShouldMatch == 1 && maxFreq == 1) {

      ...

    } else {
      scoreWindowMultipleScorers(collector, acceptDocs, windowBase, windowMin, windowMax, maxFreq);
      return head.top();
    }
  }
```
scoreWindow函数一次只处理最多SIZE大小的文档，windowMin和windowMax分别表示当前处理的文档号的最小值和最大值。接下来获得对应的BulkScorerAndDoc保存在leads数组中，最后调用scoreWindowMultipleScorers函数继续处理。

BooleanScorer::score->scoreWindow->scoreWindowMultipleScorers
```
  private void scoreWindowMultipleScorers(LeafCollector collector, Bits acceptDocs, int windowBase, int windowMin, int windowMax, int maxFreq) throws IOException {

    ...

    if (maxFreq >= minShouldMatch) {

      ...

      scoreWindowIntoBitSetAndReplay(collector, acceptDocs, windowBase, windowMin, windowMax, leads, maxFreq);
    }

    ...
  }
```
scoreWindowMultipleScorers函数会继续调用scoreWindowIntoBitSetAndReplay进行处理。

BooleanScorer::score->scoreWindow->scoreWindowMultipleScorers->scoreWindowIntoBitSetAndReplay
```
  private void scoreWindowIntoBitSetAndReplay(LeafCollector collector, Bits acceptDocs,
      int base, int min, int max, BulkScorerAndDoc[] scorers, int numScorers) throws IOException {
    for (int i = 0; i < numScorers; ++i) {
      final BulkScorerAndDoc scorer = scorers[i];
      scorer.score(orCollector, acceptDocs, min, max);
    }

    scoreMatches(collector, base);
    Arrays.fill(matching, 0L);
  }
```
scoreWindowIntoBitSetAndReplay函数遍历当前的BulkScorerAndDoc数组，调用其score函数计算评分。BulkScorerAndDoc的score函数最终会调用到OrCollector的collect函数。scoreMatches对本次的处理结果进行最终处理。最终清空matching数组，以便后续2048个文档的分析。

BooleanScorer::score->scoreWindow->scoreWindowMultipleScorers->scoreWindowIntoBitSetAndReplay->BulkScorerAndDoc::score->DefaultBulkScorer::score->scoreRange->OrCollector::collect
```
    public void collect(int doc) throws IOException {
      final int i = doc & MASK;
      final int idx = i >>> 6;
      matching[idx] |= 1L << i;
      final Bucket bucket = buckets[i];
      bucket.freq++;
      bucket.score += scorer.score();
    }
```
collect函数一次最多处理2048个文档，成员变量matching用比特位记录匹配到了哪些文档，buckets存储当前处理的最多2048个文档的得分，分别调用score函数计算得到。其中，2048个文档被分成32个组，每组64个比特位记录哪些文档匹配。

BooleanScorer::score->scoreWindow->scoreWindowMultipleScorers->scoreWindowIntoBitSetAndReplay->scoreMatches
```
  private void scoreMatches(LeafCollector collector, int base) throws IOException {
    long matching[] = this.matching;
    for (int idx = 0; idx < matching.length; idx++) {
      long bits = matching[idx];
      while (bits != 0L) {
        int ntz = Long.numberOfTrailingZeros(bits);
        int doc = idx << 6 | ntz;
        scoreDocument(collector, base, doc);
        bits ^= 1L << ntz;
      }
    }
  }

```
scoreMatches根据比特位查看匹配到的文档号是多少，然后调用scoreDocument函数计算最终得分并排序。

BooleanScorer::score->scoreWindow->scoreWindowMultipleScorers->scoreWindowIntoBitSetAndReplay->scoreMatches->scoreDocument
```
  private void scoreDocument(LeafCollector collector, int base, int i) throws IOException {
    final FakeScorer fakeScorer = this.fakeScorer;
    final Bucket bucket = buckets[i];
    if (bucket.freq >= minShouldMatch) {
      fakeScorer.freq = bucket.freq;
      fakeScorer.score = (float) bucket.score * coordFactors[bucket.freq];
      final int doc = base | i;
      fakeScorer.doc = doc;
      collector.collect(doc);
    }
    bucket.freq = 0;
    bucket.score = 0;
  }
```
这里计算文档号，并从buckets数组中取出前面的计算结果，然后调用collect函数处理最终结果。这里的collector是最先创建的SimpleTopScoreDocCollector，其collect函数就是比较分数，对最终要返回的文档进行排序。


## PhraseQuery短语查询
PhraseQuery的源码，PhraseQuery顾名思义是短语查询，先来看PhraseQuery是如何构造的，
```
       PhraseQuery.Builder queryBuilder = new PhraseQuery.Builder();
        queryBuilder.add(new Term("body","hello"));
        queryBuilder.add(new Term("body","world"));
        queryBuilder.setSlop(100);
        PhraseQuery query = queryBuilder.build();
```
首先创建PhraseQuery.Builder，然后向其添加短语中的每个词Term，PhraseQuery要求Term的数量至少为2个，这很容易理解，例子中的“body”表示域名，“hello”和“world”代表词的内容。然后通过setSlop设置词的最大间距，最后通过build函数创建PhraseQuery 。

创建完PhraseQuery后，就调用IndexSearcher的search函数进行查询，最终会执行到如下代码，
```
  public void search(Query query, Collector results)
    throws IOException {
    search(leafContexts, createNormalizedWeight(query, results.needsScores()), results);
  }
```
createNormalizedWeight函数找出短语所在文档的整体信息，然后通过search函数对每个文档进行评分，筛选出最后的文档。

IndexSearcher::createNormalizedWeight
```
  public Weight createNormalizedWeight(Query query, boolean needsScores) throws IOException {
    query = rewrite(query);
    Weight weight = createWeight(query, needsScores);

    ...

    return weight;
  }
```
对PhraseQuery而言，rewrite函数什么也不做，即PhraseQuery不需要改写。createNormalizedWeight函数接下来通过createWeight函数创建Weight，封装短语的整体信息。

IndexSearcher::createNormalizedWeight->createWeight
```
  public Weight createWeight(Query query, boolean needsScores) throws IOException {

    ...

    Weight weight = query.createWeight(this, needsScores);

    ...

    return weight;
  }

  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
    return new PhraseWeight(searcher, needsScores);
  }
```
createWeight函数最终会创建PhraseWeight。

IndexSearcher::createNormalizedWeight->createWeight->PhraseQuery::createWeight->PhraseWeight::PhraseWeight
```
  public PhraseWeight(IndexSearcher searcher, boolean needsScores)
    throws IOException {
    final int[] positions = PhraseQuery.this.getPositions();
    this.needsScores = needsScores;
    this.similarity = searcher.getSimilarity(needsScores);
    final IndexReaderContext context = searcher.getTopReaderContext();
    states = new TermContext[terms.length];
    TermStatistics termStats[] = new TermStatistics[terms.length];
    for (int i = 0; i < terms.length; i++) {
      final Term term = terms[i];
      states[i] = TermContext.build(context, term);
      termStats[i] = searcher.termStatistics(term, states[i]);
    }
    stats = similarity.computeWeight(searcher.collectionStatistics(field), termStats);
  }
```
getPositions函数返回查询的每个词在查询短语中的位置，例如查询“hello world”，则getPositions返回[0，1]。
getSimilarity默认返回BM25Similarity，用于评分，开发者可以定义自己的Similarity并设置进IndexerSearch中，这里就会返回自定义的Similarity。
getTopReaderContext默认返回CompositeReaderContext。再往下，遍历每个查询的词Term，通过TermContext的build函数获取每个词的整体信息，然后通过termStatistics函数将部分信息封装成TermStatistics。
最后通过Similarity的computeWeight函数计算所有词的整体信息保存在stats中。
这些函数在前面的章节已经分析过了，和PhraseQuery并无关系，这里就不往下看了。

回到一开始的search函数中，接下来继续通过search函数进行搜索。

IndexSearcher::search
```
  protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector)
      throws IOException {
    for (LeafReaderContext ctx : leaves) {
      final LeafCollector leafCollector = collector.getLeafCollector(ctx);
      BulkScorer scorer = weight.bulkScorer(ctx);
      scorer.score(leafCollector, ctx.reader().getLiveDocs());
    }
  }
```
getLeafCollector返回的LeafCollector用来打分并对评分后的文档进行排序。
bulkScorer返回的BulkScorer用来为多个文档打分。
最终通过BulkScorer的scorer函数进行打分并获得最终的结果。

IndexSearcher::search->PhraseWeight::bulkScorer
```
  public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {
    Scorer scorer = scorer(context);
    return new DefaultBulkScorer(scorer);
  }
```
scorer函数最终会调用PhraseWeight的scorer函数获得Scorer，然后通过DefaultBulkScorer进行封装和处理。

IndexSearcher::search->PhraseWeight::bulkScorer->scorer
```
    public Scorer scorer(LeafReaderContext context) throws IOException {
      final LeafReader reader = context.reader();
      PostingsAndFreq[] postingsFreqs = new PostingsAndFreq[terms.length];

      final Terms fieldTerms = reader.terms(field);
      final TermsEnum te = fieldTerms.iterator();
      float totalMatchCost = 0;

      for (int i = 0; i < terms.length; i++) {
        final Term t = terms[i];
        final TermState state = states[i].get(context.ord);
        te.seekExact(t.bytes(), state);
        PostingsEnum postingsEnum = te.postings(null, PostingsEnum.POSITIONS);
        postingsFreqs[i] = new PostingsAndFreq(postingsEnum, positions[i], t);
        totalMatchCost += termPositionsCost(te);
      }

      return new SloppyPhraseScorer(this, postingsFreqs, slop,
                                        similarity.simScorer(stats, context),
                                        needsScores, totalMatchCost);
    }
```

terms获得FieldReader，并通过其iterator函数创建SegmentTermsEnum，该过程在前面的章节有叙述。
接下来遍历待搜索的词Term，针对每个Term，通过SegmentTermsEnum的postings函数返回倒排列表对应的.doc、.pos和.pay文件的读取信息，并根据.tip文件中的索引数据设置这三个文件的输出流指针。封装成PostingsAndFreq。
假设slop>0，scorer函数最后创建SloppyPhraseScorer并返回，SloppyPhraseScorer用于计算短语查询的最终得分。

IndexSearcher::search->PhraseWeight::bulkScorer->DefaultBulkScorer
```
    public DefaultBulkScorer(Scorer scorer) {
      this.scorer = scorer;
      this.iterator = scorer.iterator();
      this.twoPhase = scorer.twoPhaseIterator();
    }
```
DefaultBulkScorer构造函数中的iterator和twoPhaseIterator函数都是创建迭代器，用于获取到下一个文档ID，还可以计算词位置登信息。

回到IndexSearcher函数中，接下来通过刚刚创建的DefaultBulkScorer的score函数计算最终的文档得分并排序，DefaultBulkScorer的score函数最终会调用到scoreAll函数。

IndexSearcher::search->DefaultBulkScorer::score->score->scoreAll
```
    static void scoreAll(LeafCollector collector, DocIdSetIterator iterator, TwoPhaseIterator twoPhase, Bits acceptDocs) throws IOException {
      final DocIdSetIterator approximation = twoPhase.approximation();
      for (int doc = approximation.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = approximation.nextDoc()) {
        if ((acceptDocs == null || acceptDocs.get(doc)) && twoPhase.matches()) {
          collector.collect(doc);
        }
      }
    }
```
approximation是前面在DefaultBulkScorer的构造函数中创建的继承自DocIdSetIterator的ConjunctionDISI，通过其nextDoc函数获取下一个文档ID，然后调用TwoPhaseIterator的matches信息获取词的位置信息，最后通过LeafCollector的collect函数完成最终的打分并排序。

IndexSearcher::search->DefaultBulkScorer::score->score->scoreAll->ConjunctionDISI::nextDoc
```
  public int nextDoc() throws IOException {
    return doNext(lead.nextDoc());
  }
```
这里的变量lead实质上未BlockPostingsEnum，其nextDoc获得.doc文件中对应的倒排列表中的下一个文档ID。然后通过doNext函数查找下一个文档ID，对应的文档包含了查询短语中的所有词，如果不包含，则要继续查找下一个文档。

IndexSearcher::search->DefaultBulkScorer::score->score->scoreAll->ConjunctionDISI::nextDoc->doNext
```
  private int doNext(int doc) throws IOException {
    for(;;) {
      advanceHead: for(;;) {
        for (DocIdSetIterator other : others) {
          if (other.docID() < doc) {
            final int next = other.advance(doc);

            if (next > doc) {
              doc = lead.advance(next);
              break advanceHead;
            }
          }
        }
        return doc;
      }
    }
  }
```
others是除了lead外的所有词对应的DocIdSetIterator列表，例如查找“hello world champion”，则假设此时lead为“hello”，others对应的DocIdSetIterator列表则分别包含“world”和“champion”，advance函数从other中找到一个等于或大于当前文档ID的文档，这里有一个默认条件，就是lead中的文档ID是最小的，因此当others中的所有文档ID不大于当前文档ID，就说明该文档ID包含了所有的词，返回当前文档ID。如果某个other的文档ID大于了当前的文档ID，则要重新计算当前文档ID，并继续循环直到找到为止。

获取到文档ID后，就要通过phraseFreq计算词的位置信息了。

IndexSearcher::search->DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq
```
  private float phraseFreq() throws IOException {
    if (!initPhrasePositions()) {
      return 0.0f;
    }
    float freq = 0.0f;
    numMatches = 0;
    PhrasePositions pp = pq.pop();
    int matchLength = end - pp.position;
    int next = pq.top().position; 
    while (advancePP(pp)) {
      if (pp.position > next) {
        if (matchLength <= slop) {
          freq += docScorer.computeSlopFactor(matchLength);
          numMatches++;
        }      
        pq.add(pp);
        pp = pq.pop();
        next = pq.top().position;
        matchLength = end - pp.position;
      } else {
        int matchLength2 = end - pp.position;
        if (matchLength2 < matchLength) {
          matchLength = matchLength2;
        }
      }
    }
    if (matchLength <= slop) {
      freq += docScorer.computeSlopFactor(matchLength);
      numMatches++;
    }    
    return freq;
  }
```
省略的部分代码用于处理重复的词，例如当查询“hello hello world”时，就会出现两个重复的“hello”。initPhrasePositions用来初始化。matchLength表示计算得到的两个词的距离，当matchLength小于slop时，调用computeSlopFactor计算得分并累加，公式为1/(distance+1)。
这里不逐行逐字地分析这段代码了，举个例子就明白了，假设一篇文档如下
“…hello(5)…world(10)…world(13)…hello(21)…”
查询短语为“hello world”。
则首先找到第一个hello和第一个world，计算得到matchLength为10-5=5，

IndexSearcher::search->DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq->initPhrasePositions
```
  private boolean initPhrasePositions() throws IOException {
    end = Integer.MIN_VALUE;
    if (!checkedRpts) {
      return initFirstTime();
    }
    if (!hasRpts) {
      initSimple();
      return true;
    }
    return initComplex();
  }
```
DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq->initPhrasePositions->initFirstTime
```
  private boolean initFirstTime() throws IOException {
    checkedRpts = true;
    placeFirstPositions();

    LinkedHashMap<Term,Integer> rptTerms = repeatingTerms(); 
    hasRpts = !rptTerms.isEmpty();

    if (hasRpts) {
      rptStack = new PhrasePositions[numPostings];
      ArrayList<ArrayList<PhrasePositions>> rgs = gatherRptGroups(rptTerms);
      sortRptGroups(rgs);
      if (!advanceRepeatGroups()) {
        return false;
      }
    }

    fillQueue();
    return true;
  }
```
checkedRpts设置为true，表示初始化过了。
placeFirstPositions函数初始化位置信息。
repeatingTerms函数获取重复的词。
如果存在重复的词，则此时hasRpts为true，则要进行相应的处理，本章不考虑这种情况。
最后调用fillQueue函数设置end成员变量，end表示当前最大的位置。

DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq->initPhrasePositions->initFirstTime->placeFirstPositions
```
  private void placeFirstPositions() throws IOException {
    for (PhrasePositions pp : phrasePositions) {
      pp.firstPosition();
    }
  }
```
phrasePositions对应每个Term的PhrasePositions，内部封装了倒排列表信息，firstPosition函数初始化位置信息，内部会读取.pos文件。

DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq->initPhrasePositions->initFirstTime->placeFirstPositions->PhrasePositions::firstPosition
```
  final void firstPosition() throws IOException {
    count = postings.freq();
    nextPosition();
  }

  final boolean nextPosition() throws IOException {
    if (count-- > 0) {
      position = postings.nextPosition() - offset;
      return true;
    } else
      return false;
  }
```
freq表示当前有多少可以读取的位置信息。
然后调用nextPosition获得下一个位置信息，进而调用BlockPostingsEnum的nextPosition函数获取下一个位置信息，offset是起始的位移，例如查询“abc def”，当获得abc所在文档的位置时，offset为0，当获得def所在文档的位置时，offset为1。

DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq->initPhrasePositions->initFirstTime->placeFirstPositions->PhrasePositions::firstPosition->nextPosition->BlockPostingsEnum::nextPosition
```
    public int nextPosition() throws IOException {

      if (posPendingFP != -1) {
        posIn.seek(posPendingFP);
        posPendingFP = -1;
        posBufferUpto = BLOCK_SIZE;
      }

      if (posPendingCount > freq) {
        skipPositions();
        posPendingCount = freq;
      }

      if (posBufferUpto == BLOCK_SIZE) {
        refillPositions();
        posBufferUpto = 0;
      }
      position += posDeltaBuffer[posBufferUpto++];
      posPendingCount--;
      return position;
    }
```
posPendingFP是从.tip文件中读取得到的.pos文件的指针。
通过seek函数移动到读取位置posPendingFP，然后将posPendingFP指针重置。
设置posBufferUpto为BLOCK_SIZE后面会调用refillPositions将.pos文件中的信息读入缓存中。
posPendingCount变量表示待读取的词位置的个数，假设单词“abc”在文档1中的频率为2，在文档2中的频率为3，当读取完文档1中的频率2而没有处理，再读取文档2中的频率3，此时posPendingCount为2+3=5。
当posPendingCount大于当前文档的频率freq时，则调用skipPositions跳过之前读取的信息。
posBufferUpto等于BLOCK_SIZE表示缓存已经读到末尾，或者表示首次读取缓存，此时通过refillPositions函数将.pos文件中的信息读取到缓存中，并将缓存的指针posBufferUpto设置为0。
最后返回的position是从缓存posDeltaBuffer读取到的最终的位置，并将待读取的个数posPendingCount减1。

DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq->initPhrasePositions->initFirstTime->placeFirstPositions->PhrasePositions::firstPosition->nextPosition->BlockPostingsEnum::nextPosition->skipPositions
```
    private void skipPositions() throws IOException {
      int toSkip = posPendingCount - freq;

      final int leftInBlock = BLOCK_SIZE - posBufferUpto;
      if (toSkip < leftInBlock) {
        posBufferUpto += toSkip;
      } else {
        toSkip -= leftInBlock;
        while(toSkip >= BLOCK_SIZE) {
          forUtil.skipBlock(posIn);
          toSkip -= BLOCK_SIZE;
        }
        refillPositions();
        posBufferUpto = toSkip;
      }

      position = 0;
    }

```
toSkip计算需要跳过多少。
leftInBlock表示当前缓存中的剩余空间。
如果toSkip小于leftInBlock则表示缓存中还有富余空间，此时直接更新缓存的指针posBufferUpto即可。
如果toSkip大于等于leftInBlock，则先减去当前缓存的剩余空间，然后判断剩余的toSkip是否大于缓存的最大空间BLOCK_SIZE，如果大于，就循环将文件指针向前移动BLOCK_SIZE，再将toSkip减去BLOCK_SIZE，知道toSkip小于BLOCK_SIZE，此时通过refillPositions函数再向缓存填充BLOCK_SIZE大小的数据，设置文件指针为toSkip。
因为位置信息存储的是差值，因此最后将位置position重置为0。

DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq->initPhrasePositions->initFirstTime->repeatingTerms
```
  private LinkedHashMap<Term,Integer> repeatingTerms() {
    LinkedHashMap<Term,Integer> tord = new LinkedHashMap<>();
    HashMap<Term,Integer> tcnt = new HashMap<>();
    for (PhrasePositions pp : phrasePositions) {
      for (Term t : pp.terms) {
        Integer cnt0 = tcnt.get(t);
        Integer cnt = cnt0==null ? new Integer(1) : new Integer(1+cnt0.intValue());
        tcnt.put(t, cnt);
        if (cnt==2) {
          tord.put(t,tord.size());
        }
      }
    }
    return tord;
  }
```
PhrasePositions的terms成员变量保存了该PhrasePositions的词，这里其实是遍历搜索的所有词，将重复的词放入最终返回的HashMap中。

DefaultBulkScorer::score->score->scoreAll->TwoPhaseIterator::matches->phraseFreq->initPhrasePositions->initFirstTime->fillQueue
```
  private void fillQueue() {
    pq.clear();
    for (PhrasePositions pp : phrasePositions) {
      if (pp.position > end) {
        end = pp.position;
      }
      pq.add(pp);
    }
  }
```
fillQueue依次读取每个PhrasePositions，将第一个Term在文档中出现位置的最大值保存在end中。






























