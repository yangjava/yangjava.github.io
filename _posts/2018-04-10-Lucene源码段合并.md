---
layout: post
categories: [Lucene]
description: none
keywords: Lucene
---
# Lucene源码段合并
Lucene索引是基于多个段Segment创建。为了实现索引高效实现，背后使用基于Segment的分段架构存储。数据批量写入的同时存储在一个单独的段中，然后定期进行段合并。

## 引言
理解段的合并之前，你必须对段的数据结构有深入的理解。毕竟多个段的合并本质就是数据结构内容重新的组织。

Lucene索引结构由正排和倒排结构组成，用于支持文本的检索。索引Index分为多个段segment，每次添加新的doc都会存入一个新的segment中，多个小的segment最终又会合并成新的较大的segment。

segment存储着具体的documents文档集，每个文档由多个fields域组成，一个field的值由多个term分词组成，一个term包含了文档词频，分词在字段值的偏移量和位置信息。

简言之，存在如下递进表达关系：
```
Index -> Segments -> Documents -> Fields -> Terms
```

## 测试用例
SegmentReader有来管理不同段的文档。对于二个不同SegmentReader段的文档内容进行合并，将二个小的Segments生成一个较大的Segment。
```
public void testMerge() {                             
  SegmentMerger merger 
  = new SegmentMerger(mergedDir, mergedSegment);
  merger.add(reader1);
  merger.add(reader2);
  int docsMerged = merger.merge();
  merger.closeReaders();
  assertTrue(docsMerged == 2); 
```

## Segment merge
merge作为段合并的入口函数。段之间的合并事实上涉及到文档域元信息(field metadata)，分词字典(term-dictionary)，分词倒排索引结构(Posting)和分词的正向索引结构(Term-Vector)的重组。
```
final int merge(boolean mergeDocStores) {
  this.mergeDocStores = mergeDocStores;
  mergedDocs = mergeFields();
  mergeTerms();
  mergeNorms();

  if (mergeDocStores && fieldInfos.hasVectors())
    mergeVectors();
  return mergedDocs;
}
```

## 合并Field域：SegmentMerger#mergeFields
mergeFields函数根据当前多个segment段内文档管理的Fields信息，即对fdt与fdx文件进行合并生成新的文件。
```
private final int mergeFields() {

  if (!mergeDocStores) {
    final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);
    fieldInfos = (FieldInfos) sr.fieldInfos.clone();
  } else {
    fieldInfos = new FieldInfos(); 
  }

  for (int i = 0; i < readers.size(); i++) {
    IndexReader reader = (IndexReader) readers.elementAt(i);
    if (reader instanceof SegmentReader) {
      SegmentReader segmentReader = (SegmentReader) reader;
      for (int j = 0; 
           j < segmentReader.getFieldInfos().size(); j++) {
        
        FieldInfo fi 
          = segmentReader.getFieldInfos().fieldInfo(j);
        fieldInfos.add(fi.name, fi.isIndexed, 
                       fi.storeTermVector, 
                       fi.storePositionWithTermVector, 
                       fi.storeOffsetWithTermVector, 
                       !reader.hasNorms(fi.name), 
                       fi.storePayloads);
      }
    } 
```

## add 添加域
如果一个field已经在一个doc中出现，并提前向byName进行了注册，此时直接进入else分支，对存在的fieldInfo实例字段进行“丰富”。反之，进入#addinternal函数才是真正向byName字典添加新field
```
public FieldInfo fieldInfo(String fieldName) {
  return (FieldInfo) byName.get(fieldName);
}
public FieldInfo add(String name, boolean isIndexed, 
                     boolean storeTermVector,
                     boolean storePositionWithTermVector, 
                     boolean storeOffsetWithTermVector,
                     boolean omitNorms, boolean storePayloads){
  FieldInfo fi = fieldInfo(name);
  if (fi == null) {
    return addInternal(name, isIndexed, storeTermVector, 
                       storePositionWithTermVector, 
                       storeOffsetWithTermVector, omitNorms, 
                       storePayloads);
  } else {
    if (fi.isIndexed != isIndexed) {
      fi.isIndexed = true;                  
    }
    if (fi.storeTermVector != storeTermVector) {
      fi.storeTermVector = true;              
    }
    if (fi.storePositionWithTermVector != storePositionWithTermVector) {
      fi.storePositionWithTermVector = true;      
    }
    if (fi.storeOffsetWithTermVector != storeOffsetWithTermVector) {
      fi.storeOffsetWithTermVector = true;   
    }
    if (fi.omitNorms != omitNorms) {
      fi.omitNorms = false;         
    }
    if (fi.storePayloads != storePayloads) {
      fi.storePayloads = true;
    }

  }
  return fi;
}
```
写fnm文件
```
fieldInfos.write(directory, segment + ".fnm");
```

## XXX.fnm
fnm索引文件包含某个segment段中管理的所有field域的元信息，比如，记录了当前segment段中域的数量FieldsCount，每个域field的名称FieldName，以及每个域存储和索引方式的Bit位标记FieldBit。

索引index bit与存储store bit对比？
为什么存在一个域允许存储而不被索引呢？一个文档中所有的域信息中，有一部分域希望不被索引但可以被搜索。简言之，文档中允许存储但不允许索引的域，可能由于其他域信息被搜索到一并作为结果返回。

payload设计与应用？
Lucene索引是以倒排索引结构进行存储与管理。对于每一个term分词，它的term-diction分词字典与分词的倒排索引分开存储在tii/tis文件和frq/prx文件中。

当一个term分词包含了payload负载信息时，Lucence会在倒排索引结构中为分词分配相应的Payload存储空间。具体体现在prx文件的Payload --> <PayloadLength?,PayloadData>数据格式定义。这是一个高效率的空间实现。
```
ProxFile (.prx) --> <TermPositions> TermCount

TermPositions --> <Positions> DocFreq
Positions --> <PositionDelta,Payload?> Freq
PositionDelta --> VInt

Payload --> <PayloadLength?,PayloadData>
PayloadLength --> VInt
PayloadData --> bytePayloadLength
```
为什么Payload与分词集中存储在prx文件中是一种高效方案呢？

我们知道，XXX.fnm文件描绘段内每个Field域的存储和索引方式的Bit位标记。其中Byte字节的第3个位就表达当前域是否包含了Payload信息，那么，Payload信息为什么不直接存储到fnm或者fdt文件中呢？

我们知道Field倒排索引信息(frq/prx)与Field元信息(fnm/fdt)是分开独立存储的，如果查询涉及大量文档元数据，I/O效率会变差。相反，Lucene将payload信息直接存储在分词倒排索引文件(prx)，集中存储与集中读取对IO也算是一种效率提升。

目前Payload已广泛应用于Lucene引擎，比如改进Lucene对日期检索，提高特定分词的权重。未来会单独开辟一篇文章来讨论Payload。

## mergeDocStores
```
final int merge() {
  return merge(true);
}
```
merge(true)调用传参为true，表达出mergeDocStores(true)需要将field/field-value，即field payload信息进行merge并持久化，涉及到fdt与fdx二个Lucene索引文件。
```
if (mergeDocStores) {
  SegmentReader[] matchingSegmentReaders 
    = new SegmentReader[readers.size()];
```

## XXX.fdt
域数据文件fdt保存某个段内所有域的具体信息。

比如某个段有SegSize篇文档， fdt文件中包含SegSize项，每项是DocFieldData类型，代表着一篇文档的域信息。

DocFieldData代表每一篇文档，首先包含了FieldCount个域，接下来FieldCount个元素组成的FieldData数组。FieldData表示一个域的元数据，其中包含有域的编号FieldNum，8位比特标记以及域值FieldValue.

## XXX.fdx
域文件fdx文件用来对fdt中每一篇文档的起始与结束地址进行索引。

如果段内有SegSize个文档，那么fdx文件就有SegSize个FieldValuePosition项，对应着SegSize篇文档在fdt的起始偏移量。

merge fdt/fdx
same
```
 for (int j = 0; same && j < segmentFieldInfos.size(); 
      j++)
      same = fieldInfos.fieldName(j).equals(segmentFieldInfos.fieldName(j));
```
对于不同segmentreader下的doc之间field是否完全相同，它必须将文档管理的所有字段与另一个文档下的所有字段进行累积的一一比较，只要文档中有一个字段与另一个文档的对应字段不匹配，将视为文档之间元信息的不同，会影响后续fdt合并的手段。
```
for (int i = 0; i < readers.size(); i++) {
  IndexReader reader = (IndexReader) readers.elementAt(i);
  if (reader instanceof SegmentReader) {
    SegmentReader segmentReader = (SegmentReader) reader;
    boolean same = true;
    FieldInfos segmentFieldInfos = segmentReader.getFieldInfos();
    for (int j = 0; same && j < segmentFieldInfos.size(); j++)
      same = fieldInfos.fieldName(j).equals(segmentFieldInfos.fieldName(j));
    if (same) {
      matchingSegmentReaders[i] = segmentReader;
    }
  }
}
```
FieldsWriter负责封装对 fdt文件索引的写
```
// merge field values
    final FieldsWriter fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);

    try {
      for (int i = 0; i < readers.size(); i++) {
        final IndexReader reader = (IndexReader) readers.elementAt(i);
        final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];
        final FieldsReader matchingFieldsReader;
        if (matchingSegmentReader != null)
          matchingFieldsReader = matchingSegmentReader.getFieldsReader();
        else
          matchingFieldsReader = null;
        final int maxDoc = reader.maxDoc();
        for (int j = 0; j < maxDoc;) {
          if (!reader.isDeleted(j)) { // skip deleted docs
            if (matchingSegmentReader != null) {
              // We can optimize this case (doing a bulk
              // byte copy) since the field numbers are
              // identical
              int start = j;
              int numDocs = 0;
              do {
                j++;
                numDocs++;
              } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);

              IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);
              fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);
              docCount += numDocs;
              if (checkAbort != null)
                checkAbort.work(300*numDocs);
            } else {
              fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));
              j++;
              docCount++;
              if (checkAbort != null)
                checkAbort.work(300);
            }
          } else
            j++;
        }
      }
    } finally {
      fieldsWriter.close();
    }

  } else
    for (int i = 0; i < readers.size(); i++)
      docCount += ((IndexReader) readers.elementAt(i)).numDocs();

  return docCount;
}
```
统计一段连续的，未标识删除的文档列表
统计一段连续的，未标识删除的文档列表，并记录它的起始start-docid和文档数目numDocs。满足条件的连续文档列表，在fdt索引文件中关于 field/field-value (即field payload)的信息，可以进行批量拷贝。（批量字节拷贝：doing a bulk byte copy since the field numbers are identical)
```
for (int j = 0; j < maxDoc;) {
  if (!reader.isDeleted(j)) { 
    if (matchingSegmentReader != null) {
      int start = j;
      int numDocs = 0;
      do {
        j++;
        numDocs++;
      } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);
```
matchingFieldsReader.rawDocs
FieldsReader#rawDocs函数内部引用indexStream与fieldsSteram分别操作fdx与fdt索引文件。
```
IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);
fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);


final class FieldsReader {
  private final FieldInfos fieldInfos;
  private final IndexInput cloneableFieldsStream;
 
  private final IndexInput fieldsStream;
  private final IndexInput indexStream;

  final IndexInput rawDocs(int[] lengths, 
                           int startDocID, int numDocs){
    indexStream.seek(startDocID * 8L);
    long startOffset = indexStream.readLong();
    long lastOffset = startOffset;
    int count = 0;
    while (count < numDocs) {
      final long offset;
      final int docID = startDocID + count + 1;
      assert docID <= numTotalDocs;
      if (docID < numTotalDocs) 
        offset = indexStream.readLong();
      else
        offset = fieldsStream.length();
      lengths[count++] = (int) (offset-lastOffset);
      lastOffset = offset;
    }

    fieldsStream.seek(startOffset);

    return fieldsStream;
  }
}
```
fdx索引文件用于在fdt文件中定位每个document的起始地址与终止地址，fdt文件segSize大小 ，其中对应着segment段内的segSize篇文档。每个项FieldValuePosition有固定长度大小，为8个字节。由于是固定长度的项，所以方便随机定位与访问。

这也是rawDoc函数中，indexStream.seek(startDocID * 8L)使用一个固定的公式 docID * FieldValuePosition项占用8个字节，用来从fdx索引文件中快速定位fdt文件中docID文档在起始地址。比如说rawDocs函数需要读取numDocs个文档在fdt的field/field-value的数据，它需要简单遍历[startDocID, startDocID+numDocs]区间，区间内每个元素存储的是此docID对应文档的field/field-value信息在fdt文件的偏移量，最终返回一个lengths[]数组，其元素内容即是每个doc文档在fdt索引文件中的字节大小。
```
while (count < numDocs) {
  final long offset;
  final int docID = startDocID + count + 1;
  assert docID <= numTotalDocs;
  if (docID < numTotalDocs) 
    offset = indexStream.readLong();
  else
    offset = fieldsStream.length();
  lengths[count++] = (int) (offset-lastOffset);
  lastOffset = offset;
}
```
rawDocs函数退出前，将fdt文件指向当前访问docID对应的文档偏移位置，方便后面对DocFieldData数据的分析。
```
fieldsStream.seek(startOffset); 
```

## fieldsWriter.addRawDocuments
addRawDocuments函数接收三个参数：stream即是fdt索引文件的句柄，它的读指针已经指向了第一个访问docID的偏移位置，lengths[]数组存储着连续numDocs文档在fdt文件中各自DocFieldData项字节大小。

既然是合并多个连续doc的field/field-value信息，数据本身存储在fdt文件，为了提高查询效率，间隔采样的索引数据存储在fdx文件中。

fieldsStream.copyBytes(stream, position-start)实现将多个连续doc的field/field-value从旧的fdt批量字节拷贝到新的fdt全并后的文件中。

for循环内部执行indexStream.writeLong(position)将每个docID在fdt文件各自偏移量更新进fdx索引文件。
```
final void addRawDocuments(IndexInput stream, 
                           int[] lengths, int numDocs) {
  long position = fieldsStream.getFilePointer();
  long start = position;
  for(int i=0;i<numDocs;i++) {
    indexStream.writeLong(position);
    position += lengths[i];
  }
  fieldsStream.copyBytes(stream, position-start);
  assert fieldsStream.getFilePointer() == position;
}
```

## 合并分词：SegmentMerger#mergeTerms
对于来自不同segment下的多个文档，不同文档相同域下的相同分词，以及不同文档不同域下的相同分词。这二种情况下都需要将相同分词的term进行合并存储。

分词存储涉及到二块，分词字典与分词的倒排索引信息。它们是分开存储在不同的索引文件中。分词字典使用tii和tis文件，而分词倒排索引使用frq和prx文件。
```
freqOutput = directory.createOutput(segment + ".frq");
proxOutput = directory.createOutput(segment + ".prx");
termInfosWriter =
        new TermInfosWriter(directory, segment, fieldInfos,
```
freqOutput, proxOutput, termInfosWriter内存建模结构用来写分词字典与倒排索引信息文件。
```
private final void mergeTerms() {
  try {
    freqOutput = directory.createOutput(segment + ".frq");
    proxOutput = directory.createOutput(segment + ".prx");
    termInfosWriter =
            new TermInfosWriter(directory, segment, fieldInfos,
                                termIndexInterval);
    skipInterval = termInfosWriter.skipInterval;
    maxSkipLevels = termInfosWriter.maxSkipLevels;
    skipListWriter = new DefaultSkipListWriter(skipInterval, maxSkipLevels, mergedDocs, freqOutput, proxOutput);
    
    queue = new SegmentMergeQueue(readers.size());
    mergeTermInfos();
  }
}
```
SegmentMergeQueue优先队列
SegmentMergeQueue合并段的优先级队列。
```
queue = new SegmentMergeQueue(readers.size());
mergeTermInfos();
```

## mergeTermInfos
queue = new SegmentMergeQueue(readers.size())创建了一个priorityQueue优先队列。

第一个for循环，将待合并段segment的第一个分词term取出来，并放入queue优先队列中。第二个while循环，从优先队列中取出相同分词term的SegmentMergeInfo对象，并放入match数组中。SegmentMergeInfo数据结构用来表达一个合并段元信息，如前所述，docID编码在segment级别的唯一性，现在将多个segment下文档进行合并，需要对文档docID进行重新编码，从而确保所有文档id在Index级别的唯一性。base是每个待合并段的baseID的基准值。
```
SegmentMergeInfo smi = new SegmentMergeInfo(base, termEnum, reader);
base += reader.numDocs();
```
mergeTermInfo(match, matchSize)对相同词条的多个SegmentMergeInfo对象进行实际的合并操作。
```
private final void mergeTermInfos(){
  int base = 0;
  for (int i = 0; i < readers.size(); i++) {
    IndexReader reader = (IndexReader) readers.elementAt(i);
    TermEnum termEnum = reader.terms();
    SegmentMergeInfo smi = new SegmentMergeInfo(base, termEnum, reader);
    base += reader.numDocs();
    if (smi.next())
      queue.put(smi);             // initialize queue
    else
      smi.close();
  }

  SegmentMergeInfo[] match = new SegmentMergeInfo[readers.size()];

  while (queue.size() > 0) {
    int matchSize = 0;         // pop matching terms
    match[matchSize++] = (SegmentMergeInfo) queue.pop();
    Term term = match[0].term;
    SegmentMergeInfo top = (SegmentMergeInfo) queue.top();

    while (top != null && term.compareTo(top.term) == 0) {
      match[matchSize++] = (SegmentMergeInfo) queue.pop();
      top = (SegmentMergeInfo) queue.top();
    }

    final int df = mergeTermInfo(match, matchSize);         
    // add new TermInfo
      
    while (matchSize > 0) {
      SegmentMergeInfo smi = match[--matchSize];
      if (smi.next())
        queue.put(smi);          // restore queue
      else
        smi.close();            // done with a segment
    }
}
while (matchSize > 0) 遍历所有待合并段，移动到下一个分词继续处理。
```

## mergeTermInfo
对多个段中相同分词term的全并操作还是交给了#appendPostings函数来完成。
```
private final int mergeTermInfo(SegmentMergeInfo[] smis, 
                                int n){
  long freqPointer = freqOutput.getFilePointer();
  long proxPointer = proxOutput.getFilePointer();

  int df = appendPostings(smis, n);     
  long skipPointer = skipListWriter.writeSkip(freqOutput);

  if (df > 0) {
    termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
    termInfosWriter.add(smis[0].term, termInfo);
  }

  return df;
}
```

## appendPostings
appendPostings函数重新写入频率文件和位置文件，即合并后的同名分词term的posting倒排索引表重新填写到索引中。

getDocMap函数返回一个docMap[]数组，下标是docID, 而 value为-1，说明当前docID在Lucene索引中被标记为删除。

我们知道Lucene的删除只是一个软删，在 del对应文件中做了一个标记，并没有真正从物理磁盘中进行清除。直到小的索引段segment不断进行合并的时候，才会真正从物理磁盘中进行清除，与此同时，对段内的所有文档docID会进行重新分配，保证连续存储在磁盘的文件的docID也是连续。

for (int i = 0; i < n; i++) 最外层循环遍历具有相同term分词的多个段，对多个段的相同term的<docID, docFrq>与<docID, poition1, position2,...>元信息进行合并。

对于while (postings.next()) 与int doc = postings.doc() 调用，分别定位当前segment段某个分词的<docID, docFreq>词频信息和<docID, poition1, position2,...>位置信息。

写<docID, docFrq>元信息至frq文件
举例如下：有二个segment1/2需要对 leon-term分词进行合并。
```
Segment1:
leon-term -> <doc1, 2> <doc2, 5> <doc3-del, 2> <doc4, 6>

Segment2:
leon-term -> <doc1, 2> <doc2, 5> <doc3-del, 2>
```
Segment1建模的SegmentMergeInfo1结构，它的 base置为0， 并且SegmentMergeInfo1管理的leon-term分词对应的文档数目为<doc1,doc2,doc3,doc4>，即SegmentMergeInfo1.maxDocSize=4。

Segment2建模的SegmentMergeInfo2结构，它的 base置为SegmentMergeInfo2.maxDocSize的大小， 并且SegmentMergeInfo2管理的leon-term分词对应的文档数目为<doc1,doc2,doc3>，即SegmentMergeInfo2.maxDocSize=3。

appendPostings将多个小的segment生成一个更大的segment时，需要解决docID冲突的问题，其编码如下：
```
Segment1:
leon-term -> <doc1, 2> <doc2, 5><doc3, 6>

Segment2:
leon-term -> <doc4, 2> <doc5, 5> <doc6-del, 2>
```
其中segment2管理的所有文档ID，都在segment2.base=segment1.maxDocSize=4的基础上进行递增，保证segment2与segment1文档ID不冲突。

最后写入frq文件中格式如下：
```
leon-term -> <doc1, 2> <doc2, 5><doc3, 6> <doc4, 2> <doc5, 5> <doc6-del, 2>
```
其中prs位置信息文件写入逻辑相似，此处省略。
```
appendPostings(SegmentMergeInfo[] smis, int n){
    for (int i = 0; i < n; i++) {
      SegmentMergeInfo smi = smis[i];
      TermPositions postings = smi.getPositions();
      int base = smi.base;
      int[] docMap = smi.getDocMap();
      postings.seek(smi.termEnum);
      while (postings.next()) {
        int doc = postings.doc();
        if (docMap != null)
          doc = docMap[doc];                      
        doc += base;                           

     
        df++;

        if ((df % skipInterval) == 0) {
          skipListWriter.setSkipData(lastDoc, storePayloads, lastPayloadLength);
          skipListWriter.bufferSkip(df);
        }

        int docCode = (doc - lastDoc) << 1;   
        lastDoc = doc;

        int freq = postings.freq();
        if (freq == 1) {
          freqOutput.writeVInt(docCode | 1);    
        } else {
          freqOutput.writeVInt(docCode);    
          freqOutput.writeVInt(freq);    
        }
         
    freqOutput.writeVInt(docCode);     // write doc
    freqOutput.writeVInt(freq);       // write frequency in doc
  }
```

## XXX.frq

更新tii/tis词典索引文件
```
private final int mergeTermInfo(SegmentMergeInfo[] smis, int n) {
  long freqPointer = freqOutput.getFilePointer();
  long proxPointer = proxOutput.getFilePointer();

  int df = appendPostings(smis, n);       
  long skipPointer = skipListWriter.writeSkip(freqOutput);

  if (df > 0) {
    termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
    termInfosWriter.add(smis[0].term, termInfo);
  }

  return df;
}
```
tis词信息文件
tis是词信息文件(TermInfos)，词信息文件用于存储分词后的词条，即某个segment中所有Term数据，这些term按照所属field的名称排序。格式如下
```
TermInfoFile (.tis)–> TIVersion, TermCount, IndexInterval, SkipInterval, MaxSkipLevels, TermInfos

TermInfos –> <TermInfo>^ TermCount
TermInfo –> <Term, DocFreq, FreqDelta, ProxDelta, SkipDelta>

Term –> <PrefixLength, Suffix, FieldNum>
```

XXX.tis

TermInfo.freqPointer可以定位到当前term在frq文件中对应的TermPostingList(倒排表)项位置的偏移量。TermInfo.proxPointer可以定位当前term在prx文件中对应TermPosition项位置的偏移量。

换言之，利用tis文件中的TermInfo项的FreqDelta和ProxDelta字段，间接将term分词的倒排索引文件frq与prx联系起来了。
```
final class TermInfo {
  long freqPointer = 0;
  long proxPointer = 0;
  int skipOffset;
 
  final void set(int docFreq, long freqPointer, 
                long proxPointer, int skipOffset) {
    this.docFreq = docFreq;
    this.freqPointer = freqPointer;
    this.proxPointer = proxPointer;
    this.skipOffset = skipOffset;
  }
```

termInfo存储的是frq与prx文件指向当前term的绝对偏移量位置。
```
long freqPointer = freqOutput.getFilePointer();
long proxPointer = proxOutput.getFilePointer();

termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
```
termInfosWriter.close()执行tis文件写磁盘操作。
```
private final void mergeTerms()  {
  try {
    mergeTermInfos();
  } finally { 
    if (termInfosWriter != null) termInfosWriter.close();
  }
}
```

合并词向量：mergeVectors

### XXX.tvf/tvd/tvx
词向量term vector对一个field域是一个可选项。词向量从索引index到段 segment，再到document文档，Field域信息，最后到term分词的正向索引信息，即索引包含了哪些段，每个段又包含了哪些文档，每个文档包含了哪些域，每个域包含了哪些词。

### tvx索引文件
一个段segment包含M篇文档，此tvx文件就有M项，每一项代表一篇文档。每一项包含二部分信息：第一部分是tvd中此文档位置的偏移量，第二部分是tvf中此文档中第一个域field位置的偏移量。
```
.tvx -> TVXVersion<DocumentPosition,FieldPosition> NumDocs
FieldPosition --> UInt64 (offset in the .tvf file)
```

### tvd文件
tvd文件中每一项FieldInformation包含了一个文档的所有域的信息。每一项首先是NumFields，表示此文档包含域的个数，然后是一个NumFields大小的数组，其元素是每个field的id，最后还是一个NumFields-1大小的数组，其元素对应NumFields-1域field在tvf位置的偏移量。
```
.tvd -> TVDVersion<NumFields, FieldNums, FieldPositions> NumDocs
FieldNums --> <FieldNumDelta> NumFields
FieldPositions --> <FieldPositionDelta> NumFields-1
```

### tvf文件
tvf文件包含了些段中所有文档的所有field域。那么如何区别每个文档的起始与结束位置？由tvf第一个field域的偏移量以及tvd文件中的(NumFields-1)个字段域的偏移量来决定。

对于tvf的每一个field域，首先是此field-value经过分词器解析出的分词term的个数numTerms，然后是一个8位的字节，其中最后二位分别表示是否保存此分词的偏移信息和位置信息，接着是numTerms项的数组，其中每个数组元素代表一个term分词信息。

我们知道，对于一个term分词，它由term-text,term-freq词频，term-position词的位置和term-offset词的偏移量组成。
```
.tvf -> TVFVersion<NumTerms, Position/Offset, TermFreqs> NumFields
TermFreqs --> <TermText, TermFreq, Positions?, Offsets?> NumTerms

TermText --> <PrefixLength, Suffix>
Positions --> <VInt>TermFreq
Offsets --> <VInt, VInt>TermFreq
```

### mergeVectors
```
private final void mergeVectors() {
  TermVectorsWriter termVectorsWriter = 
    new TermVectorsWriter(directory, segment, fieldInfos);

  try {
    for (int r = 0; r < readers.size(); r++) {
      IndexReader reader = (IndexReader) readers.elementAt(r);
      int maxDoc = reader.maxDoc();
      for (int docNum = 0; docNum < maxDoc; docNum++) {
        // skip deleted docs
        if (reader.isDeleted(docNum)) 
          continue;
        termVectorsWriter.addAllDocVectors(reader.getTermFreqVectors(docNum));
      }
    }
  }
}
```

### getTermFreqVectors
TermVectorsReader#get完全依据tvx/tvd/tvf三个文件的数据格式来进行线性检索的过程。
```
public TermFreqVector[] getTermFreqVectors(int docNumber) throws IOException {
  ensureOpen();
  if (termVectorsReaderOrig == null)
    return null;
  
  TermVectorsReader termVectorsReader = getTermVectorsReader();
  if (termVectorsReader == null)
    return null;
  
  return termVectorsReader.get(docNumber);
}

TermFreqVector[] get(int docNum) throws IOException {
    TermFreqVector[] result = null;
    if (tvx != null) {
      //We need to offset by
      tvx.seek(((docNum + docStoreOffset) * 8L) + FORMAT_SIZE);
      long position = tvx.readLong();

      tvd.seek(position);
      int fieldCount = tvd.readVInt();

      if (fieldCount != 0) {
        int number = 0;
        String[] fields = new String[fieldCount];

        for (int i = 0; i < fieldCount; i++) {
          if(tvdFormat == FORMAT_VERSION)
            number = tvd.readVInt();
          else
            number += tvd.readVInt();

          fields[i] = fieldInfos.fieldName(number);
        }

        position = 0;
        long[] tvfPointers = new long[fieldCount];
        for (int i = 0; i < fieldCount; i++) {
          position += tvd.readVLong();
          tvfPointers[i] = position;
        }

        result = readTermVectors(docNum, fields, tvfPointers);
      }
    } 
    return result;
  }
```
readTermVectors
从tvf文件读取某个文档下所有的field信息。SegmentTermVector结构代表某个field域经过分词器后生成的多个分词term在tvf文件中存储的元信息。
```
class SegmentTermVector implements TermFreqVector {
  private String field;
  private String terms[];
  private int termFreqs[];

```
SegmentTermVector.field代表文档的某个field。
SegmentTermVector.terms[]代表文档的某个field下分析出的所有分词。
SegmentTermVector.termFreqs[]代表文档某个field下分析出的所有分词关于词频的信息。

换言之，SegmentTermVector结构体代表tvf文件中最小的存储单元。

对于tvf的每一个field域，首先是此field-value经过分词器解析出的分词term的个数numTerms，然后是一个8位的字节，其中最后二位分别表示是否保存此分词的偏移信息和位置信息，接着是numTerms项的数组，其中每个数组元素代表一个term分词信息。
我们知道，对于一个term分词，它由term-text,term-freq词频，term-position词的位置和term-offset词的偏移量组成。
```
private SegmentTermVector[] readTermVectors(int docNum, String fields[], long tvfPointers[]) {
  SegmentTermVector res[] = new SegmentTermVector[fields.length];
  for (int i = 0; i < fields.length; i++) {
    ParallelArrayTermVectorMapper mapper = new ParallelArrayTermVectorMapper();
    mapper.setDocumentNumber(docNum);
    readTermVector(fields[i], tvfPointers[i], mapper);
    res[i] = (SegmentTermVector) mapper.materializeVector();
  }
  return res;
}  
```
addAllDocVectors
```
public final void addAllDocVectors(TermFreqVector[] vectors)
```