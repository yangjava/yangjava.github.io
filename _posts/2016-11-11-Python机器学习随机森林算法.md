---
layout: post
categories: [Python]
description: none
keywords: Python
---
# Python机器学习随机森林

## 随机森林
随机森林是一种相对较新的机器学习算法，几乎能预测任何数据类型的问题，拥有广泛的应用前景。熟悉了决策树就能很容易理解随机森林。随机森林就是将多棵决策树集成在一起的算法，它的基本单元是决策树，而它本质上属于机器学习的一大分支—集成学习（Ensemble Learning）方法。

随机森林包含的每棵决策树都是一个分类模型，对于一个输入样本，每个分类模型都会产生一个分类结果，类似投票表决。随机森林集成了所有的“投票”分类结果，并将“投票”次数最多的类别指定为最终的输出类别。随机森林每颗决策树的训练样本都是随机的，决策树中训练集的特征列也是随机选择确定的。正是因为这两个随机性的存在，使得随机森林不容易陷入过拟合，并且具有很好的抗噪能力。

考虑到随机森林的每一棵决策树中训练集的特征列是随机选择确定的，因此它更适合处理具有多特征列的数据，所以选择Scikit-learn内置的威斯康星州乳腺癌数据集来演示随机森林分类模型的使用。

该数据集有569个乳腺癌样本，每个样本包含半径、纹理、周长、面积、是否平滑、是否紧凑、是否凹凸等30个特征。
```
from sklearn.datasets import load_breast_cancer
ds = load_breast_cancer() # 加载威斯康星州乳腺癌数据集
ds.data.shape # 569个乳腺癌样本，每个样本包含30个特征
# (569, 30)
```
下面的代码使用交叉验证函数cross_val_score( )来评估决策树分类模型和随机森林分类模型的精度。交叉验证的原理是将样本分成n份，每次用其中的n-1份作训练集，剩余1份作测试集，训练n次，返回每次的训练结果。随机森林分类模型从Scikit-learn的集成子模块（ensemble）导入。
```
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
dtc = DecisionTreeClassifier() # 实例化决策树分类模型
rfc = RandomForestClassifier() # 实例化随机森林分类模型
dtc_scroe = cross_val_score(dtc, ds.data, ds.target, cv=10) # 交叉验证
dtc_scroe # 决策树分类模型交叉验证10次的结果
# array([0.94736842, 0.87719298, 0.92982456, 0.89473684, 0.94736842,
#       0.87719298, 0.89473684, 0.94736842, 0.92982456, 0.91071429])
dtc_scroe.mean() # 决策树分类模型交叉验证10次的平均精度
# 0.9156328320802005
# rfc_scroe = cross_val_score(rfc, ds.data, ds.target, cv=10) # 交叉验证
# rfc_scroe # 随机森林分类模型交叉验证10次的结果
# array([0.98245614, 0.89473684, 0.94736842, 0.94736842, 0.98245614,
#       0.98245614, 0.96491228, 0.98245614, 0.94736842, 0.96428571])
rfc_scroe.mean()# 随机森林分类模型交叉验证10次的平均精度
# 0.9595864661654134
```
决策树分类模型和随机森林分类模型的交叉验证结果表明，随机森林分类模型的分类精度明显优于决策树分类模型。而且合理选择参数（参数调优），随机森林分类模型的分类精度还有提升空间。

在当前所有算法中，随机森林分类模型具有很高的准确率，能够处理很高维度（特征很多）的数据，并且不用做特征选择（因为特征子集是随机选择的）。随机森林分类模型包含的各个决策树之间是相互独立的，可以通过并行训练提升训练速度。不过，随机森林分类模型也不是完美无缺的，它的参数调优相对复杂，在某些噪声较大的分类或回归问题上会出现过拟合。

## 支持向量机分类
支持向量机（Support Vector Machine，SVM）的基本原理是找到一个将所有数据样本分割成两部分的超平面，使所有样本到这个超平面的累计距离最短。什么是超平面呢？超平面是指n维线性空间中维度为n-1的子空间。例如，在二维平面中，一维的直线可以将二维平面分割成两部分；在三维空间中，二维的平面可以将空间分成两部分。

显然，SVM是一种二分类模型。SVM在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并且还能够推广应用到函数拟合等其他机器学习问题中。不过，SVM解决多元分类问题时效率较低，也难以对大规模训练样本实施训练。

Scikit-learn的支持向量机子模块（svm）提供了三个分类模型：LinearSVC、NuSVC和SVC。SVC分类模型和NuSVC分类模型的方法类似，都是基于libsvm实现，它们的区别是损失函数的度量方式不同（NuSVC分类模型中的nu参数和SVC分类模型中的C参数）；LinearSVC分类模型实现了线性分类支持向量机，基于liblinear实现，既可以用于二类分类，也可以用于多类分类。

使用的威斯康星州乳腺癌数据集只有良性和恶性两个标签，正好适合测试支持向量机分类模型。以下代码使用交叉验证函数测试了三个分类模型的分类精度。其中SVC分类模型和NuSVC分类模型均使用默认参数，而LinearSVC分类模型指定了参数dual为False（当样本数量大于特征数量时，参数dual倾向为False，否则liblinear不收敛）。

```
>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn import svm
>>> ds = load_breast_cancer() # 加载威斯康星州乳腺癌数据集
>>> msvc = svm.SVC() # 实例化SVC分类模型
>>> mnusvc = svm.NuSVC() # 实例化NuSVC分类模型
>>> mlsvc = svm.LinearSVC(dual=False) # 实例化LinearSVC分类模型
>>> score_msvc = cross_val_score(msvc, ds.data, ds.target, cv=10)
>>> score_mnusvc = cross_val_score(mnusvc, ds.data, ds.target, cv=10)
>>> score_mlsvc = cross_val_score(mlsvc, ds.data, ds.target, cv=10)
>>> score_msvc.mean() # SVC分类模型交叉验证10次的平均精度
0.9138784461152882
>>> score_mnusvc.mean() # NuSVC分类模型交叉验证10次的平均精度
0.8734962406015038
>>> score_mlsvc.mean() # LinearSVC分类模型交叉验证10次的平均精度
0.9542931402580526
```
针对威斯康星州乳腺癌数据集的分类，LinearSVC分类模型的表现最好，准确率超过95%，NuSVC分类模型和SVC分类模型的表现只能算差强人意。









