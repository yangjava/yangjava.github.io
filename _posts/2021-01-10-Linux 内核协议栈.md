---
layout: post
categories: Linux
description: none
keywords: Linux
---
# Linux内核协议栈

## Linux 内核协议栈分层架构
- 驱动程序层（Physical device hardware）：提供连接硬件设备的各种软件接口。
- 设备接口层（Device agnostic interface）：提供驱动程序的各种抽象接口。作为驱动层和协议层之间的中间层，提供无关具体设备的统一接口定义。
- 网络协议层（Network protocols）：提供 L3-4 TCP/IP 网络协议的具体实现。例如：ARP、IP、ICMP、TCP 等。
- 协议接口层（Protocol agnostic interface）：也称为 BSD Socket API 层，本质是 SCI（系统调用接口）的一部分，向上层应用程序提供各种网络编程接口。作为协议层和应用层之间的中间层，提供无关具体协议的统一 Socket 接口定义。

## 报文处理流程概览

## sk_buff 结构体
sk_buff 结构体是 Kernel 定义的一个用于描述 Frame 的数据结构。

Net driver 的初始化流程中包括对 DMA 空间进行初始化，主要的工作就是在 ZONE_DMA 中分配好用于存储 sk_buff 的内存空间。

当 Frame 到达 NIC 后，DMA Controller 就会将 Frame 的数据 Copy 到 sk_buff 结构体中，以此来完成 Frame => sk_buff 数据格式的封装。在后续的流程中，sk_buff 还会从 ZONE_DMA Copy 到 Kernel Socket Receive Buffer 中，等待 Application 接收。
```
struct igb_ring {
...
 union {
...
  /* RX */
  struct {
   struct sk_buff *skb;
   struct igb_rx_queue_stats rx_stats;
   struct u64_stats_sync rx_syncp;
  };
...
}
```
sk_buff 的结构体定义如下图所示，它包含了一个 Frame 的 Interface（网络接口）、Protocol（协议类型）、Headers（协议头）指针、Data（业务数据）指针等信息。

值得留意的是，在 Kernel 中，一个 Frame 的 Headers 和 Payload 可能是分开存放到不同内存块种的。有以下几点原因：
- 两者具有不同的特征和用途：Headers 包含了网络协议的元数据信息，而 Payload 包含了 Application 的业务信息。
- 两者具有不同的大小和格式：Headers 的格式通常是标准的，而且数据量比 Payload 小的多。
- 两者具有不同的处理逻辑：Headers 需要被快速识别、分析和校验，而 Payload 则需要被快速的传输和存储。
分开存储和处理的方式，可以有效提高网络传输的效率和可靠性。

同时，sk_buff 是一个双向链表数据结构，支持链表操作。

## Net driver Rx/Tx Ring Buffer
Net driver 实现了 2 个 Ring Buffer 用于数据报文的收发处理。

Ring queue 是高性能数据包处理场景中常见的数据结构，它将 Buffer 内存空间设计成一个首尾相连的环。当 Buffer 空间溢满后，新来的 Frames 会从头开始存放，而不是为其分配新的内存空间。相较于传统的 FIFO queue 数据结构，Ring queue 可以避免频繁的内存分配和数据复制，从而提高传输效率。此外还具有缓存友好、易于并行处理等优势。

值得注意的是，Rx/Tx Ring Buffer 中存储的是 sk_buff 的 Descriptor，而不是 sk_buff 本身，本质是一个指针，也称为 Packet Descriptor。

Packet Descriptor 有 Ready 和 Used 这 2 种状态。初始时 Descriptor 指向一个预先分配好且是空的 sk_buff 空间，处在 Ready 状态。当有 Frame 到达时，DMA Controller 从 Rx Ring Buffer 中按顺序找到下一个 Ready 的 Descriptor，将 Frame 的数据 Copy 到该 Descriptor 指向的 sk_buff 空间中，最后标记为 Used 状态。

这样设计的原因是 Rx/Tx Ring Buffer 作为 I/O 控制单元，不应该持有太多数据量。数据传输由 DMA 实现会非常快，而 Ring Buffer 也只需要记录相应的指针即可。

## Buffer Descriptor Table
Rx/Tx Ring Buffer 的具体实现为一张 Buffer Descriptor Table（BDT）。

BDT 是一个 Table 数据结构，拥有多个 Entries，每条 Entry 都对应了 Ring Buffer 中的一个 Rx/Tx Desc，它们记录了存放 sk_buff 的入口地址、长度以及状态标志位。

收包时：DMA Controller 搜索 Rx BDT，取出空闲的 DB Entry，将 Frame 存放到 Entry 指向的 sk_buff，修改 Entry 为 Ready。然后 DBT 指针下移一项。
发包时：DMA Controller 搜索 Tx BDT，取出状态为 Ready 的 DB Entry 所指向的 sk_buff 并转化为 Frame 发送出去。然后 DBT 指针下移一项。

## NAPI 收包机制
NAPI（New API）是一种 “中断 + 轮训” 收包机制，相较于传统的单一中断（硬中断 + 软中断）收包的方式效率更高。

NAPI 的工作流程如下：
- Net driver 初始化流程中，注册 NAPI 收包机制所必须的 poll() 函数到 ksoftirqd（软中断处理）内核线程。
- Frame 到达 NIC；
- DMA Controller 写入 sk_buff；
- NIC Controller 发起硬中断，进入 Net driver 的 napi_schedule() 硬中断处理函数，然后将一个 napi_struct 结构体加入到 poll_queue（NAPI 软中断队列）中。此时 NIC Controller 立即禁用了硬中断，开始切换到 NAPI “轮训“ 收包工作模式。
- 再进入 raise_softirq_irqoff(NET_RX_SOFTIRQ) 软中断处理程序，唤醒 NAPI 子系统，新建 ksoftirqd 内核线程。
- ksoftirqd 线程调用 Net driver 注册的 poll() 函数。
- poll() 调用 netif_receive_skb() 开始从 sk_buff 空间中收包，并将它传递给网络协议栈进行处理。
- 直到一段时间内没有 Frame 到达为止，关闭 ksoftirqd 内核线程，NIC Controller 重新切换到 NAPI “中断” 收包工作模式。
在具体的实现中，poll() 会轮训检查 BDT Entries 的状态，如果发现当前 BDT 指针指向的 Entry Ready，则将该 Entry 对应的 sk_buff 取出来，并恢复该 Entry 的空闲状态。

可见，和传统方式相比，NAPI 一次中断就可以接收多个包，因此可以减少硬件中断的数量。

## 网卡多队列
在以往，一张 NIC 只会提供一组 HW Rx/Tx Ring queue，对应一个 CPU 来进行处理。在多核时代，NIC 也相应的提供了 Multi-Queue 功能，可以将多个 Queue 通过硬中断绑定到不同的 CPU Cores 上处理。

以 Intel 82575 为例。

在硬件层面：它拥有 4 组硬件队列，它们的硬中断分别绑定到 4 个 Core 上，并通过 RSS（Receive Side Scaling）技术实现负载均衡。RSS 技术通过 HASH Packet Header IP 4-tuple（srcIP、srcPort、dstIP、dstPort），将同一条 Flow 总是送到相同的队列，从而避免了报文乱序问题。

在软件层面：Linux Kernel v2.6.21 开始支持网卡多队列特性。在 Net driver 初始化流程中，Kernel 获悉 Net device 所支持的硬件队列数量。然后结合 CPU Cores 的数量，通过 Sum=Min(NIC queue, CPU core) 公式计算得出应该被激活 Sum 个硬件队列，并申请 Sum 个中断号，分配给激活的每个队列。

如上图所示，当某个硬件队列收到 Frames 时，就触发相应的硬中断，收到中断的 CPU Core 就中断处理任务下发给该 Core 的 NET_RX_SOFTIRQ 实例处理（每个 Core 都有一个 NET_RX_SOFTIRQ 实例）。

在 NET_RX_SOFTIRQ 中调用 NAPI 的收包接口，将 Frames 收到具有多个 netdev_queue 的 net_device 结构体中。

## 内核协议栈收包/发包流程概览

## 内核协议栈收包流程详解
NIC Controller 接收到高低电信号，表示 Frame 到达。PHY 芯片首先将电信号转换为比特流，然后 MAC 芯片再将比特流转换为 Frame 格式。

DMA Controller 将 Frame Copy 到 Rx Ring Buffer 中的一个 Rx Desc 指向的 sk_buff 空间。

DMA Controller 更新相应的 BD Entry 状态为 Ready，并将 BDT 指针下移一项。

NIC Controller  给 CPU 的相关引脚上触发一个电压变化，硬中断 CPU。每个硬中断都对应一个中断号，CPU 开始收包硬中断处理程序。硬中断处理程序由 Kernel 回调 Net driver 具体实现的注册函数，根据是否开启了 NAPI 有两条不同的执行路径。

以 NAPI 模式为例，Net driver 执行 napi_schedule() 硬中断处理函数，然后将一个 napi 结构体加入到 poll_queue（NAPI 软中断队列）中。此时 NIC Controller 立即禁用了硬中断，开始切换到 “NAPI 轮训“ 收包工作模式。

再进入 raise_softirq_irqoff() 软中断处理程序，唤醒 NAPI 子系统，新建 ksoftirqd 内核线程。

ksoftirqd 线程调用 Net driver 注册的 poll() 函数。
```
// linux/drivers/net/ethernet/intel/igb/igb_main.c

/**
 *  igb_poll - NAPI Rx polling callback
 *  @napi: napi polling structure
 *  @budget: count of how many packets we should handle
 **/
static int igb_poll(struct napi_struct *napi, int budget)
{
...
 if (q_vector->tx.ring)
  clean_complete = igb_clean_tx_irq(q_vector, budget);

 if (q_vector->rx.ring) {
  int cleaned = igb_clean_rx_irq(q_vector, budget);
...
}
```
poll() 调用 netif_receive_skb() 开始从 sk_buff 空间中收包，并将它传递给 TCP/IP 网络协议栈进行处理。
```
// linux/net/core/dev.c

int netif_receive_skb(struct sk_buff *skb)
{
 int ret;

 trace_netif_receive_skb_entry(skb);

 ret = netif_receive_skb_internal(skb);
 trace_netif_receive_skb_exit(ret);

 return ret;
}
```
直到一段时间内没有 Frame 到达为止，关闭 ksoftirqd 内核线程，NIC Controller 重新切换到硬中断收包工作模式。

以 UDP 数据报为例：

协议接口层：BSD socket 层的 sock_write() 会调用 INET socket 层的 inet_wirte()。INET socket 层会调用具体传输层协议的 write 函数，该函数是通过调用本层的 inet_send() 来实现的，inet_send() 的 UDP 协议对应的函数为 udp_write()。

L4 子系统：udp_write() 调用本层的 udp_sendto() 完成功能。udp_sendto() 完成 sk_buff 结构体相应的设置和 Header 的填写后会调用 udp_send() 来发送数据。而在 udp_send() 中，最后会调用 ip_queue_xmit() 将数据包下放的网络层。

L3 子系统：函数 ip_queue_xmit() 的功能是将数据包进行一系列复杂的操作，比如是检查数据包是否需要分片，是否是多播等一系列检查，最后调用 dev_queue_xmit() 发送数据。

驱动程序层：函数调用会调用具体设备提供的发送函数来发送数据包，e.g. hard_start_xmit(skb, dev)。具体设备的发送函数在协议栈初始化的时候已经设置了。

## Socket 网络编程框架
Socket（套接字）是一个网络编程概念，描述了一个通信端点（Endpoint），用于建立网络连接（Connection）并传输数据。

Linux Kernel 提供了一套面向 Socket 的网络编程框架，并通过提供一组标准的 System call APIs，使得开发者可以在 Userspace 中便捷的开发各种 Network Applications，例如：基于 HTTP 协议的 Web 服务器、基于 SMTP 协议的邮件服务器、基于 FTP 协议的文件服务器等等。

Linux Socket 网络编程框架主要由 3 大模块组成：
- BSD Socket APIs
- Socket Abstraction Layer
- VFS Layer

## BSD Socket APIs 概览
BSD Socket APIs（Berkeley Software Distribution Socket APIs），是面向 Userspace Application 的接口封装层，提供了一套兼容绝大部分网络通信协议族的标准 Socket APIs。

socket()：创建一个新的 socket，返回一个 int 类型的 socket fd（File Descriptor，套接字文件描述符），用于后续的网络连接操作。

bind()：将 socket 与一个本地 IP:Port  绑定，通常用于服务端，以便在本地监听网络连接。

connect()：建立与远程主机的连接，通常用于客户端，以便连接到远程服务器。

listen()：开始监听来自远程主机的连接请求，通常用于服务器端，等待来自客户端的连接请求。

accept()：接受一个连接请求，返回一个新的 socket fd，通常用于服务器端，用于接收客户端的连接请求。

send()：向 socket 发送数据。

recv()：从 socket 接收数据。

close()：关闭 socket 连接。

Socket API 的使用通常可以分为以下几个步骤：

创建套接字：使用 socket() 函数创建一个新的 socket fd。

配置套接字：使用一些其他的 Socket API 函数，例如 bind()、connect() 和 listen() 来配置 socket，使其能够接收和发送数据。

数据传输：使用 send() 和 recv() 函数进行数据传输。

关闭套接字：使用 close() 函数关闭 socket 连接。

需要注意的是，Socket API 并不是线程安全的，如果有多个线程同时使用了同一个 socket fd，则可能会导致数据传输错误或其他问题。为了避免这种情况，Application 需要进行适当的同步和数据处理。

Socket Abstraction Layer
Socket Abstraction Layer（Socket 抽象层），是 Socket API 的底层支撑，主要负责以下工作：

实现了 Socket File System（套接字文件系统），用于管理 User Process 和 socket fd 之间的关系，包括 socket fd 的创建、打开、读写等操作。
实现了 Struct Socket、Struct Sock、Protocol Family（协议族）、Address Family（地址族）等数据结构。
实现了 TCP/IP 协议栈，包括：TCP、UDP、ICMP 等协议。
实现了 L4 传输层功能，处理传输层协议的连接建立、数据传输、连接维护等操作。

## Socket & Sock
Struct Socket 是在 Socket Layer 中定义的数据结构，面向上层 Socket API，包含了一个 Socket 所具有的各种属性，例如：状态、类型、标记、关联的 Sock 等。

Struct Sock 是在 Sock Layer 中定义的数据结构，面向底层协议栈实现，表示一个 Socket 对应的 PCB（Protocol Control Block，协议控制块），即：与某种网络协议相关的一些信息和状态，例如：TCP PCB 就包括了 TCP 连接状态、发送缓冲区、接收缓冲区、拥塞窗口等。

Socket Layer 与 Network Driver（网络设备驱动程序）之间通过 Socket Buffer（skb_buff）进行交互，当 Socket Layer 接收到 Application 的数据时，会将数据存储在 Socket Buffer 中，并将 Socket Buffer 传递给对应的 Sock Layer 进行处理。Struct Socket 和 Struct Sock 之间通过指针进行关联绑定，共同实现 Socket API 的功能。

## Socket Layer
```
// linux/include/linux/net.h

/**
 *  struct socket - general BSD socket
 *  @state: socket state (%SS_CONNECTED, etc)
 *  @type: socket type (%SOCK_STREAM, etc)
 *  @flags: socket flags (%SOCK_NOSPACE, etc)
 *  @ops: protocol specific socket operations
 *  @file: File back pointer for gc
 *  @sk: internal networking protocol agnostic socket representation
 *  @wq: wait queue for several uses
 */
struct socket {
 socket_state      state;
 short           type;   // 套接字类型，如 SOCK_STREAM、SOCK_DGRAM 等；
 unsigned long      flags;  // 套接字标志，如 O_NONBLOCK、O_ASYNC 等；
 struct file          *file;  // 套接字对应的文件结构体；
 struct sock          *sk;    // 指向套接字对应的 Sock 结构体；
 const struct proto_ops *ops;   // 套接字对应的操作函数集，如 inet_stream_ops、inet_dgram_ops 等；
 struct socket_wq     wq;     // 套接字等待队列；
};

typedef enum
{
  SS_FREE=0;      // 未分配
  SS_UNCONNECTED;  // 未连接到任何套接字
  SS_CONNECTING;  // 处于连接过程中
  SS_CONNECTED;   // 已经连接到另一个套接字
  SS_DISCONNECTING;     // 处于断开连接过程中
} socket_state;
```

Sock Layer
Struct Sock 包含了 Socket 的各种底层执行状态和操作信息，例如：接收和发送缓冲区、套接字队列、套接字协议信息等。
```
// linux/include/net/sock.h

struct sock {
    /* Socket family and type */
    unsigned short      family;  // 协议族，如 AF_INET、AF_PACKET 等；
    __u16               type;    // 套接字类型，如 SOCK_STREAM、SOCK_DGRAM 等；
    unsigned long       flags;   // 套接字标志，如 O_NONBLOCK、O_ASYNC 等；

    /* Protocol specific elements of the socket */
    struct proto        *ops;      // 协议特定操作函数集；
    struct net_device   *sk_net;   // 套接字所在的网络设备；

    /* Memory allocation cache */
    kmem_cache_t        *sk_slab;  // 套接字内存分配缓存；

    /* Socket state */
    atomic_t            refcnt;    // 套接字引用计数；
    struct mutex        sk_lock;   // 套接字锁，用于保护套接字状态的一致性；

    /* Send and receive buffers */
 struct sk_buff_head sk_receive_queue;  // 接收队列，保存了等待接收的数据包；
    struct sk_buff_head sk_write_queue;    // 发送队列，保存了等待发送的数据包；
    struct sk_buff      *sk_send_head;     // 发送缓冲区的头指针；
    struct sk_buff      *sk_send_tail;     // 发送缓冲区的尾指针；

    /* Receive queue */
    struct sk_buff      *sk_receive_skb;   // 当前正在接收的数据包；

    /* Transport specific fields */
    __u32           sk_priority;           // 套接字优先级；
    struct dst_entry    *sk_dst_cache;     // 缓存的目标设备；
    struct dst_entry    *sk_dst_pending_confirm;
    struct flowi        sk_fl;             // Flowi 结构体，保存了套接字相关的流信息；
    struct sk_filter    *sk_filter;        // 过滤器；
    struct sk_buff_head sk_async_wait_queue;  // 异步等待队列；

    /* Socket buffer allocations */
    unsigned long       sk_wmem_alloc;  // 发送缓冲区已分配的内存；
    unsigned long       sk_omem_alloc;

    /* User and kernel buffers */
    struct socket_wq    *sk_wq;   // 套接字等待队列；
    struct page_frag    sk_frag;  // 内存分配器的页片段；
    int         sk_forward_alloc; // 前向分配的字节数；
    int         sk_rxhash;        // 套接字是否支持接收哈希。
};
```

## Protocol Family
Socket 支持广泛的 PFs，主要有以下 4 类：

PF_INETv4v6 sockets（IP Socket）：基于 IPv4v6 网络层协议，支持 TCP、UDP 传输层协议。

SOCK_STREAM：TCP 字节流式传输。
SOCK_DGRAM：UDP 数据包式传输。
SOCK_RAW：原始套接字，可以处理 IPv4、ICMP、IGMP 等报文，常用于网络监听、检验新的协议或者访问新的设备。
PF_PACKET sockets（Packet Socket）：基于 Device Driver（设备驱动），支持对底层数据包的捕获和注入，常用于网络安全、网络监测等场景，例如：网络嗅探、协议分析、数据包过滤等。

PF_NETLINK sockets（Netlink Socket）：支持 Kernel Space 和 User Space 之间的通信，常用于网络管理和网络监测等场景，例如：获取内核中的网络信息、配置内核的网络参数、监控网络状态等。

PF_UNIX sockets（UNIX socket）：用于 Unix-like 系统中的多进程之间通信。

值得注意的是，虽然不同的协议族都使用了同一套 Socket API，但也可能会存在一些特有的函数或者数据结构，用于实现协议族特有的功能。例如：

PF_PACKET 协议族可以使用 pcap 库来进行网络数据包捕获和注入；
PF_NETLINK 协议族可以使用 netlink 库来进行内核和用户空间之间的通信。
但是，这些特有的函数和数据结构通常不会影响套接字编程接口的基本使用方式和语法。

## VFS Layer
VFS Layer 属于 Linux VFS sub-system（虚拟文件子系统），提供了一组通用的 Linux File System Call APIs（SCI），使得 Application 可以使用相同的 API 来完成文件 I/O。

当 Application 使用 Socket API 发送或接收数据时，Socket Abstraction Layer 会借助 VFS Layer 来完成 Socket File System 的管理。例如：

Application 调用 Socket API socket() 创建 socket 时：在 VFS I/O Layer 中，Socket FD 文件句柄被创建。
Application 调用 Socket API close() 关闭 socket 时：在 VFS I/O Layer 中，文件句柄被释放，并释放相关资源。

## Socket I/O 处理流程
每次 Socket I/O 操作，大体上都需要经历 2 个阶段：

准备数据阶段：数据包到达 Kernel 并就绪，Application 可以开始数据拷贝。
拷贝数据阶段：Application 通过 SCI 将数据从 Kernel 拷贝到 Userspace（进程虚拟地址空间）中。
对于上述 I/O 流程，基于 BSD Socket API 可以实现以下几种 I/O 模式：

阻塞式 I/O（Blocking I/O）
非阻塞式 I/O（Non-Blocking I/O）
I/O 多路复用（I/O Multiplexing）
从阻塞程度的角度出发，效率由低到高为：阻塞 IO > 非阻塞 IO > 多路复用 IO。

## 阻塞式 I/O（Blocking I/O）
阻塞：是指调用结果返回之前，当前线程会被挂起，一直处于等待消息通知，不能够执行其他业务。

默认情况下的  Socket I/O 都是阻塞式 I/O。

如下图所示：

准备数据阶段：当 Application 调用 recvfrom() 后，Kernel 开始准备数据，需等待足够多的数据再进行拷贝。而在等待 “数据就绪” 的过程中，Application 会被阻塞；
拷贝数据阶段：数据就绪后，Kernel 开始拷贝数据，将数据复制到 Application Buffer。拷贝完成后，Application 才会从阻塞态进入到就绪态。
可见，阻塞式 I/O 模式，Application 在调用 recvfrom() 开始，直到从 recvfrom() 返回的这段时间里，Application 都是被阻塞的。

## 阻塞式 I/O 模式的缺点：

当并发较大时，需要创建大量的线程来处理连接，占用了大量的系统资源。
TCP connection 建立完成后，如果当前线程没有数据可读，将会阻塞在 recvfrom() 操作上，造成线程资源的浪费。

## 非阻塞式 I/O（Non-Blocking I/O）
非阻塞：是指函数不会因为等待数据就绪而阻塞当前线程，而是会立刻返回。

可以使用 fcntl() 函数显式地为 socket fd 设置 O_NONBLOCK 标志，以此来启动非阻塞式 I/O 模式的 Socket API。

数据准备阶段：

Application 调用 read() 后，如果 Kernel BSD Socket Buffer 中的数据还没有准备好（或者没有任务数据），那么 non-blocking Socket 立刻返回一个 EWOULDBLOCK Error 给 Application。
Application 接收到 EWOULDBLOCK Error 后，得知 “数据未就绪“，Application 不被阻塞，继续执行其他任务，并且等待一段时候之后，再次调用 read()。
数据拷贝阶段：直到数据就绪后，正常进行数据拷贝，然后返回。

可见，非阻塞 I/O 模式，Application 通过不断地询问 Kernel 数据是否就绪，以此来规避了 “空等"。

非阻塞式 I/O 模式的缺点：

Application 使用 non-blocking Socket 时，会不停的 Polling（轮询）Kernel，以此检查是否 I/O 操作已经就绪。这极大的浪费了 CPU 的资源。
另外，非阻塞 IO 需要 Application 多次发起 read()，频繁的 SCI（系统调用）也是比较消耗系统资源的。

## I/O 多路复用（I/O Multiplexing）
多路复用（Multiplexing）是一个通信领域的术语，指在同一个信道上传输多路信号或数据流的过程和技术。

而在 Socket I/O 场景中的多路复用，即：使用同一个 Server Socket fd（信道）处理多个 Client Socket fds 的 I/O 请求，以此来避免单一连接的阻塞，进而提升 Application 的处理性能。

值得注意的是，I/O 多路复用的本质是一种 “设备 I/O” 技术，常用于 “网络 I/O” 场景，当然也可以用于其他 I/O 场景。

Linux Kernel 提供了 3 个用于支持 I/O 多路复用的设备 I/O 接口，包括：select()、poll() 和 epoll()。其中又以 epoll() 的性能最佳，所以后面会着重介绍这一方式。

### select()
函数声明：

n 参数：读、写、异常集合中的 fds（文件描述符）数量的最大值 +1。
readfds 参数：读 fds 集合。
writefds 参数：写 fds 集合。
exceptfds 参数：异常 fds 集合。
timeout 参数：超时设置。
int 函数返回值：返回监听到的数据就绪的 fds 的标记。
```
int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```
select() 的缺点：
- 单一 Application 能监视的 Socket fds 数量有限制，一般为 1024 个，需要通过调整 Kernel 参数进行修改。
- 每次调用 select() 时都需要将所有的 Socket fds Set 从 Userspace 复制到 Kernel space，造成性能开销。
- Kernel 根据实际情况为所有的 Socket fds Set 逐一打上可读、可写或异常的标记，然后返回。Application 需要循环遍历这些 fds 的标记，然后逐一进行相应的处理。

## poll()
函数声明：

fds 参数：struct pollfd 类型指针，用于指定需要监视的 fds 以及 Application 所关心的 Events。
nfds 参数：指示 fds 的数量。
timeout 参数：超时配置，单位是毫秒。为负数时，表示无限期等待，直到有事件发生。
```
#include <poll.h>

int poll(struct pollfd *fds, unsigned int nfds, int timeout);
```
poll() 相较于 select() 的改进：

没有了 Socket fds 监听数量的限制。
引入了事件监听机制，支持为每个 Socket fd 设置监听事件。
pollfd 结构体包含了要监视的 events 和已经发生了的 revents。当 Application 关心的事件发生时，poll() 才会返回，然后 Application 可以开始执行相应的操作。
```
struct pollfd {
    int fd;         // 监视的文件描述符
    short events;   // 文件描述符所关心的事件
    short revents;  // 实际发生的事件
};
```
poll() 的缺点：

依旧需要在 Userspace 和 Kernel space 之间传递大量的 Socket fds Set，造成性能损耗。
依旧需要  Application 遍历所有的 Socket fds，处理效率不高。

### epoll
为了彻底解决 select() 和 poll() 的不足，epoll() 设计了全新的运行模式，通过下面两张图片来直观的进行比较。

select() 运行原理：

每次调用 select() 都需要在 User space 和 Kernel space 之间传递 Socket fds set。
Application 需要对 Kernel 返回的 Socket fds Set 进行轮询遍历处理。

epoll() 运行原理：

首先调用 epoll_create()，在 Kernel space 开辟一段内存空间，用来存放所有的 Socket fds Set，以此避免了模式转换的数据复制。
然后调用 epoll_ctrll()，根据需要逐一的为 Socket fd 绑定监听 events 并添加到这块空间中。
最后调用 epoll_wait()， 等待 events 发生，并进行异步回调，使用 “回调监听” 代替 “轮询监听”，以此提升了性能的同时降低的损耗。

epoll 的工作模式
epoll 提供了 2 种工作模式，可以根据实际需要进行设置：

LT（Level Trigger，水平触发）模式：默认的工作模式，当 epoll_wait 监听到有 Socket fd 就绪时，就将相应的 Event 返回给 Application。此时 Application 可以根据具体的情况选择立即处理或延后处理该 Event。如果不处理，那么待下次调用 epoll_wait 时，epoll 还会再次返回此 Event。同时支持阻塞 I/O 和非阻塞 I/O 模式。

ET（Edge Trigger，边缘触发）模式：相对的，ET 模式则不会再次返回 Missed 的 Event。避免了 epoll events 被重复触发的情况，因此 ET 的处理效率要比 LT 高，常应用于高压场景。只支持非阻塞 I/O 模式。



























