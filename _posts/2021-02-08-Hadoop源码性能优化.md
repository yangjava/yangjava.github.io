---
layout: post
categories: [Hadoop]
description: none
keywords: Hadoop
---
# Hadoop源码性能优化

## 数据倾斜
数据倾斜的原因
什么叫数据倾斜？倾斜主要发生在Reduce阶段，而很少发生在 Map阶段，我们上述讲到了map阶段会均匀拆分数据到不同块，而这个过程如果倾斜往往是因为hdfs的内存不够问题，很少见。
而Reduce阶段的数据倾斜几乎都是因为数据研发工程师没有考虑到某种key值数据量偏多的情况而导致的。像我们上述降到，会把相同的key放在一个partition传送到reduce， 那比如reduce_1的数据量远远大于reduce_2的时候，就会出现倾斜。卡在这里。

！原理就是上述谈到的那些，分配资源不均，我记得好像分配reduce资源的时候是按照不同分桶组合的平均值来分的，所以倾斜才会导致整体处理能力不足。

！还有一个方法，我们在看mapreduce task 和拆分的instance时候，某个instance 的计算耗时明显大于平均的3倍，那么铁定就倾斜了。












